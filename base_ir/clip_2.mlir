module @compiled_clip {
  util.global private @_params.text_encoder_model.text_model.embeddings.token_embedding.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.embeddings.token_embedding.weight"> : tensor<49408x1280xf16>
  util.global private @_params.text_encoder_model.text_model.embeddings.position_embedding.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.embeddings.position_embedding.weight"> : tensor<77x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.0.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.1.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.2.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.3.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.4.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.5.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.6.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.7.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.8.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.9.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.10.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.11.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.12.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.13.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.14.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.15.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.16.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.17.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.18.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.19.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.20.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.21.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.22.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.23.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.24.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.25.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.26.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.27.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.28.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.29.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.30.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.layer_norm1.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.layer_norm1.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.mlp.fc1.weight"> : tensor<5120x1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.mlp.fc1.bias"> : tensor<5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.mlp.fc2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.mlp.fc2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.layer_norm2.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.encoder.layers.31.layer_norm2.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.final_layer_norm.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.final_layer_norm.weight"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_model.final_layer_norm.bias {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_model.final_layer_norm.bias"> : tensor<1280xf16>
  util.global private @_params.text_encoder_model.text_projection.weight {noinline} = #stream.parameter.named<"model"::"text_encoder_model.text_projection.weight"> : tensor<1280x1280xf16>
  func.func @main(%arg0: tensor<1x64xi64>) -> (tensor<1x64x1280xf16>, tensor<1x1280xf16>) attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.return_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]}]"} {
    %0 = torch_c.from_builtin_tensor %arg0 : tensor<1x64xi64> -> !torch.vtensor<[1,64],si64>
    %1:2 = call @forward(%0) : (!torch.vtensor<[1,64],si64>) -> (!torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,1280],f16>)
    %2 = torch_c.to_builtin_tensor %1#0 : !torch.vtensor<[1,64,1280],f16> -> tensor<1x64x1280xf16>
    %3 = torch_c.to_builtin_tensor %1#1 : !torch.vtensor<[1,1280],f16> -> tensor<1x1280xf16>
    return %2, %3 : tensor<1x64x1280xf16>, tensor<1x1280xf16>
  }
  func.func private @forward(%arg0: !torch.vtensor<[1,64],si64>) -> (!torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,1280],f16>) {
    %int-1 = torch.constant.int -1
    %int64 = torch.constant.int 64
    %0 = torch.prim.ListConstruct %int-1, %int64 : (!torch.int, !torch.int) -> !torch.list<int>
    %1 = torch.aten.view %arg0, %0 : !torch.vtensor<[1,64],si64>, !torch.list<int> -> !torch.vtensor<[1,64],si64>
    %2 = torch.vtensor.literal(dense_resource<torch_tensor_1_77_torch.int64> : tensor<1x77xsi64>) : !torch.vtensor<[1,77],si64>
    %int0 = torch.constant.int 0
    %int0_0 = torch.constant.int 0
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1 = torch.constant.int 1
    %3 = torch.aten.slice.Tensor %2, %int0, %int0_0, %int9223372036854775807, %int1 : !torch.vtensor<[1,77],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,77],si64>
    %int1_1 = torch.constant.int 1
    %int0_2 = torch.constant.int 0
    %int64_3 = torch.constant.int 64
    %int1_4 = torch.constant.int 1
    %4 = torch.aten.slice.Tensor %3, %int1_1, %int0_2, %int64_3, %int1_4 : !torch.vtensor<[1,77],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,64],si64>
    %_params.text_encoder_model.text_model.embeddings.token_embedding.weight = util.global.load @_params.text_encoder_model.text_model.embeddings.token_embedding.weight : tensor<49408x1280xf16>
    %5 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.embeddings.token_embedding.weight : tensor<49408x1280xf16> -> !torch.vtensor<[49408,1280],f16>
    %int-1_5 = torch.constant.int -1
    %false = torch.constant.bool false
    %false_6 = torch.constant.bool false
    %6 = torch.aten.embedding %5, %1, %int-1_5, %false, %false_6 : !torch.vtensor<[49408,1280],f16>, !torch.vtensor<[1,64],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[1,64,1280],f16>
    %_params.text_encoder_model.text_model.embeddings.position_embedding.weight = util.global.load @_params.text_encoder_model.text_model.embeddings.position_embedding.weight : tensor<77x1280xf16>
    %7 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.embeddings.position_embedding.weight : tensor<77x1280xf16> -> !torch.vtensor<[77,1280],f16>
    %int-1_7 = torch.constant.int -1
    %false_8 = torch.constant.bool false
    %false_9 = torch.constant.bool false
    %8 = torch.aten.embedding %7, %4, %int-1_7, %false_8, %false_9 : !torch.vtensor<[77,1280],f16>, !torch.vtensor<[1,64],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[1,64,1280],f16>
    %int1_10 = torch.constant.int 1
    %9 = torch.aten.add.Tensor %6, %8, %int1_10 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int64_11 = torch.constant.int 64
    %int64_12 = torch.constant.int 64
    %10 = torch.prim.ListConstruct %int64_11, %int64_12 : (!torch.int, !torch.int) -> !torch.list<int>
    %int64_13 = torch.constant.int 64
    %int1_14 = torch.constant.int 1
    %11 = torch.prim.ListConstruct %int64_13, %int1_14 : (!torch.int, !torch.int) -> !torch.list<int>
    %int6 = torch.constant.int 6
    %int0_15 = torch.constant.int 0
    %cpu = torch.constant.device "cpu"
    %false_16 = torch.constant.bool false
    %12 = torch.aten.empty_strided %10, %11, %int6, %int0_15, %cpu, %false_16 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[64,64],f32>
    %float-6.550400e04 = torch.constant.float -6.550400e+04
    %13 = torch.aten.fill.Scalar %12, %float-6.550400e04 : !torch.vtensor<[64,64],f32>, !torch.float -> !torch.vtensor<[64,64],f32>
    %int64_17 = torch.constant.int 64
    %none = torch.constant.none
    %none_18 = torch.constant.none
    %cpu_19 = torch.constant.device "cpu"
    %false_20 = torch.constant.bool false
    %14 = torch.aten.arange %int64_17, %none, %none_18, %cpu_19, %false_20 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[64],si64>
    %int1_21 = torch.constant.int 1
    %int1_22 = torch.constant.int 1
    %15 = torch.aten.add.Scalar %14, %int1_21, %int1_22 : !torch.vtensor<[64],si64>, !torch.int, !torch.int -> !torch.vtensor<[64],si64>
    %int64_23 = torch.constant.int 64
    %int1_24 = torch.constant.int 1
    %16 = torch.prim.ListConstruct %int64_23, %int1_24 : (!torch.int, !torch.int) -> !torch.list<int>
    %17 = torch.aten.view %15, %16 : !torch.vtensor<[64],si64>, !torch.list<int> -> !torch.vtensor<[64,1],si64>
    %18 = torch.aten.lt.Tensor %14, %17 : !torch.vtensor<[64],si64>, !torch.vtensor<[64,1],si64> -> !torch.vtensor<[64,64],i1>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %int6_25 = torch.constant.int 6
    %int0_26 = torch.constant.int 0
    %cpu_27 = torch.constant.device "cpu"
    %none_28 = torch.constant.none
    %19 = torch.aten.scalar_tensor %float0.000000e00, %int6_25, %int0_26, %cpu_27, %none_28 : !torch.float, !torch.int, !torch.int, !torch.Device, !torch.none -> !torch.vtensor<[],f32>
    %20 = torch.aten.where.self %18, %19, %13 : !torch.vtensor<[64,64],i1>, !torch.vtensor<[],f32>, !torch.vtensor<[64,64],f32> -> !torch.vtensor<[64,64],f32>
    %int5 = torch.constant.int 5
    %21 = torch.prims.convert_element_type %20, %int5 : !torch.vtensor<[64,64],f32>, !torch.int -> !torch.vtensor<[64,64],f16>
    %int0_29 = torch.constant.int 0
    %22 = torch.aten.unsqueeze %21, %int0_29 : !torch.vtensor<[64,64],f16>, !torch.int -> !torch.vtensor<[1,64,64],f16>
    %int1_30 = torch.constant.int 1
    %23 = torch.aten.unsqueeze %22, %int1_30 : !torch.vtensor<[1,64,64],f16>, !torch.int -> !torch.vtensor<[1,1,64,64],f16>
    %int2 = torch.constant.int 2
    %int0_31 = torch.constant.int 0
    %int9223372036854775807_32 = torch.constant.int 9223372036854775807
    %int1_33 = torch.constant.int 1
    %24 = torch.aten.slice.Tensor %23, %int2, %int0_31, %int9223372036854775807_32, %int1_33 : !torch.vtensor<[1,1,64,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,64,64],f16>
    %int3 = torch.constant.int 3
    %int0_34 = torch.constant.int 0
    %int9223372036854775807_35 = torch.constant.int 9223372036854775807
    %int1_36 = torch.constant.int 1
    %25 = torch.aten.slice.Tensor %24, %int3, %int0_34, %int9223372036854775807_35, %int1_36 : !torch.vtensor<[1,1,64,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,1,64,64],f16>
    %int1_37 = torch.constant.int 1
    %int1_38 = torch.constant.int 1
    %int64_39 = torch.constant.int 64
    %int64_40 = torch.constant.int 64
    %26 = torch.prim.ListConstruct %int1_37, %int1_38, %int64_39, %int64_40 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_41 = torch.constant.bool false
    %27 = torch.aten.expand %25, %26, %false_41 : !torch.vtensor<[1,1,64,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,1,64,64],f16>
    %int6_42 = torch.constant.int 6
    %28 = torch.prims.convert_element_type %9, %int6_42 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_43 = torch.constant.int 2
    %29 = torch.prim.ListConstruct %int2_43 : (!torch.int) -> !torch.list<int>
    %int0_44 = torch.constant.int 0
    %true = torch.constant.bool true
    %result0, %result1 = torch.aten.var_mean.correction %28, %29, %int0_44, %true : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05 = torch.constant.float 1.000000e-05
    %int1_45 = torch.constant.int 1
    %30 = torch.aten.add.Scalar %result0, %float1.000000e-05, %int1_45 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %31 = torch.aten.rsqrt %30 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_46 = torch.constant.int 1
    %32 = torch.aten.sub.Tensor %9, %result1, %int1_46 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %33 = torch.aten.mul.Tensor %32, %31 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.weight : tensor<1280xf16>
    %34 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %35 = torch.aten.mul.Tensor %33, %34 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.bias : tensor<1280xf16>
    %36 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_47 = torch.constant.int 1
    %37 = torch.aten.add.Tensor %35, %36, %int1_47 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_48 = torch.constant.int 5
    %38 = torch.prims.convert_element_type %37, %int5_48 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_49 = torch.constant.int 5
    %39 = torch.prims.convert_element_type %result1, %int5_49 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_50 = torch.constant.int 5
    %40 = torch.prims.convert_element_type %31, %int5_50 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_51 = torch.constant.int 64
    %int1280 = torch.constant.int 1280
    %41 = torch.prim.ListConstruct %int64_51, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %42 = torch.aten.view %38, %41 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %43 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_52 = torch.constant.int 0
    %int1_53 = torch.constant.int 1
    %44 = torch.aten.transpose.int %43, %int0_52, %int1_53 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.bias : tensor<1280xf16>
    %45 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_54 = torch.constant.int 6
    %46 = torch.prims.convert_element_type %45, %int6_54 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_55 = torch.constant.int 6
    %47 = torch.prims.convert_element_type %42, %int6_55 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_56 = torch.constant.int 6
    %48 = torch.prims.convert_element_type %44, %int6_56 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %49 = torch.aten.mm %47, %48 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_57 = torch.constant.int 1
    %50 = torch.aten.mul.Scalar %49, %int1_57 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_58 = torch.constant.int 1
    %51 = torch.aten.mul.Scalar %46, %int1_58 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_59 = torch.constant.int 1
    %52 = torch.aten.add.Tensor %50, %51, %int1_59 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_60 = torch.constant.int 5
    %53 = torch.prims.convert_element_type %52, %int5_60 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_61 = torch.constant.int 1
    %int64_62 = torch.constant.int 64
    %int1280_63 = torch.constant.int 1280
    %54 = torch.prim.ListConstruct %int1_61, %int64_62, %int1280_63 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %55 = torch.aten.view %53, %54 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01 = torch.constant.float 1.250000e-01
    %56 = torch.aten.mul.Scalar %55, %float1.250000e-01 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_64 = torch.constant.int 64
    %int1280_65 = torch.constant.int 1280
    %57 = torch.prim.ListConstruct %int64_64, %int1280_65 : (!torch.int, !torch.int) -> !torch.list<int>
    %58 = torch.aten.view %38, %57 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %59 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_66 = torch.constant.int 0
    %int1_67 = torch.constant.int 1
    %60 = torch.aten.transpose.int %59, %int0_66, %int1_67 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.bias : tensor<1280xf16>
    %61 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_68 = torch.constant.int 6
    %62 = torch.prims.convert_element_type %61, %int6_68 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_69 = torch.constant.int 6
    %63 = torch.prims.convert_element_type %58, %int6_69 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_70 = torch.constant.int 6
    %64 = torch.prims.convert_element_type %60, %int6_70 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %65 = torch.aten.mm %63, %64 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_71 = torch.constant.int 1
    %66 = torch.aten.mul.Scalar %65, %int1_71 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_72 = torch.constant.int 1
    %67 = torch.aten.mul.Scalar %62, %int1_72 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_73 = torch.constant.int 1
    %68 = torch.aten.add.Tensor %66, %67, %int1_73 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_74 = torch.constant.int 5
    %69 = torch.prims.convert_element_type %68, %int5_74 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_75 = torch.constant.int 1
    %int64_76 = torch.constant.int 64
    %int1280_77 = torch.constant.int 1280
    %70 = torch.prim.ListConstruct %int1_75, %int64_76, %int1280_77 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %71 = torch.aten.view %69, %70 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_78 = torch.constant.int 1
    %int-1_79 = torch.constant.int -1
    %int20 = torch.constant.int 20
    %int64_80 = torch.constant.int 64
    %72 = torch.prim.ListConstruct %int1_78, %int-1_79, %int20, %int64_80 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %73 = torch.aten.view %71, %72 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_81 = torch.constant.int 1
    %int2_82 = torch.constant.int 2
    %74 = torch.aten.transpose.int %73, %int1_81, %int2_82 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_83 = torch.constant.int 0
    %75 = torch.aten.clone %74, %int0_83 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_84 = torch.constant.int 64
    %int1280_85 = torch.constant.int 1280
    %76 = torch.prim.ListConstruct %int64_84, %int1280_85 : (!torch.int, !torch.int) -> !torch.list<int>
    %77 = torch.aten.view %38, %76 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %78 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_86 = torch.constant.int 0
    %int1_87 = torch.constant.int 1
    %79 = torch.aten.transpose.int %78, %int0_86, %int1_87 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.bias : tensor<1280xf16>
    %80 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_88 = torch.constant.int 6
    %81 = torch.prims.convert_element_type %80, %int6_88 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_89 = torch.constant.int 6
    %82 = torch.prims.convert_element_type %77, %int6_89 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_90 = torch.constant.int 6
    %83 = torch.prims.convert_element_type %79, %int6_90 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %84 = torch.aten.mm %82, %83 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_91 = torch.constant.int 1
    %85 = torch.aten.mul.Scalar %84, %int1_91 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_92 = torch.constant.int 1
    %86 = torch.aten.mul.Scalar %81, %int1_92 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_93 = torch.constant.int 1
    %87 = torch.aten.add.Tensor %85, %86, %int1_93 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_94 = torch.constant.int 5
    %88 = torch.prims.convert_element_type %87, %int5_94 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_95 = torch.constant.int 1
    %int64_96 = torch.constant.int 64
    %int1280_97 = torch.constant.int 1280
    %89 = torch.prim.ListConstruct %int1_95, %int64_96, %int1280_97 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %90 = torch.aten.view %88, %89 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_98 = torch.constant.int 1
    %int-1_99 = torch.constant.int -1
    %int20_100 = torch.constant.int 20
    %int64_101 = torch.constant.int 64
    %91 = torch.prim.ListConstruct %int1_98, %int-1_99, %int20_100, %int64_101 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %92 = torch.aten.view %90, %91 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_102 = torch.constant.int 1
    %int2_103 = torch.constant.int 2
    %93 = torch.aten.transpose.int %92, %int1_102, %int2_103 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_104 = torch.constant.int 0
    %94 = torch.aten.clone %93, %int0_104 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_105 = torch.constant.int 1
    %int64_106 = torch.constant.int 64
    %int20_107 = torch.constant.int 20
    %int64_108 = torch.constant.int 64
    %95 = torch.prim.ListConstruct %int1_105, %int64_106, %int20_107, %int64_108 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %96 = torch.aten.view %56, %95 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_109 = torch.constant.int 1
    %int2_110 = torch.constant.int 2
    %97 = torch.aten.transpose.int %96, %int1_109, %int2_110 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_111 = torch.constant.int 0
    %98 = torch.aten.clone %97, %int0_111 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_112 = torch.constant.int 20
    %int-1_113 = torch.constant.int -1
    %int64_114 = torch.constant.int 64
    %99 = torch.prim.ListConstruct %int20_112, %int-1_113, %int64_114 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %100 = torch.aten.view %98, %99 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_115 = torch.constant.int 20
    %int-1_116 = torch.constant.int -1
    %int64_117 = torch.constant.int 64
    %101 = torch.prim.ListConstruct %int20_115, %int-1_116, %int64_117 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %102 = torch.aten.view %75, %101 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_118 = torch.constant.int 20
    %int-1_119 = torch.constant.int -1
    %int64_120 = torch.constant.int 64
    %103 = torch.prim.ListConstruct %int20_118, %int-1_119, %int64_120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %104 = torch.aten.view %94, %103 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_121 = torch.constant.int 1
    %int2_122 = torch.constant.int 2
    %105 = torch.aten.transpose.int %102, %int1_121, %int2_122 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %106 = torch.aten.bmm %100, %105 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_123 = torch.constant.int 1
    %int20_124 = torch.constant.int 20
    %int64_125 = torch.constant.int 64
    %int64_126 = torch.constant.int 64
    %107 = torch.prim.ListConstruct %int1_123, %int20_124, %int64_125, %int64_126 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %108 = torch.aten.view %106, %107 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_127 = torch.constant.int 1
    %109 = torch.aten.add.Tensor %108, %27, %int1_127 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_128 = torch.constant.int 20
    %int64_129 = torch.constant.int 64
    %int64_130 = torch.constant.int 64
    %110 = torch.prim.ListConstruct %int20_128, %int64_129, %int64_130 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %111 = torch.aten.view %109, %110 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_131 = torch.constant.int -1
    %false_132 = torch.constant.bool false
    %112 = torch.aten._softmax %111, %int-1_131, %false_132 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %113 = torch.aten.detach %112 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_133 = torch.constant.none
    %114 = torch.aten.clone %112, %none_133 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %115 = torch.aten.bmm %114, %104 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_134 = torch.constant.int 1
    %int20_135 = torch.constant.int 20
    %int64_136 = torch.constant.int 64
    %int64_137 = torch.constant.int 64
    %116 = torch.prim.ListConstruct %int1_134, %int20_135, %int64_136, %int64_137 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %117 = torch.aten.view %115, %116 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_138 = torch.constant.int 1
    %int2_139 = torch.constant.int 2
    %118 = torch.aten.transpose.int %117, %int1_138, %int2_139 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_140 = torch.constant.int 0
    %119 = torch.aten.clone %118, %int0_140 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_141 = torch.constant.int 1
    %int64_142 = torch.constant.int 64
    %int1280_143 = torch.constant.int 1280
    %120 = torch.prim.ListConstruct %int1_141, %int64_142, %int1280_143 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %121 = torch.aten._unsafe_view %119, %120 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_144 = torch.constant.int 64
    %int1280_145 = torch.constant.int 1280
    %122 = torch.prim.ListConstruct %int64_144, %int1280_145 : (!torch.int, !torch.int) -> !torch.list<int>
    %123 = torch.aten.view %121, %122 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %124 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_146 = torch.constant.int 0
    %int1_147 = torch.constant.int 1
    %125 = torch.aten.transpose.int %124, %int0_146, %int1_147 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.bias : tensor<1280xf16>
    %126 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_148 = torch.constant.int 6
    %127 = torch.prims.convert_element_type %126, %int6_148 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_149 = torch.constant.int 6
    %128 = torch.prims.convert_element_type %123, %int6_149 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_150 = torch.constant.int 6
    %129 = torch.prims.convert_element_type %125, %int6_150 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %130 = torch.aten.mm %128, %129 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_151 = torch.constant.int 1
    %131 = torch.aten.mul.Scalar %130, %int1_151 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_152 = torch.constant.int 1
    %132 = torch.aten.mul.Scalar %127, %int1_152 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_153 = torch.constant.int 1
    %133 = torch.aten.add.Tensor %131, %132, %int1_153 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_154 = torch.constant.int 5
    %134 = torch.prims.convert_element_type %133, %int5_154 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_155 = torch.constant.int 1
    %int64_156 = torch.constant.int 64
    %int1280_157 = torch.constant.int 1280
    %135 = torch.prim.ListConstruct %int1_155, %int64_156, %int1280_157 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %136 = torch.aten.view %134, %135 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_158 = torch.constant.int 1
    %137 = torch.aten.add.Tensor %9, %136, %int1_158 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_159 = torch.constant.int 6
    %138 = torch.prims.convert_element_type %137, %int6_159 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_160 = torch.constant.int 2
    %139 = torch.prim.ListConstruct %int2_160 : (!torch.int) -> !torch.list<int>
    %int0_161 = torch.constant.int 0
    %true_162 = torch.constant.bool true
    %result0_163, %result1_164 = torch.aten.var_mean.correction %138, %139, %int0_161, %true_162 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_165 = torch.constant.float 1.000000e-05
    %int1_166 = torch.constant.int 1
    %140 = torch.aten.add.Scalar %result0_163, %float1.000000e-05_165, %int1_166 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %141 = torch.aten.rsqrt %140 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_167 = torch.constant.int 1
    %142 = torch.aten.sub.Tensor %137, %result1_164, %int1_167 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %143 = torch.aten.mul.Tensor %142, %141 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.weight : tensor<1280xf16>
    %144 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %145 = torch.aten.mul.Tensor %143, %144 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.bias : tensor<1280xf16>
    %146 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_168 = torch.constant.int 1
    %147 = torch.aten.add.Tensor %145, %146, %int1_168 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_169 = torch.constant.int 5
    %148 = torch.prims.convert_element_type %147, %int5_169 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_170 = torch.constant.int 5
    %149 = torch.prims.convert_element_type %result1_164, %int5_170 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_171 = torch.constant.int 5
    %150 = torch.prims.convert_element_type %141, %int5_171 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_172 = torch.constant.int 64
    %int1280_173 = torch.constant.int 1280
    %151 = torch.prim.ListConstruct %int64_172, %int1280_173 : (!torch.int, !torch.int) -> !torch.list<int>
    %152 = torch.aten.view %148, %151 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.weight : tensor<5120x1280xf16>
    %153 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_174 = torch.constant.int 0
    %int1_175 = torch.constant.int 1
    %154 = torch.aten.transpose.int %153, %int0_174, %int1_175 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.bias : tensor<5120xf16>
    %155 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_176 = torch.constant.int 6
    %156 = torch.prims.convert_element_type %155, %int6_176 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_177 = torch.constant.int 6
    %157 = torch.prims.convert_element_type %152, %int6_177 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_178 = torch.constant.int 6
    %158 = torch.prims.convert_element_type %154, %int6_178 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %159 = torch.aten.mm %157, %158 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_179 = torch.constant.int 1
    %160 = torch.aten.mul.Scalar %159, %int1_179 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_180 = torch.constant.int 1
    %161 = torch.aten.mul.Scalar %156, %int1_180 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_181 = torch.constant.int 1
    %162 = torch.aten.add.Tensor %160, %161, %int1_181 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_182 = torch.constant.int 5
    %163 = torch.prims.convert_element_type %162, %int5_182 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_183 = torch.constant.int 1
    %int64_184 = torch.constant.int 64
    %int5120 = torch.constant.int 5120
    %164 = torch.prim.ListConstruct %int1_183, %int64_184, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %165 = torch.aten.view %163, %164 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str = torch.constant.str "none"
    %166 = torch.aten.gelu %165, %str : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_185 = torch.constant.int 64
    %int5120_186 = torch.constant.int 5120
    %167 = torch.prim.ListConstruct %int64_185, %int5120_186 : (!torch.int, !torch.int) -> !torch.list<int>
    %168 = torch.aten.view %166, %167 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.weight : tensor<1280x5120xf16>
    %169 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_187 = torch.constant.int 0
    %int1_188 = torch.constant.int 1
    %170 = torch.aten.transpose.int %169, %int0_187, %int1_188 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.bias : tensor<1280xf16>
    %171 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.0.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_189 = torch.constant.int 6
    %172 = torch.prims.convert_element_type %171, %int6_189 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_190 = torch.constant.int 6
    %173 = torch.prims.convert_element_type %168, %int6_190 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_191 = torch.constant.int 6
    %174 = torch.prims.convert_element_type %170, %int6_191 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %175 = torch.aten.mm %173, %174 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_192 = torch.constant.int 1
    %176 = torch.aten.mul.Scalar %175, %int1_192 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_193 = torch.constant.int 1
    %177 = torch.aten.mul.Scalar %172, %int1_193 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_194 = torch.constant.int 1
    %178 = torch.aten.add.Tensor %176, %177, %int1_194 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_195 = torch.constant.int 5
    %179 = torch.prims.convert_element_type %178, %int5_195 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_196 = torch.constant.int 1
    %int64_197 = torch.constant.int 64
    %int1280_198 = torch.constant.int 1280
    %180 = torch.prim.ListConstruct %int1_196, %int64_197, %int1280_198 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %181 = torch.aten.view %179, %180 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_199 = torch.constant.int 1
    %182 = torch.aten.add.Tensor %137, %181, %int1_199 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_200 = torch.constant.int 6
    %183 = torch.prims.convert_element_type %182, %int6_200 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_201 = torch.constant.int 2
    %184 = torch.prim.ListConstruct %int2_201 : (!torch.int) -> !torch.list<int>
    %int0_202 = torch.constant.int 0
    %true_203 = torch.constant.bool true
    %result0_204, %result1_205 = torch.aten.var_mean.correction %183, %184, %int0_202, %true_203 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_206 = torch.constant.float 1.000000e-05
    %int1_207 = torch.constant.int 1
    %185 = torch.aten.add.Scalar %result0_204, %float1.000000e-05_206, %int1_207 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %186 = torch.aten.rsqrt %185 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_208 = torch.constant.int 1
    %187 = torch.aten.sub.Tensor %182, %result1_205, %int1_208 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %188 = torch.aten.mul.Tensor %187, %186 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.weight : tensor<1280xf16>
    %189 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %190 = torch.aten.mul.Tensor %188, %189 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.bias : tensor<1280xf16>
    %191 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_209 = torch.constant.int 1
    %192 = torch.aten.add.Tensor %190, %191, %int1_209 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_210 = torch.constant.int 5
    %193 = torch.prims.convert_element_type %192, %int5_210 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_211 = torch.constant.int 5
    %194 = torch.prims.convert_element_type %result1_205, %int5_211 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_212 = torch.constant.int 5
    %195 = torch.prims.convert_element_type %186, %int5_212 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_213 = torch.constant.int 64
    %int1280_214 = torch.constant.int 1280
    %196 = torch.prim.ListConstruct %int64_213, %int1280_214 : (!torch.int, !torch.int) -> !torch.list<int>
    %197 = torch.aten.view %193, %196 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %198 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_215 = torch.constant.int 0
    %int1_216 = torch.constant.int 1
    %199 = torch.aten.transpose.int %198, %int0_215, %int1_216 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.bias : tensor<1280xf16>
    %200 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_217 = torch.constant.int 6
    %201 = torch.prims.convert_element_type %200, %int6_217 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_218 = torch.constant.int 6
    %202 = torch.prims.convert_element_type %197, %int6_218 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_219 = torch.constant.int 6
    %203 = torch.prims.convert_element_type %199, %int6_219 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %204 = torch.aten.mm %202, %203 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_220 = torch.constant.int 1
    %205 = torch.aten.mul.Scalar %204, %int1_220 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_221 = torch.constant.int 1
    %206 = torch.aten.mul.Scalar %201, %int1_221 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_222 = torch.constant.int 1
    %207 = torch.aten.add.Tensor %205, %206, %int1_222 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_223 = torch.constant.int 5
    %208 = torch.prims.convert_element_type %207, %int5_223 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_224 = torch.constant.int 1
    %int64_225 = torch.constant.int 64
    %int1280_226 = torch.constant.int 1280
    %209 = torch.prim.ListConstruct %int1_224, %int64_225, %int1280_226 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %210 = torch.aten.view %208, %209 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_227 = torch.constant.float 1.250000e-01
    %211 = torch.aten.mul.Scalar %210, %float1.250000e-01_227 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_228 = torch.constant.int 64
    %int1280_229 = torch.constant.int 1280
    %212 = torch.prim.ListConstruct %int64_228, %int1280_229 : (!torch.int, !torch.int) -> !torch.list<int>
    %213 = torch.aten.view %193, %212 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %214 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_230 = torch.constant.int 0
    %int1_231 = torch.constant.int 1
    %215 = torch.aten.transpose.int %214, %int0_230, %int1_231 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.bias : tensor<1280xf16>
    %216 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_232 = torch.constant.int 6
    %217 = torch.prims.convert_element_type %216, %int6_232 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_233 = torch.constant.int 6
    %218 = torch.prims.convert_element_type %213, %int6_233 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_234 = torch.constant.int 6
    %219 = torch.prims.convert_element_type %215, %int6_234 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %220 = torch.aten.mm %218, %219 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_235 = torch.constant.int 1
    %221 = torch.aten.mul.Scalar %220, %int1_235 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_236 = torch.constant.int 1
    %222 = torch.aten.mul.Scalar %217, %int1_236 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_237 = torch.constant.int 1
    %223 = torch.aten.add.Tensor %221, %222, %int1_237 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_238 = torch.constant.int 5
    %224 = torch.prims.convert_element_type %223, %int5_238 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_239 = torch.constant.int 1
    %int64_240 = torch.constant.int 64
    %int1280_241 = torch.constant.int 1280
    %225 = torch.prim.ListConstruct %int1_239, %int64_240, %int1280_241 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %226 = torch.aten.view %224, %225 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_242 = torch.constant.int 1
    %int-1_243 = torch.constant.int -1
    %int20_244 = torch.constant.int 20
    %int64_245 = torch.constant.int 64
    %227 = torch.prim.ListConstruct %int1_242, %int-1_243, %int20_244, %int64_245 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %228 = torch.aten.view %226, %227 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_246 = torch.constant.int 1
    %int2_247 = torch.constant.int 2
    %229 = torch.aten.transpose.int %228, %int1_246, %int2_247 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_248 = torch.constant.int 0
    %230 = torch.aten.clone %229, %int0_248 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_249 = torch.constant.int 64
    %int1280_250 = torch.constant.int 1280
    %231 = torch.prim.ListConstruct %int64_249, %int1280_250 : (!torch.int, !torch.int) -> !torch.list<int>
    %232 = torch.aten.view %193, %231 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %233 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_251 = torch.constant.int 0
    %int1_252 = torch.constant.int 1
    %234 = torch.aten.transpose.int %233, %int0_251, %int1_252 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.bias : tensor<1280xf16>
    %235 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_253 = torch.constant.int 6
    %236 = torch.prims.convert_element_type %235, %int6_253 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_254 = torch.constant.int 6
    %237 = torch.prims.convert_element_type %232, %int6_254 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_255 = torch.constant.int 6
    %238 = torch.prims.convert_element_type %234, %int6_255 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %239 = torch.aten.mm %237, %238 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_256 = torch.constant.int 1
    %240 = torch.aten.mul.Scalar %239, %int1_256 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_257 = torch.constant.int 1
    %241 = torch.aten.mul.Scalar %236, %int1_257 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_258 = torch.constant.int 1
    %242 = torch.aten.add.Tensor %240, %241, %int1_258 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_259 = torch.constant.int 5
    %243 = torch.prims.convert_element_type %242, %int5_259 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_260 = torch.constant.int 1
    %int64_261 = torch.constant.int 64
    %int1280_262 = torch.constant.int 1280
    %244 = torch.prim.ListConstruct %int1_260, %int64_261, %int1280_262 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %245 = torch.aten.view %243, %244 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_263 = torch.constant.int 1
    %int-1_264 = torch.constant.int -1
    %int20_265 = torch.constant.int 20
    %int64_266 = torch.constant.int 64
    %246 = torch.prim.ListConstruct %int1_263, %int-1_264, %int20_265, %int64_266 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %247 = torch.aten.view %245, %246 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_267 = torch.constant.int 1
    %int2_268 = torch.constant.int 2
    %248 = torch.aten.transpose.int %247, %int1_267, %int2_268 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_269 = torch.constant.int 0
    %249 = torch.aten.clone %248, %int0_269 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_270 = torch.constant.int 1
    %int64_271 = torch.constant.int 64
    %int20_272 = torch.constant.int 20
    %int64_273 = torch.constant.int 64
    %250 = torch.prim.ListConstruct %int1_270, %int64_271, %int20_272, %int64_273 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %251 = torch.aten.view %211, %250 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_274 = torch.constant.int 1
    %int2_275 = torch.constant.int 2
    %252 = torch.aten.transpose.int %251, %int1_274, %int2_275 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_276 = torch.constant.int 0
    %253 = torch.aten.clone %252, %int0_276 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_277 = torch.constant.int 20
    %int-1_278 = torch.constant.int -1
    %int64_279 = torch.constant.int 64
    %254 = torch.prim.ListConstruct %int20_277, %int-1_278, %int64_279 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %255 = torch.aten.view %253, %254 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_280 = torch.constant.int 20
    %int-1_281 = torch.constant.int -1
    %int64_282 = torch.constant.int 64
    %256 = torch.prim.ListConstruct %int20_280, %int-1_281, %int64_282 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %257 = torch.aten.view %230, %256 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_283 = torch.constant.int 20
    %int-1_284 = torch.constant.int -1
    %int64_285 = torch.constant.int 64
    %258 = torch.prim.ListConstruct %int20_283, %int-1_284, %int64_285 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %259 = torch.aten.view %249, %258 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_286 = torch.constant.int 1
    %int2_287 = torch.constant.int 2
    %260 = torch.aten.transpose.int %257, %int1_286, %int2_287 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %261 = torch.aten.bmm %255, %260 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_288 = torch.constant.int 1
    %int20_289 = torch.constant.int 20
    %int64_290 = torch.constant.int 64
    %int64_291 = torch.constant.int 64
    %262 = torch.prim.ListConstruct %int1_288, %int20_289, %int64_290, %int64_291 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %263 = torch.aten.view %261, %262 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_292 = torch.constant.int 1
    %264 = torch.aten.add.Tensor %263, %27, %int1_292 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_293 = torch.constant.int 20
    %int64_294 = torch.constant.int 64
    %int64_295 = torch.constant.int 64
    %265 = torch.prim.ListConstruct %int20_293, %int64_294, %int64_295 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %266 = torch.aten.view %264, %265 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_296 = torch.constant.int -1
    %false_297 = torch.constant.bool false
    %267 = torch.aten._softmax %266, %int-1_296, %false_297 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %268 = torch.aten.detach %267 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_298 = torch.constant.none
    %269 = torch.aten.clone %267, %none_298 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %270 = torch.aten.bmm %269, %259 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_299 = torch.constant.int 1
    %int20_300 = torch.constant.int 20
    %int64_301 = torch.constant.int 64
    %int64_302 = torch.constant.int 64
    %271 = torch.prim.ListConstruct %int1_299, %int20_300, %int64_301, %int64_302 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %272 = torch.aten.view %270, %271 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_303 = torch.constant.int 1
    %int2_304 = torch.constant.int 2
    %273 = torch.aten.transpose.int %272, %int1_303, %int2_304 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_305 = torch.constant.int 0
    %274 = torch.aten.clone %273, %int0_305 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_306 = torch.constant.int 1
    %int64_307 = torch.constant.int 64
    %int1280_308 = torch.constant.int 1280
    %275 = torch.prim.ListConstruct %int1_306, %int64_307, %int1280_308 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %276 = torch.aten._unsafe_view %274, %275 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_309 = torch.constant.int 64
    %int1280_310 = torch.constant.int 1280
    %277 = torch.prim.ListConstruct %int64_309, %int1280_310 : (!torch.int, !torch.int) -> !torch.list<int>
    %278 = torch.aten.view %276, %277 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %279 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_311 = torch.constant.int 0
    %int1_312 = torch.constant.int 1
    %280 = torch.aten.transpose.int %279, %int0_311, %int1_312 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.bias : tensor<1280xf16>
    %281 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_313 = torch.constant.int 6
    %282 = torch.prims.convert_element_type %281, %int6_313 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_314 = torch.constant.int 6
    %283 = torch.prims.convert_element_type %278, %int6_314 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_315 = torch.constant.int 6
    %284 = torch.prims.convert_element_type %280, %int6_315 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %285 = torch.aten.mm %283, %284 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_316 = torch.constant.int 1
    %286 = torch.aten.mul.Scalar %285, %int1_316 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_317 = torch.constant.int 1
    %287 = torch.aten.mul.Scalar %282, %int1_317 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_318 = torch.constant.int 1
    %288 = torch.aten.add.Tensor %286, %287, %int1_318 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_319 = torch.constant.int 5
    %289 = torch.prims.convert_element_type %288, %int5_319 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_320 = torch.constant.int 1
    %int64_321 = torch.constant.int 64
    %int1280_322 = torch.constant.int 1280
    %290 = torch.prim.ListConstruct %int1_320, %int64_321, %int1280_322 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %291 = torch.aten.view %289, %290 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_323 = torch.constant.int 1
    %292 = torch.aten.add.Tensor %182, %291, %int1_323 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_324 = torch.constant.int 6
    %293 = torch.prims.convert_element_type %292, %int6_324 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_325 = torch.constant.int 2
    %294 = torch.prim.ListConstruct %int2_325 : (!torch.int) -> !torch.list<int>
    %int0_326 = torch.constant.int 0
    %true_327 = torch.constant.bool true
    %result0_328, %result1_329 = torch.aten.var_mean.correction %293, %294, %int0_326, %true_327 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_330 = torch.constant.float 1.000000e-05
    %int1_331 = torch.constant.int 1
    %295 = torch.aten.add.Scalar %result0_328, %float1.000000e-05_330, %int1_331 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %296 = torch.aten.rsqrt %295 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_332 = torch.constant.int 1
    %297 = torch.aten.sub.Tensor %292, %result1_329, %int1_332 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %298 = torch.aten.mul.Tensor %297, %296 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.weight : tensor<1280xf16>
    %299 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %300 = torch.aten.mul.Tensor %298, %299 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.bias : tensor<1280xf16>
    %301 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_333 = torch.constant.int 1
    %302 = torch.aten.add.Tensor %300, %301, %int1_333 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_334 = torch.constant.int 5
    %303 = torch.prims.convert_element_type %302, %int5_334 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_335 = torch.constant.int 5
    %304 = torch.prims.convert_element_type %result1_329, %int5_335 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_336 = torch.constant.int 5
    %305 = torch.prims.convert_element_type %296, %int5_336 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_337 = torch.constant.int 64
    %int1280_338 = torch.constant.int 1280
    %306 = torch.prim.ListConstruct %int64_337, %int1280_338 : (!torch.int, !torch.int) -> !torch.list<int>
    %307 = torch.aten.view %303, %306 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.weight : tensor<5120x1280xf16>
    %308 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_339 = torch.constant.int 0
    %int1_340 = torch.constant.int 1
    %309 = torch.aten.transpose.int %308, %int0_339, %int1_340 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.bias : tensor<5120xf16>
    %310 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_341 = torch.constant.int 6
    %311 = torch.prims.convert_element_type %310, %int6_341 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_342 = torch.constant.int 6
    %312 = torch.prims.convert_element_type %307, %int6_342 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_343 = torch.constant.int 6
    %313 = torch.prims.convert_element_type %309, %int6_343 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %314 = torch.aten.mm %312, %313 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_344 = torch.constant.int 1
    %315 = torch.aten.mul.Scalar %314, %int1_344 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_345 = torch.constant.int 1
    %316 = torch.aten.mul.Scalar %311, %int1_345 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_346 = torch.constant.int 1
    %317 = torch.aten.add.Tensor %315, %316, %int1_346 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_347 = torch.constant.int 5
    %318 = torch.prims.convert_element_type %317, %int5_347 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_348 = torch.constant.int 1
    %int64_349 = torch.constant.int 64
    %int5120_350 = torch.constant.int 5120
    %319 = torch.prim.ListConstruct %int1_348, %int64_349, %int5120_350 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %320 = torch.aten.view %318, %319 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_351 = torch.constant.str "none"
    %321 = torch.aten.gelu %320, %str_351 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_352 = torch.constant.int 64
    %int5120_353 = torch.constant.int 5120
    %322 = torch.prim.ListConstruct %int64_352, %int5120_353 : (!torch.int, !torch.int) -> !torch.list<int>
    %323 = torch.aten.view %321, %322 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.weight : tensor<1280x5120xf16>
    %324 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_354 = torch.constant.int 0
    %int1_355 = torch.constant.int 1
    %325 = torch.aten.transpose.int %324, %int0_354, %int1_355 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.bias : tensor<1280xf16>
    %326 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.1.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_356 = torch.constant.int 6
    %327 = torch.prims.convert_element_type %326, %int6_356 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_357 = torch.constant.int 6
    %328 = torch.prims.convert_element_type %323, %int6_357 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_358 = torch.constant.int 6
    %329 = torch.prims.convert_element_type %325, %int6_358 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %330 = torch.aten.mm %328, %329 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_359 = torch.constant.int 1
    %331 = torch.aten.mul.Scalar %330, %int1_359 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_360 = torch.constant.int 1
    %332 = torch.aten.mul.Scalar %327, %int1_360 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_361 = torch.constant.int 1
    %333 = torch.aten.add.Tensor %331, %332, %int1_361 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_362 = torch.constant.int 5
    %334 = torch.prims.convert_element_type %333, %int5_362 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_363 = torch.constant.int 1
    %int64_364 = torch.constant.int 64
    %int1280_365 = torch.constant.int 1280
    %335 = torch.prim.ListConstruct %int1_363, %int64_364, %int1280_365 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %336 = torch.aten.view %334, %335 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_366 = torch.constant.int 1
    %337 = torch.aten.add.Tensor %292, %336, %int1_366 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_367 = torch.constant.int 6
    %338 = torch.prims.convert_element_type %337, %int6_367 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_368 = torch.constant.int 2
    %339 = torch.prim.ListConstruct %int2_368 : (!torch.int) -> !torch.list<int>
    %int0_369 = torch.constant.int 0
    %true_370 = torch.constant.bool true
    %result0_371, %result1_372 = torch.aten.var_mean.correction %338, %339, %int0_369, %true_370 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_373 = torch.constant.float 1.000000e-05
    %int1_374 = torch.constant.int 1
    %340 = torch.aten.add.Scalar %result0_371, %float1.000000e-05_373, %int1_374 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %341 = torch.aten.rsqrt %340 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_375 = torch.constant.int 1
    %342 = torch.aten.sub.Tensor %337, %result1_372, %int1_375 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %343 = torch.aten.mul.Tensor %342, %341 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.weight : tensor<1280xf16>
    %344 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %345 = torch.aten.mul.Tensor %343, %344 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.bias : tensor<1280xf16>
    %346 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_376 = torch.constant.int 1
    %347 = torch.aten.add.Tensor %345, %346, %int1_376 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_377 = torch.constant.int 5
    %348 = torch.prims.convert_element_type %347, %int5_377 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_378 = torch.constant.int 5
    %349 = torch.prims.convert_element_type %result1_372, %int5_378 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_379 = torch.constant.int 5
    %350 = torch.prims.convert_element_type %341, %int5_379 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_380 = torch.constant.int 64
    %int1280_381 = torch.constant.int 1280
    %351 = torch.prim.ListConstruct %int64_380, %int1280_381 : (!torch.int, !torch.int) -> !torch.list<int>
    %352 = torch.aten.view %348, %351 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %353 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_382 = torch.constant.int 0
    %int1_383 = torch.constant.int 1
    %354 = torch.aten.transpose.int %353, %int0_382, %int1_383 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.bias : tensor<1280xf16>
    %355 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_384 = torch.constant.int 6
    %356 = torch.prims.convert_element_type %355, %int6_384 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_385 = torch.constant.int 6
    %357 = torch.prims.convert_element_type %352, %int6_385 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_386 = torch.constant.int 6
    %358 = torch.prims.convert_element_type %354, %int6_386 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %359 = torch.aten.mm %357, %358 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_387 = torch.constant.int 1
    %360 = torch.aten.mul.Scalar %359, %int1_387 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_388 = torch.constant.int 1
    %361 = torch.aten.mul.Scalar %356, %int1_388 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_389 = torch.constant.int 1
    %362 = torch.aten.add.Tensor %360, %361, %int1_389 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_390 = torch.constant.int 5
    %363 = torch.prims.convert_element_type %362, %int5_390 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_391 = torch.constant.int 1
    %int64_392 = torch.constant.int 64
    %int1280_393 = torch.constant.int 1280
    %364 = torch.prim.ListConstruct %int1_391, %int64_392, %int1280_393 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %365 = torch.aten.view %363, %364 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_394 = torch.constant.float 1.250000e-01
    %366 = torch.aten.mul.Scalar %365, %float1.250000e-01_394 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_395 = torch.constant.int 64
    %int1280_396 = torch.constant.int 1280
    %367 = torch.prim.ListConstruct %int64_395, %int1280_396 : (!torch.int, !torch.int) -> !torch.list<int>
    %368 = torch.aten.view %348, %367 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %369 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_397 = torch.constant.int 0
    %int1_398 = torch.constant.int 1
    %370 = torch.aten.transpose.int %369, %int0_397, %int1_398 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.bias : tensor<1280xf16>
    %371 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_399 = torch.constant.int 6
    %372 = torch.prims.convert_element_type %371, %int6_399 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_400 = torch.constant.int 6
    %373 = torch.prims.convert_element_type %368, %int6_400 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_401 = torch.constant.int 6
    %374 = torch.prims.convert_element_type %370, %int6_401 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %375 = torch.aten.mm %373, %374 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_402 = torch.constant.int 1
    %376 = torch.aten.mul.Scalar %375, %int1_402 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_403 = torch.constant.int 1
    %377 = torch.aten.mul.Scalar %372, %int1_403 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_404 = torch.constant.int 1
    %378 = torch.aten.add.Tensor %376, %377, %int1_404 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_405 = torch.constant.int 5
    %379 = torch.prims.convert_element_type %378, %int5_405 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_406 = torch.constant.int 1
    %int64_407 = torch.constant.int 64
    %int1280_408 = torch.constant.int 1280
    %380 = torch.prim.ListConstruct %int1_406, %int64_407, %int1280_408 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %381 = torch.aten.view %379, %380 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_409 = torch.constant.int 1
    %int-1_410 = torch.constant.int -1
    %int20_411 = torch.constant.int 20
    %int64_412 = torch.constant.int 64
    %382 = torch.prim.ListConstruct %int1_409, %int-1_410, %int20_411, %int64_412 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %383 = torch.aten.view %381, %382 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_413 = torch.constant.int 1
    %int2_414 = torch.constant.int 2
    %384 = torch.aten.transpose.int %383, %int1_413, %int2_414 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_415 = torch.constant.int 0
    %385 = torch.aten.clone %384, %int0_415 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_416 = torch.constant.int 64
    %int1280_417 = torch.constant.int 1280
    %386 = torch.prim.ListConstruct %int64_416, %int1280_417 : (!torch.int, !torch.int) -> !torch.list<int>
    %387 = torch.aten.view %348, %386 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %388 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_418 = torch.constant.int 0
    %int1_419 = torch.constant.int 1
    %389 = torch.aten.transpose.int %388, %int0_418, %int1_419 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.bias : tensor<1280xf16>
    %390 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_420 = torch.constant.int 6
    %391 = torch.prims.convert_element_type %390, %int6_420 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_421 = torch.constant.int 6
    %392 = torch.prims.convert_element_type %387, %int6_421 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_422 = torch.constant.int 6
    %393 = torch.prims.convert_element_type %389, %int6_422 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %394 = torch.aten.mm %392, %393 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_423 = torch.constant.int 1
    %395 = torch.aten.mul.Scalar %394, %int1_423 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_424 = torch.constant.int 1
    %396 = torch.aten.mul.Scalar %391, %int1_424 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_425 = torch.constant.int 1
    %397 = torch.aten.add.Tensor %395, %396, %int1_425 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_426 = torch.constant.int 5
    %398 = torch.prims.convert_element_type %397, %int5_426 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_427 = torch.constant.int 1
    %int64_428 = torch.constant.int 64
    %int1280_429 = torch.constant.int 1280
    %399 = torch.prim.ListConstruct %int1_427, %int64_428, %int1280_429 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %400 = torch.aten.view %398, %399 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_430 = torch.constant.int 1
    %int-1_431 = torch.constant.int -1
    %int20_432 = torch.constant.int 20
    %int64_433 = torch.constant.int 64
    %401 = torch.prim.ListConstruct %int1_430, %int-1_431, %int20_432, %int64_433 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %402 = torch.aten.view %400, %401 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_434 = torch.constant.int 1
    %int2_435 = torch.constant.int 2
    %403 = torch.aten.transpose.int %402, %int1_434, %int2_435 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_436 = torch.constant.int 0
    %404 = torch.aten.clone %403, %int0_436 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_437 = torch.constant.int 1
    %int64_438 = torch.constant.int 64
    %int20_439 = torch.constant.int 20
    %int64_440 = torch.constant.int 64
    %405 = torch.prim.ListConstruct %int1_437, %int64_438, %int20_439, %int64_440 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %406 = torch.aten.view %366, %405 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_441 = torch.constant.int 1
    %int2_442 = torch.constant.int 2
    %407 = torch.aten.transpose.int %406, %int1_441, %int2_442 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_443 = torch.constant.int 0
    %408 = torch.aten.clone %407, %int0_443 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_444 = torch.constant.int 20
    %int-1_445 = torch.constant.int -1
    %int64_446 = torch.constant.int 64
    %409 = torch.prim.ListConstruct %int20_444, %int-1_445, %int64_446 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %410 = torch.aten.view %408, %409 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_447 = torch.constant.int 20
    %int-1_448 = torch.constant.int -1
    %int64_449 = torch.constant.int 64
    %411 = torch.prim.ListConstruct %int20_447, %int-1_448, %int64_449 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %412 = torch.aten.view %385, %411 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_450 = torch.constant.int 20
    %int-1_451 = torch.constant.int -1
    %int64_452 = torch.constant.int 64
    %413 = torch.prim.ListConstruct %int20_450, %int-1_451, %int64_452 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %414 = torch.aten.view %404, %413 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_453 = torch.constant.int 1
    %int2_454 = torch.constant.int 2
    %415 = torch.aten.transpose.int %412, %int1_453, %int2_454 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %416 = torch.aten.bmm %410, %415 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_455 = torch.constant.int 1
    %int20_456 = torch.constant.int 20
    %int64_457 = torch.constant.int 64
    %int64_458 = torch.constant.int 64
    %417 = torch.prim.ListConstruct %int1_455, %int20_456, %int64_457, %int64_458 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %418 = torch.aten.view %416, %417 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_459 = torch.constant.int 1
    %419 = torch.aten.add.Tensor %418, %27, %int1_459 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_460 = torch.constant.int 20
    %int64_461 = torch.constant.int 64
    %int64_462 = torch.constant.int 64
    %420 = torch.prim.ListConstruct %int20_460, %int64_461, %int64_462 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %421 = torch.aten.view %419, %420 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_463 = torch.constant.int -1
    %false_464 = torch.constant.bool false
    %422 = torch.aten._softmax %421, %int-1_463, %false_464 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %423 = torch.aten.detach %422 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_465 = torch.constant.none
    %424 = torch.aten.clone %422, %none_465 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %425 = torch.aten.bmm %424, %414 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_466 = torch.constant.int 1
    %int20_467 = torch.constant.int 20
    %int64_468 = torch.constant.int 64
    %int64_469 = torch.constant.int 64
    %426 = torch.prim.ListConstruct %int1_466, %int20_467, %int64_468, %int64_469 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %427 = torch.aten.view %425, %426 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_470 = torch.constant.int 1
    %int2_471 = torch.constant.int 2
    %428 = torch.aten.transpose.int %427, %int1_470, %int2_471 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_472 = torch.constant.int 0
    %429 = torch.aten.clone %428, %int0_472 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_473 = torch.constant.int 1
    %int64_474 = torch.constant.int 64
    %int1280_475 = torch.constant.int 1280
    %430 = torch.prim.ListConstruct %int1_473, %int64_474, %int1280_475 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %431 = torch.aten._unsafe_view %429, %430 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_476 = torch.constant.int 64
    %int1280_477 = torch.constant.int 1280
    %432 = torch.prim.ListConstruct %int64_476, %int1280_477 : (!torch.int, !torch.int) -> !torch.list<int>
    %433 = torch.aten.view %431, %432 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %434 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_478 = torch.constant.int 0
    %int1_479 = torch.constant.int 1
    %435 = torch.aten.transpose.int %434, %int0_478, %int1_479 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.bias : tensor<1280xf16>
    %436 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_480 = torch.constant.int 6
    %437 = torch.prims.convert_element_type %436, %int6_480 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_481 = torch.constant.int 6
    %438 = torch.prims.convert_element_type %433, %int6_481 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_482 = torch.constant.int 6
    %439 = torch.prims.convert_element_type %435, %int6_482 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %440 = torch.aten.mm %438, %439 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_483 = torch.constant.int 1
    %441 = torch.aten.mul.Scalar %440, %int1_483 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_484 = torch.constant.int 1
    %442 = torch.aten.mul.Scalar %437, %int1_484 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_485 = torch.constant.int 1
    %443 = torch.aten.add.Tensor %441, %442, %int1_485 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_486 = torch.constant.int 5
    %444 = torch.prims.convert_element_type %443, %int5_486 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_487 = torch.constant.int 1
    %int64_488 = torch.constant.int 64
    %int1280_489 = torch.constant.int 1280
    %445 = torch.prim.ListConstruct %int1_487, %int64_488, %int1280_489 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %446 = torch.aten.view %444, %445 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_490 = torch.constant.int 1
    %447 = torch.aten.add.Tensor %337, %446, %int1_490 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_491 = torch.constant.int 6
    %448 = torch.prims.convert_element_type %447, %int6_491 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_492 = torch.constant.int 2
    %449 = torch.prim.ListConstruct %int2_492 : (!torch.int) -> !torch.list<int>
    %int0_493 = torch.constant.int 0
    %true_494 = torch.constant.bool true
    %result0_495, %result1_496 = torch.aten.var_mean.correction %448, %449, %int0_493, %true_494 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_497 = torch.constant.float 1.000000e-05
    %int1_498 = torch.constant.int 1
    %450 = torch.aten.add.Scalar %result0_495, %float1.000000e-05_497, %int1_498 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %451 = torch.aten.rsqrt %450 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_499 = torch.constant.int 1
    %452 = torch.aten.sub.Tensor %447, %result1_496, %int1_499 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %453 = torch.aten.mul.Tensor %452, %451 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.weight : tensor<1280xf16>
    %454 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %455 = torch.aten.mul.Tensor %453, %454 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.bias : tensor<1280xf16>
    %456 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_500 = torch.constant.int 1
    %457 = torch.aten.add.Tensor %455, %456, %int1_500 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_501 = torch.constant.int 5
    %458 = torch.prims.convert_element_type %457, %int5_501 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_502 = torch.constant.int 5
    %459 = torch.prims.convert_element_type %result1_496, %int5_502 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_503 = torch.constant.int 5
    %460 = torch.prims.convert_element_type %451, %int5_503 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_504 = torch.constant.int 64
    %int1280_505 = torch.constant.int 1280
    %461 = torch.prim.ListConstruct %int64_504, %int1280_505 : (!torch.int, !torch.int) -> !torch.list<int>
    %462 = torch.aten.view %458, %461 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.weight : tensor<5120x1280xf16>
    %463 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_506 = torch.constant.int 0
    %int1_507 = torch.constant.int 1
    %464 = torch.aten.transpose.int %463, %int0_506, %int1_507 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.bias : tensor<5120xf16>
    %465 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_508 = torch.constant.int 6
    %466 = torch.prims.convert_element_type %465, %int6_508 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_509 = torch.constant.int 6
    %467 = torch.prims.convert_element_type %462, %int6_509 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_510 = torch.constant.int 6
    %468 = torch.prims.convert_element_type %464, %int6_510 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %469 = torch.aten.mm %467, %468 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_511 = torch.constant.int 1
    %470 = torch.aten.mul.Scalar %469, %int1_511 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_512 = torch.constant.int 1
    %471 = torch.aten.mul.Scalar %466, %int1_512 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_513 = torch.constant.int 1
    %472 = torch.aten.add.Tensor %470, %471, %int1_513 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_514 = torch.constant.int 5
    %473 = torch.prims.convert_element_type %472, %int5_514 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_515 = torch.constant.int 1
    %int64_516 = torch.constant.int 64
    %int5120_517 = torch.constant.int 5120
    %474 = torch.prim.ListConstruct %int1_515, %int64_516, %int5120_517 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %475 = torch.aten.view %473, %474 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_518 = torch.constant.str "none"
    %476 = torch.aten.gelu %475, %str_518 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_519 = torch.constant.int 64
    %int5120_520 = torch.constant.int 5120
    %477 = torch.prim.ListConstruct %int64_519, %int5120_520 : (!torch.int, !torch.int) -> !torch.list<int>
    %478 = torch.aten.view %476, %477 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.weight : tensor<1280x5120xf16>
    %479 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_521 = torch.constant.int 0
    %int1_522 = torch.constant.int 1
    %480 = torch.aten.transpose.int %479, %int0_521, %int1_522 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.bias : tensor<1280xf16>
    %481 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.2.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_523 = torch.constant.int 6
    %482 = torch.prims.convert_element_type %481, %int6_523 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_524 = torch.constant.int 6
    %483 = torch.prims.convert_element_type %478, %int6_524 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_525 = torch.constant.int 6
    %484 = torch.prims.convert_element_type %480, %int6_525 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %485 = torch.aten.mm %483, %484 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_526 = torch.constant.int 1
    %486 = torch.aten.mul.Scalar %485, %int1_526 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_527 = torch.constant.int 1
    %487 = torch.aten.mul.Scalar %482, %int1_527 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_528 = torch.constant.int 1
    %488 = torch.aten.add.Tensor %486, %487, %int1_528 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_529 = torch.constant.int 5
    %489 = torch.prims.convert_element_type %488, %int5_529 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_530 = torch.constant.int 1
    %int64_531 = torch.constant.int 64
    %int1280_532 = torch.constant.int 1280
    %490 = torch.prim.ListConstruct %int1_530, %int64_531, %int1280_532 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %491 = torch.aten.view %489, %490 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_533 = torch.constant.int 1
    %492 = torch.aten.add.Tensor %447, %491, %int1_533 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_534 = torch.constant.int 6
    %493 = torch.prims.convert_element_type %492, %int6_534 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_535 = torch.constant.int 2
    %494 = torch.prim.ListConstruct %int2_535 : (!torch.int) -> !torch.list<int>
    %int0_536 = torch.constant.int 0
    %true_537 = torch.constant.bool true
    %result0_538, %result1_539 = torch.aten.var_mean.correction %493, %494, %int0_536, %true_537 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_540 = torch.constant.float 1.000000e-05
    %int1_541 = torch.constant.int 1
    %495 = torch.aten.add.Scalar %result0_538, %float1.000000e-05_540, %int1_541 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %496 = torch.aten.rsqrt %495 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_542 = torch.constant.int 1
    %497 = torch.aten.sub.Tensor %492, %result1_539, %int1_542 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %498 = torch.aten.mul.Tensor %497, %496 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.weight : tensor<1280xf16>
    %499 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %500 = torch.aten.mul.Tensor %498, %499 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.bias : tensor<1280xf16>
    %501 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_543 = torch.constant.int 1
    %502 = torch.aten.add.Tensor %500, %501, %int1_543 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_544 = torch.constant.int 5
    %503 = torch.prims.convert_element_type %502, %int5_544 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_545 = torch.constant.int 5
    %504 = torch.prims.convert_element_type %result1_539, %int5_545 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_546 = torch.constant.int 5
    %505 = torch.prims.convert_element_type %496, %int5_546 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_547 = torch.constant.int 64
    %int1280_548 = torch.constant.int 1280
    %506 = torch.prim.ListConstruct %int64_547, %int1280_548 : (!torch.int, !torch.int) -> !torch.list<int>
    %507 = torch.aten.view %503, %506 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %508 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_549 = torch.constant.int 0
    %int1_550 = torch.constant.int 1
    %509 = torch.aten.transpose.int %508, %int0_549, %int1_550 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.bias : tensor<1280xf16>
    %510 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_551 = torch.constant.int 6
    %511 = torch.prims.convert_element_type %510, %int6_551 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_552 = torch.constant.int 6
    %512 = torch.prims.convert_element_type %507, %int6_552 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_553 = torch.constant.int 6
    %513 = torch.prims.convert_element_type %509, %int6_553 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %514 = torch.aten.mm %512, %513 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_554 = torch.constant.int 1
    %515 = torch.aten.mul.Scalar %514, %int1_554 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_555 = torch.constant.int 1
    %516 = torch.aten.mul.Scalar %511, %int1_555 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_556 = torch.constant.int 1
    %517 = torch.aten.add.Tensor %515, %516, %int1_556 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_557 = torch.constant.int 5
    %518 = torch.prims.convert_element_type %517, %int5_557 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_558 = torch.constant.int 1
    %int64_559 = torch.constant.int 64
    %int1280_560 = torch.constant.int 1280
    %519 = torch.prim.ListConstruct %int1_558, %int64_559, %int1280_560 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %520 = torch.aten.view %518, %519 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_561 = torch.constant.float 1.250000e-01
    %521 = torch.aten.mul.Scalar %520, %float1.250000e-01_561 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_562 = torch.constant.int 64
    %int1280_563 = torch.constant.int 1280
    %522 = torch.prim.ListConstruct %int64_562, %int1280_563 : (!torch.int, !torch.int) -> !torch.list<int>
    %523 = torch.aten.view %503, %522 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %524 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_564 = torch.constant.int 0
    %int1_565 = torch.constant.int 1
    %525 = torch.aten.transpose.int %524, %int0_564, %int1_565 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.bias : tensor<1280xf16>
    %526 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_566 = torch.constant.int 6
    %527 = torch.prims.convert_element_type %526, %int6_566 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_567 = torch.constant.int 6
    %528 = torch.prims.convert_element_type %523, %int6_567 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_568 = torch.constant.int 6
    %529 = torch.prims.convert_element_type %525, %int6_568 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %530 = torch.aten.mm %528, %529 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_569 = torch.constant.int 1
    %531 = torch.aten.mul.Scalar %530, %int1_569 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_570 = torch.constant.int 1
    %532 = torch.aten.mul.Scalar %527, %int1_570 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_571 = torch.constant.int 1
    %533 = torch.aten.add.Tensor %531, %532, %int1_571 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_572 = torch.constant.int 5
    %534 = torch.prims.convert_element_type %533, %int5_572 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_573 = torch.constant.int 1
    %int64_574 = torch.constant.int 64
    %int1280_575 = torch.constant.int 1280
    %535 = torch.prim.ListConstruct %int1_573, %int64_574, %int1280_575 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %536 = torch.aten.view %534, %535 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_576 = torch.constant.int 1
    %int-1_577 = torch.constant.int -1
    %int20_578 = torch.constant.int 20
    %int64_579 = torch.constant.int 64
    %537 = torch.prim.ListConstruct %int1_576, %int-1_577, %int20_578, %int64_579 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %538 = torch.aten.view %536, %537 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_580 = torch.constant.int 1
    %int2_581 = torch.constant.int 2
    %539 = torch.aten.transpose.int %538, %int1_580, %int2_581 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_582 = torch.constant.int 0
    %540 = torch.aten.clone %539, %int0_582 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_583 = torch.constant.int 64
    %int1280_584 = torch.constant.int 1280
    %541 = torch.prim.ListConstruct %int64_583, %int1280_584 : (!torch.int, !torch.int) -> !torch.list<int>
    %542 = torch.aten.view %503, %541 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %543 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_585 = torch.constant.int 0
    %int1_586 = torch.constant.int 1
    %544 = torch.aten.transpose.int %543, %int0_585, %int1_586 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.bias : tensor<1280xf16>
    %545 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_587 = torch.constant.int 6
    %546 = torch.prims.convert_element_type %545, %int6_587 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_588 = torch.constant.int 6
    %547 = torch.prims.convert_element_type %542, %int6_588 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_589 = torch.constant.int 6
    %548 = torch.prims.convert_element_type %544, %int6_589 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %549 = torch.aten.mm %547, %548 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_590 = torch.constant.int 1
    %550 = torch.aten.mul.Scalar %549, %int1_590 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_591 = torch.constant.int 1
    %551 = torch.aten.mul.Scalar %546, %int1_591 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_592 = torch.constant.int 1
    %552 = torch.aten.add.Tensor %550, %551, %int1_592 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_593 = torch.constant.int 5
    %553 = torch.prims.convert_element_type %552, %int5_593 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_594 = torch.constant.int 1
    %int64_595 = torch.constant.int 64
    %int1280_596 = torch.constant.int 1280
    %554 = torch.prim.ListConstruct %int1_594, %int64_595, %int1280_596 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %555 = torch.aten.view %553, %554 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_597 = torch.constant.int 1
    %int-1_598 = torch.constant.int -1
    %int20_599 = torch.constant.int 20
    %int64_600 = torch.constant.int 64
    %556 = torch.prim.ListConstruct %int1_597, %int-1_598, %int20_599, %int64_600 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %557 = torch.aten.view %555, %556 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_601 = torch.constant.int 1
    %int2_602 = torch.constant.int 2
    %558 = torch.aten.transpose.int %557, %int1_601, %int2_602 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_603 = torch.constant.int 0
    %559 = torch.aten.clone %558, %int0_603 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_604 = torch.constant.int 1
    %int64_605 = torch.constant.int 64
    %int20_606 = torch.constant.int 20
    %int64_607 = torch.constant.int 64
    %560 = torch.prim.ListConstruct %int1_604, %int64_605, %int20_606, %int64_607 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %561 = torch.aten.view %521, %560 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_608 = torch.constant.int 1
    %int2_609 = torch.constant.int 2
    %562 = torch.aten.transpose.int %561, %int1_608, %int2_609 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_610 = torch.constant.int 0
    %563 = torch.aten.clone %562, %int0_610 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_611 = torch.constant.int 20
    %int-1_612 = torch.constant.int -1
    %int64_613 = torch.constant.int 64
    %564 = torch.prim.ListConstruct %int20_611, %int-1_612, %int64_613 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %565 = torch.aten.view %563, %564 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_614 = torch.constant.int 20
    %int-1_615 = torch.constant.int -1
    %int64_616 = torch.constant.int 64
    %566 = torch.prim.ListConstruct %int20_614, %int-1_615, %int64_616 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %567 = torch.aten.view %540, %566 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_617 = torch.constant.int 20
    %int-1_618 = torch.constant.int -1
    %int64_619 = torch.constant.int 64
    %568 = torch.prim.ListConstruct %int20_617, %int-1_618, %int64_619 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %569 = torch.aten.view %559, %568 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_620 = torch.constant.int 1
    %int2_621 = torch.constant.int 2
    %570 = torch.aten.transpose.int %567, %int1_620, %int2_621 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %571 = torch.aten.bmm %565, %570 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_622 = torch.constant.int 1
    %int20_623 = torch.constant.int 20
    %int64_624 = torch.constant.int 64
    %int64_625 = torch.constant.int 64
    %572 = torch.prim.ListConstruct %int1_622, %int20_623, %int64_624, %int64_625 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %573 = torch.aten.view %571, %572 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_626 = torch.constant.int 1
    %574 = torch.aten.add.Tensor %573, %27, %int1_626 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_627 = torch.constant.int 20
    %int64_628 = torch.constant.int 64
    %int64_629 = torch.constant.int 64
    %575 = torch.prim.ListConstruct %int20_627, %int64_628, %int64_629 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %576 = torch.aten.view %574, %575 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_630 = torch.constant.int -1
    %false_631 = torch.constant.bool false
    %577 = torch.aten._softmax %576, %int-1_630, %false_631 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %578 = torch.aten.detach %577 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_632 = torch.constant.none
    %579 = torch.aten.clone %577, %none_632 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %580 = torch.aten.bmm %579, %569 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_633 = torch.constant.int 1
    %int20_634 = torch.constant.int 20
    %int64_635 = torch.constant.int 64
    %int64_636 = torch.constant.int 64
    %581 = torch.prim.ListConstruct %int1_633, %int20_634, %int64_635, %int64_636 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %582 = torch.aten.view %580, %581 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_637 = torch.constant.int 1
    %int2_638 = torch.constant.int 2
    %583 = torch.aten.transpose.int %582, %int1_637, %int2_638 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_639 = torch.constant.int 0
    %584 = torch.aten.clone %583, %int0_639 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_640 = torch.constant.int 1
    %int64_641 = torch.constant.int 64
    %int1280_642 = torch.constant.int 1280
    %585 = torch.prim.ListConstruct %int1_640, %int64_641, %int1280_642 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %586 = torch.aten._unsafe_view %584, %585 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_643 = torch.constant.int 64
    %int1280_644 = torch.constant.int 1280
    %587 = torch.prim.ListConstruct %int64_643, %int1280_644 : (!torch.int, !torch.int) -> !torch.list<int>
    %588 = torch.aten.view %586, %587 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %589 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_645 = torch.constant.int 0
    %int1_646 = torch.constant.int 1
    %590 = torch.aten.transpose.int %589, %int0_645, %int1_646 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.bias : tensor<1280xf16>
    %591 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_647 = torch.constant.int 6
    %592 = torch.prims.convert_element_type %591, %int6_647 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_648 = torch.constant.int 6
    %593 = torch.prims.convert_element_type %588, %int6_648 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_649 = torch.constant.int 6
    %594 = torch.prims.convert_element_type %590, %int6_649 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %595 = torch.aten.mm %593, %594 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_650 = torch.constant.int 1
    %596 = torch.aten.mul.Scalar %595, %int1_650 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_651 = torch.constant.int 1
    %597 = torch.aten.mul.Scalar %592, %int1_651 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_652 = torch.constant.int 1
    %598 = torch.aten.add.Tensor %596, %597, %int1_652 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_653 = torch.constant.int 5
    %599 = torch.prims.convert_element_type %598, %int5_653 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_654 = torch.constant.int 1
    %int64_655 = torch.constant.int 64
    %int1280_656 = torch.constant.int 1280
    %600 = torch.prim.ListConstruct %int1_654, %int64_655, %int1280_656 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %601 = torch.aten.view %599, %600 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_657 = torch.constant.int 1
    %602 = torch.aten.add.Tensor %492, %601, %int1_657 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_658 = torch.constant.int 6
    %603 = torch.prims.convert_element_type %602, %int6_658 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_659 = torch.constant.int 2
    %604 = torch.prim.ListConstruct %int2_659 : (!torch.int) -> !torch.list<int>
    %int0_660 = torch.constant.int 0
    %true_661 = torch.constant.bool true
    %result0_662, %result1_663 = torch.aten.var_mean.correction %603, %604, %int0_660, %true_661 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_664 = torch.constant.float 1.000000e-05
    %int1_665 = torch.constant.int 1
    %605 = torch.aten.add.Scalar %result0_662, %float1.000000e-05_664, %int1_665 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %606 = torch.aten.rsqrt %605 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_666 = torch.constant.int 1
    %607 = torch.aten.sub.Tensor %602, %result1_663, %int1_666 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %608 = torch.aten.mul.Tensor %607, %606 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.weight : tensor<1280xf16>
    %609 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %610 = torch.aten.mul.Tensor %608, %609 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.bias : tensor<1280xf16>
    %611 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_667 = torch.constant.int 1
    %612 = torch.aten.add.Tensor %610, %611, %int1_667 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_668 = torch.constant.int 5
    %613 = torch.prims.convert_element_type %612, %int5_668 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_669 = torch.constant.int 5
    %614 = torch.prims.convert_element_type %result1_663, %int5_669 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_670 = torch.constant.int 5
    %615 = torch.prims.convert_element_type %606, %int5_670 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_671 = torch.constant.int 64
    %int1280_672 = torch.constant.int 1280
    %616 = torch.prim.ListConstruct %int64_671, %int1280_672 : (!torch.int, !torch.int) -> !torch.list<int>
    %617 = torch.aten.view %613, %616 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.weight : tensor<5120x1280xf16>
    %618 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_673 = torch.constant.int 0
    %int1_674 = torch.constant.int 1
    %619 = torch.aten.transpose.int %618, %int0_673, %int1_674 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.bias : tensor<5120xf16>
    %620 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_675 = torch.constant.int 6
    %621 = torch.prims.convert_element_type %620, %int6_675 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_676 = torch.constant.int 6
    %622 = torch.prims.convert_element_type %617, %int6_676 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_677 = torch.constant.int 6
    %623 = torch.prims.convert_element_type %619, %int6_677 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %624 = torch.aten.mm %622, %623 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_678 = torch.constant.int 1
    %625 = torch.aten.mul.Scalar %624, %int1_678 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_679 = torch.constant.int 1
    %626 = torch.aten.mul.Scalar %621, %int1_679 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_680 = torch.constant.int 1
    %627 = torch.aten.add.Tensor %625, %626, %int1_680 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_681 = torch.constant.int 5
    %628 = torch.prims.convert_element_type %627, %int5_681 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_682 = torch.constant.int 1
    %int64_683 = torch.constant.int 64
    %int5120_684 = torch.constant.int 5120
    %629 = torch.prim.ListConstruct %int1_682, %int64_683, %int5120_684 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %630 = torch.aten.view %628, %629 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_685 = torch.constant.str "none"
    %631 = torch.aten.gelu %630, %str_685 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_686 = torch.constant.int 64
    %int5120_687 = torch.constant.int 5120
    %632 = torch.prim.ListConstruct %int64_686, %int5120_687 : (!torch.int, !torch.int) -> !torch.list<int>
    %633 = torch.aten.view %631, %632 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.weight : tensor<1280x5120xf16>
    %634 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_688 = torch.constant.int 0
    %int1_689 = torch.constant.int 1
    %635 = torch.aten.transpose.int %634, %int0_688, %int1_689 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.bias : tensor<1280xf16>
    %636 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.3.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_690 = torch.constant.int 6
    %637 = torch.prims.convert_element_type %636, %int6_690 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_691 = torch.constant.int 6
    %638 = torch.prims.convert_element_type %633, %int6_691 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_692 = torch.constant.int 6
    %639 = torch.prims.convert_element_type %635, %int6_692 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %640 = torch.aten.mm %638, %639 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_693 = torch.constant.int 1
    %641 = torch.aten.mul.Scalar %640, %int1_693 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_694 = torch.constant.int 1
    %642 = torch.aten.mul.Scalar %637, %int1_694 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_695 = torch.constant.int 1
    %643 = torch.aten.add.Tensor %641, %642, %int1_695 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_696 = torch.constant.int 5
    %644 = torch.prims.convert_element_type %643, %int5_696 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_697 = torch.constant.int 1
    %int64_698 = torch.constant.int 64
    %int1280_699 = torch.constant.int 1280
    %645 = torch.prim.ListConstruct %int1_697, %int64_698, %int1280_699 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %646 = torch.aten.view %644, %645 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_700 = torch.constant.int 1
    %647 = torch.aten.add.Tensor %602, %646, %int1_700 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_701 = torch.constant.int 6
    %648 = torch.prims.convert_element_type %647, %int6_701 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_702 = torch.constant.int 2
    %649 = torch.prim.ListConstruct %int2_702 : (!torch.int) -> !torch.list<int>
    %int0_703 = torch.constant.int 0
    %true_704 = torch.constant.bool true
    %result0_705, %result1_706 = torch.aten.var_mean.correction %648, %649, %int0_703, %true_704 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_707 = torch.constant.float 1.000000e-05
    %int1_708 = torch.constant.int 1
    %650 = torch.aten.add.Scalar %result0_705, %float1.000000e-05_707, %int1_708 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %651 = torch.aten.rsqrt %650 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_709 = torch.constant.int 1
    %652 = torch.aten.sub.Tensor %647, %result1_706, %int1_709 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %653 = torch.aten.mul.Tensor %652, %651 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.weight : tensor<1280xf16>
    %654 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %655 = torch.aten.mul.Tensor %653, %654 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.bias : tensor<1280xf16>
    %656 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_710 = torch.constant.int 1
    %657 = torch.aten.add.Tensor %655, %656, %int1_710 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_711 = torch.constant.int 5
    %658 = torch.prims.convert_element_type %657, %int5_711 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_712 = torch.constant.int 5
    %659 = torch.prims.convert_element_type %result1_706, %int5_712 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_713 = torch.constant.int 5
    %660 = torch.prims.convert_element_type %651, %int5_713 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_714 = torch.constant.int 64
    %int1280_715 = torch.constant.int 1280
    %661 = torch.prim.ListConstruct %int64_714, %int1280_715 : (!torch.int, !torch.int) -> !torch.list<int>
    %662 = torch.aten.view %658, %661 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %663 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_716 = torch.constant.int 0
    %int1_717 = torch.constant.int 1
    %664 = torch.aten.transpose.int %663, %int0_716, %int1_717 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.bias : tensor<1280xf16>
    %665 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_718 = torch.constant.int 6
    %666 = torch.prims.convert_element_type %665, %int6_718 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_719 = torch.constant.int 6
    %667 = torch.prims.convert_element_type %662, %int6_719 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_720 = torch.constant.int 6
    %668 = torch.prims.convert_element_type %664, %int6_720 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %669 = torch.aten.mm %667, %668 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_721 = torch.constant.int 1
    %670 = torch.aten.mul.Scalar %669, %int1_721 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_722 = torch.constant.int 1
    %671 = torch.aten.mul.Scalar %666, %int1_722 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_723 = torch.constant.int 1
    %672 = torch.aten.add.Tensor %670, %671, %int1_723 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_724 = torch.constant.int 5
    %673 = torch.prims.convert_element_type %672, %int5_724 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_725 = torch.constant.int 1
    %int64_726 = torch.constant.int 64
    %int1280_727 = torch.constant.int 1280
    %674 = torch.prim.ListConstruct %int1_725, %int64_726, %int1280_727 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %675 = torch.aten.view %673, %674 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_728 = torch.constant.float 1.250000e-01
    %676 = torch.aten.mul.Scalar %675, %float1.250000e-01_728 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_729 = torch.constant.int 64
    %int1280_730 = torch.constant.int 1280
    %677 = torch.prim.ListConstruct %int64_729, %int1280_730 : (!torch.int, !torch.int) -> !torch.list<int>
    %678 = torch.aten.view %658, %677 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %679 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_731 = torch.constant.int 0
    %int1_732 = torch.constant.int 1
    %680 = torch.aten.transpose.int %679, %int0_731, %int1_732 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.bias : tensor<1280xf16>
    %681 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_733 = torch.constant.int 6
    %682 = torch.prims.convert_element_type %681, %int6_733 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_734 = torch.constant.int 6
    %683 = torch.prims.convert_element_type %678, %int6_734 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_735 = torch.constant.int 6
    %684 = torch.prims.convert_element_type %680, %int6_735 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %685 = torch.aten.mm %683, %684 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_736 = torch.constant.int 1
    %686 = torch.aten.mul.Scalar %685, %int1_736 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_737 = torch.constant.int 1
    %687 = torch.aten.mul.Scalar %682, %int1_737 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_738 = torch.constant.int 1
    %688 = torch.aten.add.Tensor %686, %687, %int1_738 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_739 = torch.constant.int 5
    %689 = torch.prims.convert_element_type %688, %int5_739 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_740 = torch.constant.int 1
    %int64_741 = torch.constant.int 64
    %int1280_742 = torch.constant.int 1280
    %690 = torch.prim.ListConstruct %int1_740, %int64_741, %int1280_742 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %691 = torch.aten.view %689, %690 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_743 = torch.constant.int 1
    %int-1_744 = torch.constant.int -1
    %int20_745 = torch.constant.int 20
    %int64_746 = torch.constant.int 64
    %692 = torch.prim.ListConstruct %int1_743, %int-1_744, %int20_745, %int64_746 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %693 = torch.aten.view %691, %692 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_747 = torch.constant.int 1
    %int2_748 = torch.constant.int 2
    %694 = torch.aten.transpose.int %693, %int1_747, %int2_748 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_749 = torch.constant.int 0
    %695 = torch.aten.clone %694, %int0_749 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_750 = torch.constant.int 64
    %int1280_751 = torch.constant.int 1280
    %696 = torch.prim.ListConstruct %int64_750, %int1280_751 : (!torch.int, !torch.int) -> !torch.list<int>
    %697 = torch.aten.view %658, %696 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %698 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_752 = torch.constant.int 0
    %int1_753 = torch.constant.int 1
    %699 = torch.aten.transpose.int %698, %int0_752, %int1_753 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.bias : tensor<1280xf16>
    %700 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_754 = torch.constant.int 6
    %701 = torch.prims.convert_element_type %700, %int6_754 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_755 = torch.constant.int 6
    %702 = torch.prims.convert_element_type %697, %int6_755 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_756 = torch.constant.int 6
    %703 = torch.prims.convert_element_type %699, %int6_756 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %704 = torch.aten.mm %702, %703 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_757 = torch.constant.int 1
    %705 = torch.aten.mul.Scalar %704, %int1_757 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_758 = torch.constant.int 1
    %706 = torch.aten.mul.Scalar %701, %int1_758 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_759 = torch.constant.int 1
    %707 = torch.aten.add.Tensor %705, %706, %int1_759 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_760 = torch.constant.int 5
    %708 = torch.prims.convert_element_type %707, %int5_760 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_761 = torch.constant.int 1
    %int64_762 = torch.constant.int 64
    %int1280_763 = torch.constant.int 1280
    %709 = torch.prim.ListConstruct %int1_761, %int64_762, %int1280_763 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %710 = torch.aten.view %708, %709 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_764 = torch.constant.int 1
    %int-1_765 = torch.constant.int -1
    %int20_766 = torch.constant.int 20
    %int64_767 = torch.constant.int 64
    %711 = torch.prim.ListConstruct %int1_764, %int-1_765, %int20_766, %int64_767 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %712 = torch.aten.view %710, %711 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_768 = torch.constant.int 1
    %int2_769 = torch.constant.int 2
    %713 = torch.aten.transpose.int %712, %int1_768, %int2_769 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_770 = torch.constant.int 0
    %714 = torch.aten.clone %713, %int0_770 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_771 = torch.constant.int 1
    %int64_772 = torch.constant.int 64
    %int20_773 = torch.constant.int 20
    %int64_774 = torch.constant.int 64
    %715 = torch.prim.ListConstruct %int1_771, %int64_772, %int20_773, %int64_774 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %716 = torch.aten.view %676, %715 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_775 = torch.constant.int 1
    %int2_776 = torch.constant.int 2
    %717 = torch.aten.transpose.int %716, %int1_775, %int2_776 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_777 = torch.constant.int 0
    %718 = torch.aten.clone %717, %int0_777 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_778 = torch.constant.int 20
    %int-1_779 = torch.constant.int -1
    %int64_780 = torch.constant.int 64
    %719 = torch.prim.ListConstruct %int20_778, %int-1_779, %int64_780 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %720 = torch.aten.view %718, %719 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_781 = torch.constant.int 20
    %int-1_782 = torch.constant.int -1
    %int64_783 = torch.constant.int 64
    %721 = torch.prim.ListConstruct %int20_781, %int-1_782, %int64_783 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %722 = torch.aten.view %695, %721 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_784 = torch.constant.int 20
    %int-1_785 = torch.constant.int -1
    %int64_786 = torch.constant.int 64
    %723 = torch.prim.ListConstruct %int20_784, %int-1_785, %int64_786 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %724 = torch.aten.view %714, %723 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_787 = torch.constant.int 1
    %int2_788 = torch.constant.int 2
    %725 = torch.aten.transpose.int %722, %int1_787, %int2_788 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %726 = torch.aten.bmm %720, %725 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_789 = torch.constant.int 1
    %int20_790 = torch.constant.int 20
    %int64_791 = torch.constant.int 64
    %int64_792 = torch.constant.int 64
    %727 = torch.prim.ListConstruct %int1_789, %int20_790, %int64_791, %int64_792 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %728 = torch.aten.view %726, %727 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_793 = torch.constant.int 1
    %729 = torch.aten.add.Tensor %728, %27, %int1_793 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_794 = torch.constant.int 20
    %int64_795 = torch.constant.int 64
    %int64_796 = torch.constant.int 64
    %730 = torch.prim.ListConstruct %int20_794, %int64_795, %int64_796 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %731 = torch.aten.view %729, %730 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_797 = torch.constant.int -1
    %false_798 = torch.constant.bool false
    %732 = torch.aten._softmax %731, %int-1_797, %false_798 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %733 = torch.aten.detach %732 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_799 = torch.constant.none
    %734 = torch.aten.clone %732, %none_799 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %735 = torch.aten.bmm %734, %724 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_800 = torch.constant.int 1
    %int20_801 = torch.constant.int 20
    %int64_802 = torch.constant.int 64
    %int64_803 = torch.constant.int 64
    %736 = torch.prim.ListConstruct %int1_800, %int20_801, %int64_802, %int64_803 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %737 = torch.aten.view %735, %736 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_804 = torch.constant.int 1
    %int2_805 = torch.constant.int 2
    %738 = torch.aten.transpose.int %737, %int1_804, %int2_805 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_806 = torch.constant.int 0
    %739 = torch.aten.clone %738, %int0_806 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_807 = torch.constant.int 1
    %int64_808 = torch.constant.int 64
    %int1280_809 = torch.constant.int 1280
    %740 = torch.prim.ListConstruct %int1_807, %int64_808, %int1280_809 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %741 = torch.aten._unsafe_view %739, %740 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_810 = torch.constant.int 64
    %int1280_811 = torch.constant.int 1280
    %742 = torch.prim.ListConstruct %int64_810, %int1280_811 : (!torch.int, !torch.int) -> !torch.list<int>
    %743 = torch.aten.view %741, %742 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %744 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_812 = torch.constant.int 0
    %int1_813 = torch.constant.int 1
    %745 = torch.aten.transpose.int %744, %int0_812, %int1_813 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.bias : tensor<1280xf16>
    %746 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_814 = torch.constant.int 6
    %747 = torch.prims.convert_element_type %746, %int6_814 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_815 = torch.constant.int 6
    %748 = torch.prims.convert_element_type %743, %int6_815 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_816 = torch.constant.int 6
    %749 = torch.prims.convert_element_type %745, %int6_816 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %750 = torch.aten.mm %748, %749 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_817 = torch.constant.int 1
    %751 = torch.aten.mul.Scalar %750, %int1_817 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_818 = torch.constant.int 1
    %752 = torch.aten.mul.Scalar %747, %int1_818 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_819 = torch.constant.int 1
    %753 = torch.aten.add.Tensor %751, %752, %int1_819 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_820 = torch.constant.int 5
    %754 = torch.prims.convert_element_type %753, %int5_820 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_821 = torch.constant.int 1
    %int64_822 = torch.constant.int 64
    %int1280_823 = torch.constant.int 1280
    %755 = torch.prim.ListConstruct %int1_821, %int64_822, %int1280_823 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %756 = torch.aten.view %754, %755 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_824 = torch.constant.int 1
    %757 = torch.aten.add.Tensor %647, %756, %int1_824 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_825 = torch.constant.int 6
    %758 = torch.prims.convert_element_type %757, %int6_825 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_826 = torch.constant.int 2
    %759 = torch.prim.ListConstruct %int2_826 : (!torch.int) -> !torch.list<int>
    %int0_827 = torch.constant.int 0
    %true_828 = torch.constant.bool true
    %result0_829, %result1_830 = torch.aten.var_mean.correction %758, %759, %int0_827, %true_828 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_831 = torch.constant.float 1.000000e-05
    %int1_832 = torch.constant.int 1
    %760 = torch.aten.add.Scalar %result0_829, %float1.000000e-05_831, %int1_832 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %761 = torch.aten.rsqrt %760 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_833 = torch.constant.int 1
    %762 = torch.aten.sub.Tensor %757, %result1_830, %int1_833 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %763 = torch.aten.mul.Tensor %762, %761 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.weight : tensor<1280xf16>
    %764 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %765 = torch.aten.mul.Tensor %763, %764 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.bias : tensor<1280xf16>
    %766 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_834 = torch.constant.int 1
    %767 = torch.aten.add.Tensor %765, %766, %int1_834 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_835 = torch.constant.int 5
    %768 = torch.prims.convert_element_type %767, %int5_835 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_836 = torch.constant.int 5
    %769 = torch.prims.convert_element_type %result1_830, %int5_836 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_837 = torch.constant.int 5
    %770 = torch.prims.convert_element_type %761, %int5_837 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_838 = torch.constant.int 64
    %int1280_839 = torch.constant.int 1280
    %771 = torch.prim.ListConstruct %int64_838, %int1280_839 : (!torch.int, !torch.int) -> !torch.list<int>
    %772 = torch.aten.view %768, %771 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.weight : tensor<5120x1280xf16>
    %773 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_840 = torch.constant.int 0
    %int1_841 = torch.constant.int 1
    %774 = torch.aten.transpose.int %773, %int0_840, %int1_841 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.bias : tensor<5120xf16>
    %775 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_842 = torch.constant.int 6
    %776 = torch.prims.convert_element_type %775, %int6_842 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_843 = torch.constant.int 6
    %777 = torch.prims.convert_element_type %772, %int6_843 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_844 = torch.constant.int 6
    %778 = torch.prims.convert_element_type %774, %int6_844 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %779 = torch.aten.mm %777, %778 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_845 = torch.constant.int 1
    %780 = torch.aten.mul.Scalar %779, %int1_845 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_846 = torch.constant.int 1
    %781 = torch.aten.mul.Scalar %776, %int1_846 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_847 = torch.constant.int 1
    %782 = torch.aten.add.Tensor %780, %781, %int1_847 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_848 = torch.constant.int 5
    %783 = torch.prims.convert_element_type %782, %int5_848 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_849 = torch.constant.int 1
    %int64_850 = torch.constant.int 64
    %int5120_851 = torch.constant.int 5120
    %784 = torch.prim.ListConstruct %int1_849, %int64_850, %int5120_851 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %785 = torch.aten.view %783, %784 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_852 = torch.constant.str "none"
    %786 = torch.aten.gelu %785, %str_852 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_853 = torch.constant.int 64
    %int5120_854 = torch.constant.int 5120
    %787 = torch.prim.ListConstruct %int64_853, %int5120_854 : (!torch.int, !torch.int) -> !torch.list<int>
    %788 = torch.aten.view %786, %787 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.weight : tensor<1280x5120xf16>
    %789 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_855 = torch.constant.int 0
    %int1_856 = torch.constant.int 1
    %790 = torch.aten.transpose.int %789, %int0_855, %int1_856 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.bias : tensor<1280xf16>
    %791 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.4.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_857 = torch.constant.int 6
    %792 = torch.prims.convert_element_type %791, %int6_857 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_858 = torch.constant.int 6
    %793 = torch.prims.convert_element_type %788, %int6_858 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_859 = torch.constant.int 6
    %794 = torch.prims.convert_element_type %790, %int6_859 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %795 = torch.aten.mm %793, %794 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_860 = torch.constant.int 1
    %796 = torch.aten.mul.Scalar %795, %int1_860 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_861 = torch.constant.int 1
    %797 = torch.aten.mul.Scalar %792, %int1_861 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_862 = torch.constant.int 1
    %798 = torch.aten.add.Tensor %796, %797, %int1_862 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_863 = torch.constant.int 5
    %799 = torch.prims.convert_element_type %798, %int5_863 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_864 = torch.constant.int 1
    %int64_865 = torch.constant.int 64
    %int1280_866 = torch.constant.int 1280
    %800 = torch.prim.ListConstruct %int1_864, %int64_865, %int1280_866 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %801 = torch.aten.view %799, %800 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_867 = torch.constant.int 1
    %802 = torch.aten.add.Tensor %757, %801, %int1_867 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_868 = torch.constant.int 6
    %803 = torch.prims.convert_element_type %802, %int6_868 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_869 = torch.constant.int 2
    %804 = torch.prim.ListConstruct %int2_869 : (!torch.int) -> !torch.list<int>
    %int0_870 = torch.constant.int 0
    %true_871 = torch.constant.bool true
    %result0_872, %result1_873 = torch.aten.var_mean.correction %803, %804, %int0_870, %true_871 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_874 = torch.constant.float 1.000000e-05
    %int1_875 = torch.constant.int 1
    %805 = torch.aten.add.Scalar %result0_872, %float1.000000e-05_874, %int1_875 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %806 = torch.aten.rsqrt %805 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_876 = torch.constant.int 1
    %807 = torch.aten.sub.Tensor %802, %result1_873, %int1_876 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %808 = torch.aten.mul.Tensor %807, %806 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.weight : tensor<1280xf16>
    %809 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %810 = torch.aten.mul.Tensor %808, %809 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.bias : tensor<1280xf16>
    %811 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_877 = torch.constant.int 1
    %812 = torch.aten.add.Tensor %810, %811, %int1_877 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_878 = torch.constant.int 5
    %813 = torch.prims.convert_element_type %812, %int5_878 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_879 = torch.constant.int 5
    %814 = torch.prims.convert_element_type %result1_873, %int5_879 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_880 = torch.constant.int 5
    %815 = torch.prims.convert_element_type %806, %int5_880 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_881 = torch.constant.int 64
    %int1280_882 = torch.constant.int 1280
    %816 = torch.prim.ListConstruct %int64_881, %int1280_882 : (!torch.int, !torch.int) -> !torch.list<int>
    %817 = torch.aten.view %813, %816 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %818 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_883 = torch.constant.int 0
    %int1_884 = torch.constant.int 1
    %819 = torch.aten.transpose.int %818, %int0_883, %int1_884 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.bias : tensor<1280xf16>
    %820 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_885 = torch.constant.int 6
    %821 = torch.prims.convert_element_type %820, %int6_885 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_886 = torch.constant.int 6
    %822 = torch.prims.convert_element_type %817, %int6_886 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_887 = torch.constant.int 6
    %823 = torch.prims.convert_element_type %819, %int6_887 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %824 = torch.aten.mm %822, %823 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_888 = torch.constant.int 1
    %825 = torch.aten.mul.Scalar %824, %int1_888 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_889 = torch.constant.int 1
    %826 = torch.aten.mul.Scalar %821, %int1_889 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_890 = torch.constant.int 1
    %827 = torch.aten.add.Tensor %825, %826, %int1_890 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_891 = torch.constant.int 5
    %828 = torch.prims.convert_element_type %827, %int5_891 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_892 = torch.constant.int 1
    %int64_893 = torch.constant.int 64
    %int1280_894 = torch.constant.int 1280
    %829 = torch.prim.ListConstruct %int1_892, %int64_893, %int1280_894 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %830 = torch.aten.view %828, %829 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_895 = torch.constant.float 1.250000e-01
    %831 = torch.aten.mul.Scalar %830, %float1.250000e-01_895 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_896 = torch.constant.int 64
    %int1280_897 = torch.constant.int 1280
    %832 = torch.prim.ListConstruct %int64_896, %int1280_897 : (!torch.int, !torch.int) -> !torch.list<int>
    %833 = torch.aten.view %813, %832 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %834 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_898 = torch.constant.int 0
    %int1_899 = torch.constant.int 1
    %835 = torch.aten.transpose.int %834, %int0_898, %int1_899 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.bias : tensor<1280xf16>
    %836 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_900 = torch.constant.int 6
    %837 = torch.prims.convert_element_type %836, %int6_900 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_901 = torch.constant.int 6
    %838 = torch.prims.convert_element_type %833, %int6_901 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_902 = torch.constant.int 6
    %839 = torch.prims.convert_element_type %835, %int6_902 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %840 = torch.aten.mm %838, %839 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_903 = torch.constant.int 1
    %841 = torch.aten.mul.Scalar %840, %int1_903 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_904 = torch.constant.int 1
    %842 = torch.aten.mul.Scalar %837, %int1_904 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_905 = torch.constant.int 1
    %843 = torch.aten.add.Tensor %841, %842, %int1_905 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_906 = torch.constant.int 5
    %844 = torch.prims.convert_element_type %843, %int5_906 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_907 = torch.constant.int 1
    %int64_908 = torch.constant.int 64
    %int1280_909 = torch.constant.int 1280
    %845 = torch.prim.ListConstruct %int1_907, %int64_908, %int1280_909 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %846 = torch.aten.view %844, %845 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_910 = torch.constant.int 1
    %int-1_911 = torch.constant.int -1
    %int20_912 = torch.constant.int 20
    %int64_913 = torch.constant.int 64
    %847 = torch.prim.ListConstruct %int1_910, %int-1_911, %int20_912, %int64_913 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %848 = torch.aten.view %846, %847 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_914 = torch.constant.int 1
    %int2_915 = torch.constant.int 2
    %849 = torch.aten.transpose.int %848, %int1_914, %int2_915 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_916 = torch.constant.int 0
    %850 = torch.aten.clone %849, %int0_916 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_917 = torch.constant.int 64
    %int1280_918 = torch.constant.int 1280
    %851 = torch.prim.ListConstruct %int64_917, %int1280_918 : (!torch.int, !torch.int) -> !torch.list<int>
    %852 = torch.aten.view %813, %851 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %853 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_919 = torch.constant.int 0
    %int1_920 = torch.constant.int 1
    %854 = torch.aten.transpose.int %853, %int0_919, %int1_920 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.bias : tensor<1280xf16>
    %855 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_921 = torch.constant.int 6
    %856 = torch.prims.convert_element_type %855, %int6_921 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_922 = torch.constant.int 6
    %857 = torch.prims.convert_element_type %852, %int6_922 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_923 = torch.constant.int 6
    %858 = torch.prims.convert_element_type %854, %int6_923 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %859 = torch.aten.mm %857, %858 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_924 = torch.constant.int 1
    %860 = torch.aten.mul.Scalar %859, %int1_924 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_925 = torch.constant.int 1
    %861 = torch.aten.mul.Scalar %856, %int1_925 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_926 = torch.constant.int 1
    %862 = torch.aten.add.Tensor %860, %861, %int1_926 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_927 = torch.constant.int 5
    %863 = torch.prims.convert_element_type %862, %int5_927 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_928 = torch.constant.int 1
    %int64_929 = torch.constant.int 64
    %int1280_930 = torch.constant.int 1280
    %864 = torch.prim.ListConstruct %int1_928, %int64_929, %int1280_930 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %865 = torch.aten.view %863, %864 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_931 = torch.constant.int 1
    %int-1_932 = torch.constant.int -1
    %int20_933 = torch.constant.int 20
    %int64_934 = torch.constant.int 64
    %866 = torch.prim.ListConstruct %int1_931, %int-1_932, %int20_933, %int64_934 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %867 = torch.aten.view %865, %866 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_935 = torch.constant.int 1
    %int2_936 = torch.constant.int 2
    %868 = torch.aten.transpose.int %867, %int1_935, %int2_936 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_937 = torch.constant.int 0
    %869 = torch.aten.clone %868, %int0_937 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_938 = torch.constant.int 1
    %int64_939 = torch.constant.int 64
    %int20_940 = torch.constant.int 20
    %int64_941 = torch.constant.int 64
    %870 = torch.prim.ListConstruct %int1_938, %int64_939, %int20_940, %int64_941 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %871 = torch.aten.view %831, %870 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_942 = torch.constant.int 1
    %int2_943 = torch.constant.int 2
    %872 = torch.aten.transpose.int %871, %int1_942, %int2_943 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_944 = torch.constant.int 0
    %873 = torch.aten.clone %872, %int0_944 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_945 = torch.constant.int 20
    %int-1_946 = torch.constant.int -1
    %int64_947 = torch.constant.int 64
    %874 = torch.prim.ListConstruct %int20_945, %int-1_946, %int64_947 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %875 = torch.aten.view %873, %874 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_948 = torch.constant.int 20
    %int-1_949 = torch.constant.int -1
    %int64_950 = torch.constant.int 64
    %876 = torch.prim.ListConstruct %int20_948, %int-1_949, %int64_950 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %877 = torch.aten.view %850, %876 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_951 = torch.constant.int 20
    %int-1_952 = torch.constant.int -1
    %int64_953 = torch.constant.int 64
    %878 = torch.prim.ListConstruct %int20_951, %int-1_952, %int64_953 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %879 = torch.aten.view %869, %878 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_954 = torch.constant.int 1
    %int2_955 = torch.constant.int 2
    %880 = torch.aten.transpose.int %877, %int1_954, %int2_955 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %881 = torch.aten.bmm %875, %880 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_956 = torch.constant.int 1
    %int20_957 = torch.constant.int 20
    %int64_958 = torch.constant.int 64
    %int64_959 = torch.constant.int 64
    %882 = torch.prim.ListConstruct %int1_956, %int20_957, %int64_958, %int64_959 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %883 = torch.aten.view %881, %882 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_960 = torch.constant.int 1
    %884 = torch.aten.add.Tensor %883, %27, %int1_960 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_961 = torch.constant.int 20
    %int64_962 = torch.constant.int 64
    %int64_963 = torch.constant.int 64
    %885 = torch.prim.ListConstruct %int20_961, %int64_962, %int64_963 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %886 = torch.aten.view %884, %885 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_964 = torch.constant.int -1
    %false_965 = torch.constant.bool false
    %887 = torch.aten._softmax %886, %int-1_964, %false_965 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %888 = torch.aten.detach %887 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_966 = torch.constant.none
    %889 = torch.aten.clone %887, %none_966 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %890 = torch.aten.bmm %889, %879 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_967 = torch.constant.int 1
    %int20_968 = torch.constant.int 20
    %int64_969 = torch.constant.int 64
    %int64_970 = torch.constant.int 64
    %891 = torch.prim.ListConstruct %int1_967, %int20_968, %int64_969, %int64_970 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %892 = torch.aten.view %890, %891 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_971 = torch.constant.int 1
    %int2_972 = torch.constant.int 2
    %893 = torch.aten.transpose.int %892, %int1_971, %int2_972 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_973 = torch.constant.int 0
    %894 = torch.aten.clone %893, %int0_973 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_974 = torch.constant.int 1
    %int64_975 = torch.constant.int 64
    %int1280_976 = torch.constant.int 1280
    %895 = torch.prim.ListConstruct %int1_974, %int64_975, %int1280_976 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %896 = torch.aten._unsafe_view %894, %895 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_977 = torch.constant.int 64
    %int1280_978 = torch.constant.int 1280
    %897 = torch.prim.ListConstruct %int64_977, %int1280_978 : (!torch.int, !torch.int) -> !torch.list<int>
    %898 = torch.aten.view %896, %897 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %899 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_979 = torch.constant.int 0
    %int1_980 = torch.constant.int 1
    %900 = torch.aten.transpose.int %899, %int0_979, %int1_980 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.bias : tensor<1280xf16>
    %901 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_981 = torch.constant.int 6
    %902 = torch.prims.convert_element_type %901, %int6_981 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_982 = torch.constant.int 6
    %903 = torch.prims.convert_element_type %898, %int6_982 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_983 = torch.constant.int 6
    %904 = torch.prims.convert_element_type %900, %int6_983 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %905 = torch.aten.mm %903, %904 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_984 = torch.constant.int 1
    %906 = torch.aten.mul.Scalar %905, %int1_984 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_985 = torch.constant.int 1
    %907 = torch.aten.mul.Scalar %902, %int1_985 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_986 = torch.constant.int 1
    %908 = torch.aten.add.Tensor %906, %907, %int1_986 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_987 = torch.constant.int 5
    %909 = torch.prims.convert_element_type %908, %int5_987 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_988 = torch.constant.int 1
    %int64_989 = torch.constant.int 64
    %int1280_990 = torch.constant.int 1280
    %910 = torch.prim.ListConstruct %int1_988, %int64_989, %int1280_990 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %911 = torch.aten.view %909, %910 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_991 = torch.constant.int 1
    %912 = torch.aten.add.Tensor %802, %911, %int1_991 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_992 = torch.constant.int 6
    %913 = torch.prims.convert_element_type %912, %int6_992 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_993 = torch.constant.int 2
    %914 = torch.prim.ListConstruct %int2_993 : (!torch.int) -> !torch.list<int>
    %int0_994 = torch.constant.int 0
    %true_995 = torch.constant.bool true
    %result0_996, %result1_997 = torch.aten.var_mean.correction %913, %914, %int0_994, %true_995 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_998 = torch.constant.float 1.000000e-05
    %int1_999 = torch.constant.int 1
    %915 = torch.aten.add.Scalar %result0_996, %float1.000000e-05_998, %int1_999 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %916 = torch.aten.rsqrt %915 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1000 = torch.constant.int 1
    %917 = torch.aten.sub.Tensor %912, %result1_997, %int1_1000 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %918 = torch.aten.mul.Tensor %917, %916 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.weight : tensor<1280xf16>
    %919 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %920 = torch.aten.mul.Tensor %918, %919 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.bias : tensor<1280xf16>
    %921 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1001 = torch.constant.int 1
    %922 = torch.aten.add.Tensor %920, %921, %int1_1001 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1002 = torch.constant.int 5
    %923 = torch.prims.convert_element_type %922, %int5_1002 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1003 = torch.constant.int 5
    %924 = torch.prims.convert_element_type %result1_997, %int5_1003 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1004 = torch.constant.int 5
    %925 = torch.prims.convert_element_type %916, %int5_1004 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1005 = torch.constant.int 64
    %int1280_1006 = torch.constant.int 1280
    %926 = torch.prim.ListConstruct %int64_1005, %int1280_1006 : (!torch.int, !torch.int) -> !torch.list<int>
    %927 = torch.aten.view %923, %926 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.weight : tensor<5120x1280xf16>
    %928 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_1007 = torch.constant.int 0
    %int1_1008 = torch.constant.int 1
    %929 = torch.aten.transpose.int %928, %int0_1007, %int1_1008 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.bias : tensor<5120xf16>
    %930 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1009 = torch.constant.int 6
    %931 = torch.prims.convert_element_type %930, %int6_1009 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1010 = torch.constant.int 6
    %932 = torch.prims.convert_element_type %927, %int6_1010 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1011 = torch.constant.int 6
    %933 = torch.prims.convert_element_type %929, %int6_1011 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %934 = torch.aten.mm %932, %933 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_1012 = torch.constant.int 1
    %935 = torch.aten.mul.Scalar %934, %int1_1012 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_1013 = torch.constant.int 1
    %936 = torch.aten.mul.Scalar %931, %int1_1013 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1014 = torch.constant.int 1
    %937 = torch.aten.add.Tensor %935, %936, %int1_1014 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_1015 = torch.constant.int 5
    %938 = torch.prims.convert_element_type %937, %int5_1015 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_1016 = torch.constant.int 1
    %int64_1017 = torch.constant.int 64
    %int5120_1018 = torch.constant.int 5120
    %939 = torch.prim.ListConstruct %int1_1016, %int64_1017, %int5120_1018 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %940 = torch.aten.view %938, %939 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_1019 = torch.constant.str "none"
    %941 = torch.aten.gelu %940, %str_1019 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_1020 = torch.constant.int 64
    %int5120_1021 = torch.constant.int 5120
    %942 = torch.prim.ListConstruct %int64_1020, %int5120_1021 : (!torch.int, !torch.int) -> !torch.list<int>
    %943 = torch.aten.view %941, %942 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.weight : tensor<1280x5120xf16>
    %944 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1022 = torch.constant.int 0
    %int1_1023 = torch.constant.int 1
    %945 = torch.aten.transpose.int %944, %int0_1022, %int1_1023 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.bias : tensor<1280xf16>
    %946 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.5.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1024 = torch.constant.int 6
    %947 = torch.prims.convert_element_type %946, %int6_1024 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1025 = torch.constant.int 6
    %948 = torch.prims.convert_element_type %943, %int6_1025 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_1026 = torch.constant.int 6
    %949 = torch.prims.convert_element_type %945, %int6_1026 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %950 = torch.aten.mm %948, %949 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1027 = torch.constant.int 1
    %951 = torch.aten.mul.Scalar %950, %int1_1027 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1028 = torch.constant.int 1
    %952 = torch.aten.mul.Scalar %947, %int1_1028 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1029 = torch.constant.int 1
    %953 = torch.aten.add.Tensor %951, %952, %int1_1029 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1030 = torch.constant.int 5
    %954 = torch.prims.convert_element_type %953, %int5_1030 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1031 = torch.constant.int 1
    %int64_1032 = torch.constant.int 64
    %int1280_1033 = torch.constant.int 1280
    %955 = torch.prim.ListConstruct %int1_1031, %int64_1032, %int1280_1033 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %956 = torch.aten.view %954, %955 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1034 = torch.constant.int 1
    %957 = torch.aten.add.Tensor %912, %956, %int1_1034 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1035 = torch.constant.int 6
    %958 = torch.prims.convert_element_type %957, %int6_1035 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1036 = torch.constant.int 2
    %959 = torch.prim.ListConstruct %int2_1036 : (!torch.int) -> !torch.list<int>
    %int0_1037 = torch.constant.int 0
    %true_1038 = torch.constant.bool true
    %result0_1039, %result1_1040 = torch.aten.var_mean.correction %958, %959, %int0_1037, %true_1038 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1041 = torch.constant.float 1.000000e-05
    %int1_1042 = torch.constant.int 1
    %960 = torch.aten.add.Scalar %result0_1039, %float1.000000e-05_1041, %int1_1042 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %961 = torch.aten.rsqrt %960 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1043 = torch.constant.int 1
    %962 = torch.aten.sub.Tensor %957, %result1_1040, %int1_1043 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %963 = torch.aten.mul.Tensor %962, %961 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.weight : tensor<1280xf16>
    %964 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %965 = torch.aten.mul.Tensor %963, %964 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.bias : tensor<1280xf16>
    %966 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1044 = torch.constant.int 1
    %967 = torch.aten.add.Tensor %965, %966, %int1_1044 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1045 = torch.constant.int 5
    %968 = torch.prims.convert_element_type %967, %int5_1045 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1046 = torch.constant.int 5
    %969 = torch.prims.convert_element_type %result1_1040, %int5_1046 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1047 = torch.constant.int 5
    %970 = torch.prims.convert_element_type %961, %int5_1047 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1048 = torch.constant.int 64
    %int1280_1049 = torch.constant.int 1280
    %971 = torch.prim.ListConstruct %int64_1048, %int1280_1049 : (!torch.int, !torch.int) -> !torch.list<int>
    %972 = torch.aten.view %968, %971 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %973 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1050 = torch.constant.int 0
    %int1_1051 = torch.constant.int 1
    %974 = torch.aten.transpose.int %973, %int0_1050, %int1_1051 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.bias : tensor<1280xf16>
    %975 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1052 = torch.constant.int 6
    %976 = torch.prims.convert_element_type %975, %int6_1052 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1053 = torch.constant.int 6
    %977 = torch.prims.convert_element_type %972, %int6_1053 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1054 = torch.constant.int 6
    %978 = torch.prims.convert_element_type %974, %int6_1054 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %979 = torch.aten.mm %977, %978 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1055 = torch.constant.int 1
    %980 = torch.aten.mul.Scalar %979, %int1_1055 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1056 = torch.constant.int 1
    %981 = torch.aten.mul.Scalar %976, %int1_1056 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1057 = torch.constant.int 1
    %982 = torch.aten.add.Tensor %980, %981, %int1_1057 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1058 = torch.constant.int 5
    %983 = torch.prims.convert_element_type %982, %int5_1058 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1059 = torch.constant.int 1
    %int64_1060 = torch.constant.int 64
    %int1280_1061 = torch.constant.int 1280
    %984 = torch.prim.ListConstruct %int1_1059, %int64_1060, %int1280_1061 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %985 = torch.aten.view %983, %984 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_1062 = torch.constant.float 1.250000e-01
    %986 = torch.aten.mul.Scalar %985, %float1.250000e-01_1062 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_1063 = torch.constant.int 64
    %int1280_1064 = torch.constant.int 1280
    %987 = torch.prim.ListConstruct %int64_1063, %int1280_1064 : (!torch.int, !torch.int) -> !torch.list<int>
    %988 = torch.aten.view %968, %987 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %989 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1065 = torch.constant.int 0
    %int1_1066 = torch.constant.int 1
    %990 = torch.aten.transpose.int %989, %int0_1065, %int1_1066 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.bias : tensor<1280xf16>
    %991 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1067 = torch.constant.int 6
    %992 = torch.prims.convert_element_type %991, %int6_1067 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1068 = torch.constant.int 6
    %993 = torch.prims.convert_element_type %988, %int6_1068 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1069 = torch.constant.int 6
    %994 = torch.prims.convert_element_type %990, %int6_1069 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %995 = torch.aten.mm %993, %994 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1070 = torch.constant.int 1
    %996 = torch.aten.mul.Scalar %995, %int1_1070 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1071 = torch.constant.int 1
    %997 = torch.aten.mul.Scalar %992, %int1_1071 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1072 = torch.constant.int 1
    %998 = torch.aten.add.Tensor %996, %997, %int1_1072 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1073 = torch.constant.int 5
    %999 = torch.prims.convert_element_type %998, %int5_1073 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1074 = torch.constant.int 1
    %int64_1075 = torch.constant.int 64
    %int1280_1076 = torch.constant.int 1280
    %1000 = torch.prim.ListConstruct %int1_1074, %int64_1075, %int1280_1076 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1001 = torch.aten.view %999, %1000 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1077 = torch.constant.int 1
    %int-1_1078 = torch.constant.int -1
    %int20_1079 = torch.constant.int 20
    %int64_1080 = torch.constant.int 64
    %1002 = torch.prim.ListConstruct %int1_1077, %int-1_1078, %int20_1079, %int64_1080 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1003 = torch.aten.view %1001, %1002 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1081 = torch.constant.int 1
    %int2_1082 = torch.constant.int 2
    %1004 = torch.aten.transpose.int %1003, %int1_1081, %int2_1082 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1083 = torch.constant.int 0
    %1005 = torch.aten.clone %1004, %int0_1083 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_1084 = torch.constant.int 64
    %int1280_1085 = torch.constant.int 1280
    %1006 = torch.prim.ListConstruct %int64_1084, %int1280_1085 : (!torch.int, !torch.int) -> !torch.list<int>
    %1007 = torch.aten.view %968, %1006 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1008 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1086 = torch.constant.int 0
    %int1_1087 = torch.constant.int 1
    %1009 = torch.aten.transpose.int %1008, %int0_1086, %int1_1087 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.bias : tensor<1280xf16>
    %1010 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1088 = torch.constant.int 6
    %1011 = torch.prims.convert_element_type %1010, %int6_1088 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1089 = torch.constant.int 6
    %1012 = torch.prims.convert_element_type %1007, %int6_1089 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1090 = torch.constant.int 6
    %1013 = torch.prims.convert_element_type %1009, %int6_1090 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1014 = torch.aten.mm %1012, %1013 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1091 = torch.constant.int 1
    %1015 = torch.aten.mul.Scalar %1014, %int1_1091 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1092 = torch.constant.int 1
    %1016 = torch.aten.mul.Scalar %1011, %int1_1092 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1093 = torch.constant.int 1
    %1017 = torch.aten.add.Tensor %1015, %1016, %int1_1093 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1094 = torch.constant.int 5
    %1018 = torch.prims.convert_element_type %1017, %int5_1094 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1095 = torch.constant.int 1
    %int64_1096 = torch.constant.int 64
    %int1280_1097 = torch.constant.int 1280
    %1019 = torch.prim.ListConstruct %int1_1095, %int64_1096, %int1280_1097 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1020 = torch.aten.view %1018, %1019 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1098 = torch.constant.int 1
    %int-1_1099 = torch.constant.int -1
    %int20_1100 = torch.constant.int 20
    %int64_1101 = torch.constant.int 64
    %1021 = torch.prim.ListConstruct %int1_1098, %int-1_1099, %int20_1100, %int64_1101 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1022 = torch.aten.view %1020, %1021 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1102 = torch.constant.int 1
    %int2_1103 = torch.constant.int 2
    %1023 = torch.aten.transpose.int %1022, %int1_1102, %int2_1103 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1104 = torch.constant.int 0
    %1024 = torch.aten.clone %1023, %int0_1104 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1105 = torch.constant.int 1
    %int64_1106 = torch.constant.int 64
    %int20_1107 = torch.constant.int 20
    %int64_1108 = torch.constant.int 64
    %1025 = torch.prim.ListConstruct %int1_1105, %int64_1106, %int20_1107, %int64_1108 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1026 = torch.aten.view %986, %1025 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1109 = torch.constant.int 1
    %int2_1110 = torch.constant.int 2
    %1027 = torch.aten.transpose.int %1026, %int1_1109, %int2_1110 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1111 = torch.constant.int 0
    %1028 = torch.aten.clone %1027, %int0_1111 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1112 = torch.constant.int 20
    %int-1_1113 = torch.constant.int -1
    %int64_1114 = torch.constant.int 64
    %1029 = torch.prim.ListConstruct %int20_1112, %int-1_1113, %int64_1114 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1030 = torch.aten.view %1028, %1029 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1115 = torch.constant.int 20
    %int-1_1116 = torch.constant.int -1
    %int64_1117 = torch.constant.int 64
    %1031 = torch.prim.ListConstruct %int20_1115, %int-1_1116, %int64_1117 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1032 = torch.aten.view %1005, %1031 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1118 = torch.constant.int 20
    %int-1_1119 = torch.constant.int -1
    %int64_1120 = torch.constant.int 64
    %1033 = torch.prim.ListConstruct %int20_1118, %int-1_1119, %int64_1120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1034 = torch.aten.view %1024, %1033 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_1121 = torch.constant.int 1
    %int2_1122 = torch.constant.int 2
    %1035 = torch.aten.transpose.int %1032, %int1_1121, %int2_1122 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1036 = torch.aten.bmm %1030, %1035 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1123 = torch.constant.int 1
    %int20_1124 = torch.constant.int 20
    %int64_1125 = torch.constant.int 64
    %int64_1126 = torch.constant.int 64
    %1037 = torch.prim.ListConstruct %int1_1123, %int20_1124, %int64_1125, %int64_1126 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1038 = torch.aten.view %1036, %1037 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1127 = torch.constant.int 1
    %1039 = torch.aten.add.Tensor %1038, %27, %int1_1127 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1128 = torch.constant.int 20
    %int64_1129 = torch.constant.int 64
    %int64_1130 = torch.constant.int 64
    %1040 = torch.prim.ListConstruct %int20_1128, %int64_1129, %int64_1130 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1041 = torch.aten.view %1039, %1040 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_1131 = torch.constant.int -1
    %false_1132 = torch.constant.bool false
    %1042 = torch.aten._softmax %1041, %int-1_1131, %false_1132 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1043 = torch.aten.detach %1042 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_1133 = torch.constant.none
    %1044 = torch.aten.clone %1042, %none_1133 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1045 = torch.aten.bmm %1044, %1034 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1134 = torch.constant.int 1
    %int20_1135 = torch.constant.int 20
    %int64_1136 = torch.constant.int 64
    %int64_1137 = torch.constant.int 64
    %1046 = torch.prim.ListConstruct %int1_1134, %int20_1135, %int64_1136, %int64_1137 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1047 = torch.aten.view %1045, %1046 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1138 = torch.constant.int 1
    %int2_1139 = torch.constant.int 2
    %1048 = torch.aten.transpose.int %1047, %int1_1138, %int2_1139 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_1140 = torch.constant.int 0
    %1049 = torch.aten.clone %1048, %int0_1140 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1141 = torch.constant.int 1
    %int64_1142 = torch.constant.int 64
    %int1280_1143 = torch.constant.int 1280
    %1050 = torch.prim.ListConstruct %int1_1141, %int64_1142, %int1280_1143 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1051 = torch.aten._unsafe_view %1049, %1050 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_1144 = torch.constant.int 64
    %int1280_1145 = torch.constant.int 1280
    %1052 = torch.prim.ListConstruct %int64_1144, %int1280_1145 : (!torch.int, !torch.int) -> !torch.list<int>
    %1053 = torch.aten.view %1051, %1052 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1054 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1146 = torch.constant.int 0
    %int1_1147 = torch.constant.int 1
    %1055 = torch.aten.transpose.int %1054, %int0_1146, %int1_1147 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.bias : tensor<1280xf16>
    %1056 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1148 = torch.constant.int 6
    %1057 = torch.prims.convert_element_type %1056, %int6_1148 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1149 = torch.constant.int 6
    %1058 = torch.prims.convert_element_type %1053, %int6_1149 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1150 = torch.constant.int 6
    %1059 = torch.prims.convert_element_type %1055, %int6_1150 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1060 = torch.aten.mm %1058, %1059 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1151 = torch.constant.int 1
    %1061 = torch.aten.mul.Scalar %1060, %int1_1151 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1152 = torch.constant.int 1
    %1062 = torch.aten.mul.Scalar %1057, %int1_1152 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1153 = torch.constant.int 1
    %1063 = torch.aten.add.Tensor %1061, %1062, %int1_1153 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1154 = torch.constant.int 5
    %1064 = torch.prims.convert_element_type %1063, %int5_1154 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1155 = torch.constant.int 1
    %int64_1156 = torch.constant.int 64
    %int1280_1157 = torch.constant.int 1280
    %1065 = torch.prim.ListConstruct %int1_1155, %int64_1156, %int1280_1157 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1066 = torch.aten.view %1064, %1065 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1158 = torch.constant.int 1
    %1067 = torch.aten.add.Tensor %957, %1066, %int1_1158 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1159 = torch.constant.int 6
    %1068 = torch.prims.convert_element_type %1067, %int6_1159 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1160 = torch.constant.int 2
    %1069 = torch.prim.ListConstruct %int2_1160 : (!torch.int) -> !torch.list<int>
    %int0_1161 = torch.constant.int 0
    %true_1162 = torch.constant.bool true
    %result0_1163, %result1_1164 = torch.aten.var_mean.correction %1068, %1069, %int0_1161, %true_1162 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1165 = torch.constant.float 1.000000e-05
    %int1_1166 = torch.constant.int 1
    %1070 = torch.aten.add.Scalar %result0_1163, %float1.000000e-05_1165, %int1_1166 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1071 = torch.aten.rsqrt %1070 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1167 = torch.constant.int 1
    %1072 = torch.aten.sub.Tensor %1067, %result1_1164, %int1_1167 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1073 = torch.aten.mul.Tensor %1072, %1071 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.weight : tensor<1280xf16>
    %1074 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1075 = torch.aten.mul.Tensor %1073, %1074 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.bias : tensor<1280xf16>
    %1076 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1168 = torch.constant.int 1
    %1077 = torch.aten.add.Tensor %1075, %1076, %int1_1168 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1169 = torch.constant.int 5
    %1078 = torch.prims.convert_element_type %1077, %int5_1169 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1170 = torch.constant.int 5
    %1079 = torch.prims.convert_element_type %result1_1164, %int5_1170 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1171 = torch.constant.int 5
    %1080 = torch.prims.convert_element_type %1071, %int5_1171 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1172 = torch.constant.int 64
    %int1280_1173 = torch.constant.int 1280
    %1081 = torch.prim.ListConstruct %int64_1172, %int1280_1173 : (!torch.int, !torch.int) -> !torch.list<int>
    %1082 = torch.aten.view %1078, %1081 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.weight : tensor<5120x1280xf16>
    %1083 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_1174 = torch.constant.int 0
    %int1_1175 = torch.constant.int 1
    %1084 = torch.aten.transpose.int %1083, %int0_1174, %int1_1175 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.bias : tensor<5120xf16>
    %1085 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1176 = torch.constant.int 6
    %1086 = torch.prims.convert_element_type %1085, %int6_1176 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1177 = torch.constant.int 6
    %1087 = torch.prims.convert_element_type %1082, %int6_1177 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1178 = torch.constant.int 6
    %1088 = torch.prims.convert_element_type %1084, %int6_1178 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %1089 = torch.aten.mm %1087, %1088 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_1179 = torch.constant.int 1
    %1090 = torch.aten.mul.Scalar %1089, %int1_1179 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_1180 = torch.constant.int 1
    %1091 = torch.aten.mul.Scalar %1086, %int1_1180 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1181 = torch.constant.int 1
    %1092 = torch.aten.add.Tensor %1090, %1091, %int1_1181 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_1182 = torch.constant.int 5
    %1093 = torch.prims.convert_element_type %1092, %int5_1182 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_1183 = torch.constant.int 1
    %int64_1184 = torch.constant.int 64
    %int5120_1185 = torch.constant.int 5120
    %1094 = torch.prim.ListConstruct %int1_1183, %int64_1184, %int5120_1185 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1095 = torch.aten.view %1093, %1094 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_1186 = torch.constant.str "none"
    %1096 = torch.aten.gelu %1095, %str_1186 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_1187 = torch.constant.int 64
    %int5120_1188 = torch.constant.int 5120
    %1097 = torch.prim.ListConstruct %int64_1187, %int5120_1188 : (!torch.int, !torch.int) -> !torch.list<int>
    %1098 = torch.aten.view %1096, %1097 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.weight : tensor<1280x5120xf16>
    %1099 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1189 = torch.constant.int 0
    %int1_1190 = torch.constant.int 1
    %1100 = torch.aten.transpose.int %1099, %int0_1189, %int1_1190 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.bias : tensor<1280xf16>
    %1101 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.6.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1191 = torch.constant.int 6
    %1102 = torch.prims.convert_element_type %1101, %int6_1191 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1192 = torch.constant.int 6
    %1103 = torch.prims.convert_element_type %1098, %int6_1192 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_1193 = torch.constant.int 6
    %1104 = torch.prims.convert_element_type %1100, %int6_1193 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1105 = torch.aten.mm %1103, %1104 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1194 = torch.constant.int 1
    %1106 = torch.aten.mul.Scalar %1105, %int1_1194 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1195 = torch.constant.int 1
    %1107 = torch.aten.mul.Scalar %1102, %int1_1195 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1196 = torch.constant.int 1
    %1108 = torch.aten.add.Tensor %1106, %1107, %int1_1196 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1197 = torch.constant.int 5
    %1109 = torch.prims.convert_element_type %1108, %int5_1197 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1198 = torch.constant.int 1
    %int64_1199 = torch.constant.int 64
    %int1280_1200 = torch.constant.int 1280
    %1110 = torch.prim.ListConstruct %int1_1198, %int64_1199, %int1280_1200 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1111 = torch.aten.view %1109, %1110 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1201 = torch.constant.int 1
    %1112 = torch.aten.add.Tensor %1067, %1111, %int1_1201 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1202 = torch.constant.int 6
    %1113 = torch.prims.convert_element_type %1112, %int6_1202 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1203 = torch.constant.int 2
    %1114 = torch.prim.ListConstruct %int2_1203 : (!torch.int) -> !torch.list<int>
    %int0_1204 = torch.constant.int 0
    %true_1205 = torch.constant.bool true
    %result0_1206, %result1_1207 = torch.aten.var_mean.correction %1113, %1114, %int0_1204, %true_1205 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1208 = torch.constant.float 1.000000e-05
    %int1_1209 = torch.constant.int 1
    %1115 = torch.aten.add.Scalar %result0_1206, %float1.000000e-05_1208, %int1_1209 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1116 = torch.aten.rsqrt %1115 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1210 = torch.constant.int 1
    %1117 = torch.aten.sub.Tensor %1112, %result1_1207, %int1_1210 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1118 = torch.aten.mul.Tensor %1117, %1116 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.weight : tensor<1280xf16>
    %1119 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1120 = torch.aten.mul.Tensor %1118, %1119 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.bias : tensor<1280xf16>
    %1121 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1211 = torch.constant.int 1
    %1122 = torch.aten.add.Tensor %1120, %1121, %int1_1211 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1212 = torch.constant.int 5
    %1123 = torch.prims.convert_element_type %1122, %int5_1212 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1213 = torch.constant.int 5
    %1124 = torch.prims.convert_element_type %result1_1207, %int5_1213 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1214 = torch.constant.int 5
    %1125 = torch.prims.convert_element_type %1116, %int5_1214 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1215 = torch.constant.int 64
    %int1280_1216 = torch.constant.int 1280
    %1126 = torch.prim.ListConstruct %int64_1215, %int1280_1216 : (!torch.int, !torch.int) -> !torch.list<int>
    %1127 = torch.aten.view %1123, %1126 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %1128 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1217 = torch.constant.int 0
    %int1_1218 = torch.constant.int 1
    %1129 = torch.aten.transpose.int %1128, %int0_1217, %int1_1218 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.bias : tensor<1280xf16>
    %1130 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1219 = torch.constant.int 6
    %1131 = torch.prims.convert_element_type %1130, %int6_1219 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1220 = torch.constant.int 6
    %1132 = torch.prims.convert_element_type %1127, %int6_1220 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1221 = torch.constant.int 6
    %1133 = torch.prims.convert_element_type %1129, %int6_1221 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1134 = torch.aten.mm %1132, %1133 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1222 = torch.constant.int 1
    %1135 = torch.aten.mul.Scalar %1134, %int1_1222 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1223 = torch.constant.int 1
    %1136 = torch.aten.mul.Scalar %1131, %int1_1223 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1224 = torch.constant.int 1
    %1137 = torch.aten.add.Tensor %1135, %1136, %int1_1224 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1225 = torch.constant.int 5
    %1138 = torch.prims.convert_element_type %1137, %int5_1225 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1226 = torch.constant.int 1
    %int64_1227 = torch.constant.int 64
    %int1280_1228 = torch.constant.int 1280
    %1139 = torch.prim.ListConstruct %int1_1226, %int64_1227, %int1280_1228 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1140 = torch.aten.view %1138, %1139 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_1229 = torch.constant.float 1.250000e-01
    %1141 = torch.aten.mul.Scalar %1140, %float1.250000e-01_1229 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_1230 = torch.constant.int 64
    %int1280_1231 = torch.constant.int 1280
    %1142 = torch.prim.ListConstruct %int64_1230, %int1280_1231 : (!torch.int, !torch.int) -> !torch.list<int>
    %1143 = torch.aten.view %1123, %1142 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %1144 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1232 = torch.constant.int 0
    %int1_1233 = torch.constant.int 1
    %1145 = torch.aten.transpose.int %1144, %int0_1232, %int1_1233 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.bias : tensor<1280xf16>
    %1146 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1234 = torch.constant.int 6
    %1147 = torch.prims.convert_element_type %1146, %int6_1234 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1235 = torch.constant.int 6
    %1148 = torch.prims.convert_element_type %1143, %int6_1235 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1236 = torch.constant.int 6
    %1149 = torch.prims.convert_element_type %1145, %int6_1236 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1150 = torch.aten.mm %1148, %1149 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1237 = torch.constant.int 1
    %1151 = torch.aten.mul.Scalar %1150, %int1_1237 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1238 = torch.constant.int 1
    %1152 = torch.aten.mul.Scalar %1147, %int1_1238 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1239 = torch.constant.int 1
    %1153 = torch.aten.add.Tensor %1151, %1152, %int1_1239 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1240 = torch.constant.int 5
    %1154 = torch.prims.convert_element_type %1153, %int5_1240 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1241 = torch.constant.int 1
    %int64_1242 = torch.constant.int 64
    %int1280_1243 = torch.constant.int 1280
    %1155 = torch.prim.ListConstruct %int1_1241, %int64_1242, %int1280_1243 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1156 = torch.aten.view %1154, %1155 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1244 = torch.constant.int 1
    %int-1_1245 = torch.constant.int -1
    %int20_1246 = torch.constant.int 20
    %int64_1247 = torch.constant.int 64
    %1157 = torch.prim.ListConstruct %int1_1244, %int-1_1245, %int20_1246, %int64_1247 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1158 = torch.aten.view %1156, %1157 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1248 = torch.constant.int 1
    %int2_1249 = torch.constant.int 2
    %1159 = torch.aten.transpose.int %1158, %int1_1248, %int2_1249 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1250 = torch.constant.int 0
    %1160 = torch.aten.clone %1159, %int0_1250 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_1251 = torch.constant.int 64
    %int1280_1252 = torch.constant.int 1280
    %1161 = torch.prim.ListConstruct %int64_1251, %int1280_1252 : (!torch.int, !torch.int) -> !torch.list<int>
    %1162 = torch.aten.view %1123, %1161 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1163 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1253 = torch.constant.int 0
    %int1_1254 = torch.constant.int 1
    %1164 = torch.aten.transpose.int %1163, %int0_1253, %int1_1254 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.bias : tensor<1280xf16>
    %1165 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1255 = torch.constant.int 6
    %1166 = torch.prims.convert_element_type %1165, %int6_1255 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1256 = torch.constant.int 6
    %1167 = torch.prims.convert_element_type %1162, %int6_1256 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1257 = torch.constant.int 6
    %1168 = torch.prims.convert_element_type %1164, %int6_1257 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1169 = torch.aten.mm %1167, %1168 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1258 = torch.constant.int 1
    %1170 = torch.aten.mul.Scalar %1169, %int1_1258 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1259 = torch.constant.int 1
    %1171 = torch.aten.mul.Scalar %1166, %int1_1259 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1260 = torch.constant.int 1
    %1172 = torch.aten.add.Tensor %1170, %1171, %int1_1260 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1261 = torch.constant.int 5
    %1173 = torch.prims.convert_element_type %1172, %int5_1261 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1262 = torch.constant.int 1
    %int64_1263 = torch.constant.int 64
    %int1280_1264 = torch.constant.int 1280
    %1174 = torch.prim.ListConstruct %int1_1262, %int64_1263, %int1280_1264 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1175 = torch.aten.view %1173, %1174 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1265 = torch.constant.int 1
    %int-1_1266 = torch.constant.int -1
    %int20_1267 = torch.constant.int 20
    %int64_1268 = torch.constant.int 64
    %1176 = torch.prim.ListConstruct %int1_1265, %int-1_1266, %int20_1267, %int64_1268 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1177 = torch.aten.view %1175, %1176 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1269 = torch.constant.int 1
    %int2_1270 = torch.constant.int 2
    %1178 = torch.aten.transpose.int %1177, %int1_1269, %int2_1270 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1271 = torch.constant.int 0
    %1179 = torch.aten.clone %1178, %int0_1271 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1272 = torch.constant.int 1
    %int64_1273 = torch.constant.int 64
    %int20_1274 = torch.constant.int 20
    %int64_1275 = torch.constant.int 64
    %1180 = torch.prim.ListConstruct %int1_1272, %int64_1273, %int20_1274, %int64_1275 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1181 = torch.aten.view %1141, %1180 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1276 = torch.constant.int 1
    %int2_1277 = torch.constant.int 2
    %1182 = torch.aten.transpose.int %1181, %int1_1276, %int2_1277 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1278 = torch.constant.int 0
    %1183 = torch.aten.clone %1182, %int0_1278 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1279 = torch.constant.int 20
    %int-1_1280 = torch.constant.int -1
    %int64_1281 = torch.constant.int 64
    %1184 = torch.prim.ListConstruct %int20_1279, %int-1_1280, %int64_1281 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1185 = torch.aten.view %1183, %1184 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1282 = torch.constant.int 20
    %int-1_1283 = torch.constant.int -1
    %int64_1284 = torch.constant.int 64
    %1186 = torch.prim.ListConstruct %int20_1282, %int-1_1283, %int64_1284 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1187 = torch.aten.view %1160, %1186 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1285 = torch.constant.int 20
    %int-1_1286 = torch.constant.int -1
    %int64_1287 = torch.constant.int 64
    %1188 = torch.prim.ListConstruct %int20_1285, %int-1_1286, %int64_1287 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1189 = torch.aten.view %1179, %1188 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_1288 = torch.constant.int 1
    %int2_1289 = torch.constant.int 2
    %1190 = torch.aten.transpose.int %1187, %int1_1288, %int2_1289 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1191 = torch.aten.bmm %1185, %1190 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1290 = torch.constant.int 1
    %int20_1291 = torch.constant.int 20
    %int64_1292 = torch.constant.int 64
    %int64_1293 = torch.constant.int 64
    %1192 = torch.prim.ListConstruct %int1_1290, %int20_1291, %int64_1292, %int64_1293 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1193 = torch.aten.view %1191, %1192 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1294 = torch.constant.int 1
    %1194 = torch.aten.add.Tensor %1193, %27, %int1_1294 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1295 = torch.constant.int 20
    %int64_1296 = torch.constant.int 64
    %int64_1297 = torch.constant.int 64
    %1195 = torch.prim.ListConstruct %int20_1295, %int64_1296, %int64_1297 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1196 = torch.aten.view %1194, %1195 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_1298 = torch.constant.int -1
    %false_1299 = torch.constant.bool false
    %1197 = torch.aten._softmax %1196, %int-1_1298, %false_1299 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1198 = torch.aten.detach %1197 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_1300 = torch.constant.none
    %1199 = torch.aten.clone %1197, %none_1300 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1200 = torch.aten.bmm %1199, %1189 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1301 = torch.constant.int 1
    %int20_1302 = torch.constant.int 20
    %int64_1303 = torch.constant.int 64
    %int64_1304 = torch.constant.int 64
    %1201 = torch.prim.ListConstruct %int1_1301, %int20_1302, %int64_1303, %int64_1304 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1202 = torch.aten.view %1200, %1201 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1305 = torch.constant.int 1
    %int2_1306 = torch.constant.int 2
    %1203 = torch.aten.transpose.int %1202, %int1_1305, %int2_1306 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_1307 = torch.constant.int 0
    %1204 = torch.aten.clone %1203, %int0_1307 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1308 = torch.constant.int 1
    %int64_1309 = torch.constant.int 64
    %int1280_1310 = torch.constant.int 1280
    %1205 = torch.prim.ListConstruct %int1_1308, %int64_1309, %int1280_1310 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1206 = torch.aten._unsafe_view %1204, %1205 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_1311 = torch.constant.int 64
    %int1280_1312 = torch.constant.int 1280
    %1207 = torch.prim.ListConstruct %int64_1311, %int1280_1312 : (!torch.int, !torch.int) -> !torch.list<int>
    %1208 = torch.aten.view %1206, %1207 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1209 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1313 = torch.constant.int 0
    %int1_1314 = torch.constant.int 1
    %1210 = torch.aten.transpose.int %1209, %int0_1313, %int1_1314 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.bias : tensor<1280xf16>
    %1211 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1315 = torch.constant.int 6
    %1212 = torch.prims.convert_element_type %1211, %int6_1315 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1316 = torch.constant.int 6
    %1213 = torch.prims.convert_element_type %1208, %int6_1316 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1317 = torch.constant.int 6
    %1214 = torch.prims.convert_element_type %1210, %int6_1317 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1215 = torch.aten.mm %1213, %1214 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1318 = torch.constant.int 1
    %1216 = torch.aten.mul.Scalar %1215, %int1_1318 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1319 = torch.constant.int 1
    %1217 = torch.aten.mul.Scalar %1212, %int1_1319 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1320 = torch.constant.int 1
    %1218 = torch.aten.add.Tensor %1216, %1217, %int1_1320 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1321 = torch.constant.int 5
    %1219 = torch.prims.convert_element_type %1218, %int5_1321 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1322 = torch.constant.int 1
    %int64_1323 = torch.constant.int 64
    %int1280_1324 = torch.constant.int 1280
    %1220 = torch.prim.ListConstruct %int1_1322, %int64_1323, %int1280_1324 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1221 = torch.aten.view %1219, %1220 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1325 = torch.constant.int 1
    %1222 = torch.aten.add.Tensor %1112, %1221, %int1_1325 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1326 = torch.constant.int 6
    %1223 = torch.prims.convert_element_type %1222, %int6_1326 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1327 = torch.constant.int 2
    %1224 = torch.prim.ListConstruct %int2_1327 : (!torch.int) -> !torch.list<int>
    %int0_1328 = torch.constant.int 0
    %true_1329 = torch.constant.bool true
    %result0_1330, %result1_1331 = torch.aten.var_mean.correction %1223, %1224, %int0_1328, %true_1329 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1332 = torch.constant.float 1.000000e-05
    %int1_1333 = torch.constant.int 1
    %1225 = torch.aten.add.Scalar %result0_1330, %float1.000000e-05_1332, %int1_1333 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1226 = torch.aten.rsqrt %1225 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1334 = torch.constant.int 1
    %1227 = torch.aten.sub.Tensor %1222, %result1_1331, %int1_1334 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1228 = torch.aten.mul.Tensor %1227, %1226 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.weight : tensor<1280xf16>
    %1229 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1230 = torch.aten.mul.Tensor %1228, %1229 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.bias : tensor<1280xf16>
    %1231 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1335 = torch.constant.int 1
    %1232 = torch.aten.add.Tensor %1230, %1231, %int1_1335 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1336 = torch.constant.int 5
    %1233 = torch.prims.convert_element_type %1232, %int5_1336 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1337 = torch.constant.int 5
    %1234 = torch.prims.convert_element_type %result1_1331, %int5_1337 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1338 = torch.constant.int 5
    %1235 = torch.prims.convert_element_type %1226, %int5_1338 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1339 = torch.constant.int 64
    %int1280_1340 = torch.constant.int 1280
    %1236 = torch.prim.ListConstruct %int64_1339, %int1280_1340 : (!torch.int, !torch.int) -> !torch.list<int>
    %1237 = torch.aten.view %1233, %1236 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.weight : tensor<5120x1280xf16>
    %1238 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_1341 = torch.constant.int 0
    %int1_1342 = torch.constant.int 1
    %1239 = torch.aten.transpose.int %1238, %int0_1341, %int1_1342 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.bias : tensor<5120xf16>
    %1240 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1343 = torch.constant.int 6
    %1241 = torch.prims.convert_element_type %1240, %int6_1343 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1344 = torch.constant.int 6
    %1242 = torch.prims.convert_element_type %1237, %int6_1344 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1345 = torch.constant.int 6
    %1243 = torch.prims.convert_element_type %1239, %int6_1345 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %1244 = torch.aten.mm %1242, %1243 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_1346 = torch.constant.int 1
    %1245 = torch.aten.mul.Scalar %1244, %int1_1346 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_1347 = torch.constant.int 1
    %1246 = torch.aten.mul.Scalar %1241, %int1_1347 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1348 = torch.constant.int 1
    %1247 = torch.aten.add.Tensor %1245, %1246, %int1_1348 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_1349 = torch.constant.int 5
    %1248 = torch.prims.convert_element_type %1247, %int5_1349 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_1350 = torch.constant.int 1
    %int64_1351 = torch.constant.int 64
    %int5120_1352 = torch.constant.int 5120
    %1249 = torch.prim.ListConstruct %int1_1350, %int64_1351, %int5120_1352 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1250 = torch.aten.view %1248, %1249 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_1353 = torch.constant.str "none"
    %1251 = torch.aten.gelu %1250, %str_1353 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_1354 = torch.constant.int 64
    %int5120_1355 = torch.constant.int 5120
    %1252 = torch.prim.ListConstruct %int64_1354, %int5120_1355 : (!torch.int, !torch.int) -> !torch.list<int>
    %1253 = torch.aten.view %1251, %1252 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.weight : tensor<1280x5120xf16>
    %1254 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1356 = torch.constant.int 0
    %int1_1357 = torch.constant.int 1
    %1255 = torch.aten.transpose.int %1254, %int0_1356, %int1_1357 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.bias : tensor<1280xf16>
    %1256 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.7.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1358 = torch.constant.int 6
    %1257 = torch.prims.convert_element_type %1256, %int6_1358 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1359 = torch.constant.int 6
    %1258 = torch.prims.convert_element_type %1253, %int6_1359 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_1360 = torch.constant.int 6
    %1259 = torch.prims.convert_element_type %1255, %int6_1360 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1260 = torch.aten.mm %1258, %1259 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1361 = torch.constant.int 1
    %1261 = torch.aten.mul.Scalar %1260, %int1_1361 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1362 = torch.constant.int 1
    %1262 = torch.aten.mul.Scalar %1257, %int1_1362 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1363 = torch.constant.int 1
    %1263 = torch.aten.add.Tensor %1261, %1262, %int1_1363 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1364 = torch.constant.int 5
    %1264 = torch.prims.convert_element_type %1263, %int5_1364 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1365 = torch.constant.int 1
    %int64_1366 = torch.constant.int 64
    %int1280_1367 = torch.constant.int 1280
    %1265 = torch.prim.ListConstruct %int1_1365, %int64_1366, %int1280_1367 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1266 = torch.aten.view %1264, %1265 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1368 = torch.constant.int 1
    %1267 = torch.aten.add.Tensor %1222, %1266, %int1_1368 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1369 = torch.constant.int 6
    %1268 = torch.prims.convert_element_type %1267, %int6_1369 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1370 = torch.constant.int 2
    %1269 = torch.prim.ListConstruct %int2_1370 : (!torch.int) -> !torch.list<int>
    %int0_1371 = torch.constant.int 0
    %true_1372 = torch.constant.bool true
    %result0_1373, %result1_1374 = torch.aten.var_mean.correction %1268, %1269, %int0_1371, %true_1372 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1375 = torch.constant.float 1.000000e-05
    %int1_1376 = torch.constant.int 1
    %1270 = torch.aten.add.Scalar %result0_1373, %float1.000000e-05_1375, %int1_1376 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1271 = torch.aten.rsqrt %1270 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1377 = torch.constant.int 1
    %1272 = torch.aten.sub.Tensor %1267, %result1_1374, %int1_1377 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1273 = torch.aten.mul.Tensor %1272, %1271 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.weight : tensor<1280xf16>
    %1274 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1275 = torch.aten.mul.Tensor %1273, %1274 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.bias : tensor<1280xf16>
    %1276 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1378 = torch.constant.int 1
    %1277 = torch.aten.add.Tensor %1275, %1276, %int1_1378 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1379 = torch.constant.int 5
    %1278 = torch.prims.convert_element_type %1277, %int5_1379 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1380 = torch.constant.int 5
    %1279 = torch.prims.convert_element_type %result1_1374, %int5_1380 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1381 = torch.constant.int 5
    %1280 = torch.prims.convert_element_type %1271, %int5_1381 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1382 = torch.constant.int 64
    %int1280_1383 = torch.constant.int 1280
    %1281 = torch.prim.ListConstruct %int64_1382, %int1280_1383 : (!torch.int, !torch.int) -> !torch.list<int>
    %1282 = torch.aten.view %1278, %1281 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %1283 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1384 = torch.constant.int 0
    %int1_1385 = torch.constant.int 1
    %1284 = torch.aten.transpose.int %1283, %int0_1384, %int1_1385 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.bias : tensor<1280xf16>
    %1285 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1386 = torch.constant.int 6
    %1286 = torch.prims.convert_element_type %1285, %int6_1386 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1387 = torch.constant.int 6
    %1287 = torch.prims.convert_element_type %1282, %int6_1387 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1388 = torch.constant.int 6
    %1288 = torch.prims.convert_element_type %1284, %int6_1388 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1289 = torch.aten.mm %1287, %1288 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1389 = torch.constant.int 1
    %1290 = torch.aten.mul.Scalar %1289, %int1_1389 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1390 = torch.constant.int 1
    %1291 = torch.aten.mul.Scalar %1286, %int1_1390 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1391 = torch.constant.int 1
    %1292 = torch.aten.add.Tensor %1290, %1291, %int1_1391 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1392 = torch.constant.int 5
    %1293 = torch.prims.convert_element_type %1292, %int5_1392 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1393 = torch.constant.int 1
    %int64_1394 = torch.constant.int 64
    %int1280_1395 = torch.constant.int 1280
    %1294 = torch.prim.ListConstruct %int1_1393, %int64_1394, %int1280_1395 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1295 = torch.aten.view %1293, %1294 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_1396 = torch.constant.float 1.250000e-01
    %1296 = torch.aten.mul.Scalar %1295, %float1.250000e-01_1396 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_1397 = torch.constant.int 64
    %int1280_1398 = torch.constant.int 1280
    %1297 = torch.prim.ListConstruct %int64_1397, %int1280_1398 : (!torch.int, !torch.int) -> !torch.list<int>
    %1298 = torch.aten.view %1278, %1297 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %1299 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1399 = torch.constant.int 0
    %int1_1400 = torch.constant.int 1
    %1300 = torch.aten.transpose.int %1299, %int0_1399, %int1_1400 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.bias : tensor<1280xf16>
    %1301 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1401 = torch.constant.int 6
    %1302 = torch.prims.convert_element_type %1301, %int6_1401 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1402 = torch.constant.int 6
    %1303 = torch.prims.convert_element_type %1298, %int6_1402 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1403 = torch.constant.int 6
    %1304 = torch.prims.convert_element_type %1300, %int6_1403 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1305 = torch.aten.mm %1303, %1304 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1404 = torch.constant.int 1
    %1306 = torch.aten.mul.Scalar %1305, %int1_1404 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1405 = torch.constant.int 1
    %1307 = torch.aten.mul.Scalar %1302, %int1_1405 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1406 = torch.constant.int 1
    %1308 = torch.aten.add.Tensor %1306, %1307, %int1_1406 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1407 = torch.constant.int 5
    %1309 = torch.prims.convert_element_type %1308, %int5_1407 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1408 = torch.constant.int 1
    %int64_1409 = torch.constant.int 64
    %int1280_1410 = torch.constant.int 1280
    %1310 = torch.prim.ListConstruct %int1_1408, %int64_1409, %int1280_1410 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1311 = torch.aten.view %1309, %1310 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1411 = torch.constant.int 1
    %int-1_1412 = torch.constant.int -1
    %int20_1413 = torch.constant.int 20
    %int64_1414 = torch.constant.int 64
    %1312 = torch.prim.ListConstruct %int1_1411, %int-1_1412, %int20_1413, %int64_1414 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1313 = torch.aten.view %1311, %1312 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1415 = torch.constant.int 1
    %int2_1416 = torch.constant.int 2
    %1314 = torch.aten.transpose.int %1313, %int1_1415, %int2_1416 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1417 = torch.constant.int 0
    %1315 = torch.aten.clone %1314, %int0_1417 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_1418 = torch.constant.int 64
    %int1280_1419 = torch.constant.int 1280
    %1316 = torch.prim.ListConstruct %int64_1418, %int1280_1419 : (!torch.int, !torch.int) -> !torch.list<int>
    %1317 = torch.aten.view %1278, %1316 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1318 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1420 = torch.constant.int 0
    %int1_1421 = torch.constant.int 1
    %1319 = torch.aten.transpose.int %1318, %int0_1420, %int1_1421 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.bias : tensor<1280xf16>
    %1320 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1422 = torch.constant.int 6
    %1321 = torch.prims.convert_element_type %1320, %int6_1422 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1423 = torch.constant.int 6
    %1322 = torch.prims.convert_element_type %1317, %int6_1423 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1424 = torch.constant.int 6
    %1323 = torch.prims.convert_element_type %1319, %int6_1424 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1324 = torch.aten.mm %1322, %1323 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1425 = torch.constant.int 1
    %1325 = torch.aten.mul.Scalar %1324, %int1_1425 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1426 = torch.constant.int 1
    %1326 = torch.aten.mul.Scalar %1321, %int1_1426 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1427 = torch.constant.int 1
    %1327 = torch.aten.add.Tensor %1325, %1326, %int1_1427 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1428 = torch.constant.int 5
    %1328 = torch.prims.convert_element_type %1327, %int5_1428 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1429 = torch.constant.int 1
    %int64_1430 = torch.constant.int 64
    %int1280_1431 = torch.constant.int 1280
    %1329 = torch.prim.ListConstruct %int1_1429, %int64_1430, %int1280_1431 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1330 = torch.aten.view %1328, %1329 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1432 = torch.constant.int 1
    %int-1_1433 = torch.constant.int -1
    %int20_1434 = torch.constant.int 20
    %int64_1435 = torch.constant.int 64
    %1331 = torch.prim.ListConstruct %int1_1432, %int-1_1433, %int20_1434, %int64_1435 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1332 = torch.aten.view %1330, %1331 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1436 = torch.constant.int 1
    %int2_1437 = torch.constant.int 2
    %1333 = torch.aten.transpose.int %1332, %int1_1436, %int2_1437 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1438 = torch.constant.int 0
    %1334 = torch.aten.clone %1333, %int0_1438 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1439 = torch.constant.int 1
    %int64_1440 = torch.constant.int 64
    %int20_1441 = torch.constant.int 20
    %int64_1442 = torch.constant.int 64
    %1335 = torch.prim.ListConstruct %int1_1439, %int64_1440, %int20_1441, %int64_1442 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1336 = torch.aten.view %1296, %1335 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1443 = torch.constant.int 1
    %int2_1444 = torch.constant.int 2
    %1337 = torch.aten.transpose.int %1336, %int1_1443, %int2_1444 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1445 = torch.constant.int 0
    %1338 = torch.aten.clone %1337, %int0_1445 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1446 = torch.constant.int 20
    %int-1_1447 = torch.constant.int -1
    %int64_1448 = torch.constant.int 64
    %1339 = torch.prim.ListConstruct %int20_1446, %int-1_1447, %int64_1448 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1340 = torch.aten.view %1338, %1339 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1449 = torch.constant.int 20
    %int-1_1450 = torch.constant.int -1
    %int64_1451 = torch.constant.int 64
    %1341 = torch.prim.ListConstruct %int20_1449, %int-1_1450, %int64_1451 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1342 = torch.aten.view %1315, %1341 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1452 = torch.constant.int 20
    %int-1_1453 = torch.constant.int -1
    %int64_1454 = torch.constant.int 64
    %1343 = torch.prim.ListConstruct %int20_1452, %int-1_1453, %int64_1454 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1344 = torch.aten.view %1334, %1343 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_1455 = torch.constant.int 1
    %int2_1456 = torch.constant.int 2
    %1345 = torch.aten.transpose.int %1342, %int1_1455, %int2_1456 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1346 = torch.aten.bmm %1340, %1345 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1457 = torch.constant.int 1
    %int20_1458 = torch.constant.int 20
    %int64_1459 = torch.constant.int 64
    %int64_1460 = torch.constant.int 64
    %1347 = torch.prim.ListConstruct %int1_1457, %int20_1458, %int64_1459, %int64_1460 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1348 = torch.aten.view %1346, %1347 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1461 = torch.constant.int 1
    %1349 = torch.aten.add.Tensor %1348, %27, %int1_1461 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1462 = torch.constant.int 20
    %int64_1463 = torch.constant.int 64
    %int64_1464 = torch.constant.int 64
    %1350 = torch.prim.ListConstruct %int20_1462, %int64_1463, %int64_1464 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1351 = torch.aten.view %1349, %1350 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_1465 = torch.constant.int -1
    %false_1466 = torch.constant.bool false
    %1352 = torch.aten._softmax %1351, %int-1_1465, %false_1466 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1353 = torch.aten.detach %1352 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_1467 = torch.constant.none
    %1354 = torch.aten.clone %1352, %none_1467 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1355 = torch.aten.bmm %1354, %1344 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1468 = torch.constant.int 1
    %int20_1469 = torch.constant.int 20
    %int64_1470 = torch.constant.int 64
    %int64_1471 = torch.constant.int 64
    %1356 = torch.prim.ListConstruct %int1_1468, %int20_1469, %int64_1470, %int64_1471 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1357 = torch.aten.view %1355, %1356 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1472 = torch.constant.int 1
    %int2_1473 = torch.constant.int 2
    %1358 = torch.aten.transpose.int %1357, %int1_1472, %int2_1473 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_1474 = torch.constant.int 0
    %1359 = torch.aten.clone %1358, %int0_1474 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1475 = torch.constant.int 1
    %int64_1476 = torch.constant.int 64
    %int1280_1477 = torch.constant.int 1280
    %1360 = torch.prim.ListConstruct %int1_1475, %int64_1476, %int1280_1477 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1361 = torch.aten._unsafe_view %1359, %1360 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_1478 = torch.constant.int 64
    %int1280_1479 = torch.constant.int 1280
    %1362 = torch.prim.ListConstruct %int64_1478, %int1280_1479 : (!torch.int, !torch.int) -> !torch.list<int>
    %1363 = torch.aten.view %1361, %1362 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1364 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1480 = torch.constant.int 0
    %int1_1481 = torch.constant.int 1
    %1365 = torch.aten.transpose.int %1364, %int0_1480, %int1_1481 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.bias : tensor<1280xf16>
    %1366 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1482 = torch.constant.int 6
    %1367 = torch.prims.convert_element_type %1366, %int6_1482 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1483 = torch.constant.int 6
    %1368 = torch.prims.convert_element_type %1363, %int6_1483 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1484 = torch.constant.int 6
    %1369 = torch.prims.convert_element_type %1365, %int6_1484 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1370 = torch.aten.mm %1368, %1369 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1485 = torch.constant.int 1
    %1371 = torch.aten.mul.Scalar %1370, %int1_1485 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1486 = torch.constant.int 1
    %1372 = torch.aten.mul.Scalar %1367, %int1_1486 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1487 = torch.constant.int 1
    %1373 = torch.aten.add.Tensor %1371, %1372, %int1_1487 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1488 = torch.constant.int 5
    %1374 = torch.prims.convert_element_type %1373, %int5_1488 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1489 = torch.constant.int 1
    %int64_1490 = torch.constant.int 64
    %int1280_1491 = torch.constant.int 1280
    %1375 = torch.prim.ListConstruct %int1_1489, %int64_1490, %int1280_1491 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1376 = torch.aten.view %1374, %1375 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1492 = torch.constant.int 1
    %1377 = torch.aten.add.Tensor %1267, %1376, %int1_1492 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1493 = torch.constant.int 6
    %1378 = torch.prims.convert_element_type %1377, %int6_1493 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1494 = torch.constant.int 2
    %1379 = torch.prim.ListConstruct %int2_1494 : (!torch.int) -> !torch.list<int>
    %int0_1495 = torch.constant.int 0
    %true_1496 = torch.constant.bool true
    %result0_1497, %result1_1498 = torch.aten.var_mean.correction %1378, %1379, %int0_1495, %true_1496 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1499 = torch.constant.float 1.000000e-05
    %int1_1500 = torch.constant.int 1
    %1380 = torch.aten.add.Scalar %result0_1497, %float1.000000e-05_1499, %int1_1500 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1381 = torch.aten.rsqrt %1380 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1501 = torch.constant.int 1
    %1382 = torch.aten.sub.Tensor %1377, %result1_1498, %int1_1501 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1383 = torch.aten.mul.Tensor %1382, %1381 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.weight : tensor<1280xf16>
    %1384 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1385 = torch.aten.mul.Tensor %1383, %1384 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.bias : tensor<1280xf16>
    %1386 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1502 = torch.constant.int 1
    %1387 = torch.aten.add.Tensor %1385, %1386, %int1_1502 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1503 = torch.constant.int 5
    %1388 = torch.prims.convert_element_type %1387, %int5_1503 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1504 = torch.constant.int 5
    %1389 = torch.prims.convert_element_type %result1_1498, %int5_1504 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1505 = torch.constant.int 5
    %1390 = torch.prims.convert_element_type %1381, %int5_1505 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1506 = torch.constant.int 64
    %int1280_1507 = torch.constant.int 1280
    %1391 = torch.prim.ListConstruct %int64_1506, %int1280_1507 : (!torch.int, !torch.int) -> !torch.list<int>
    %1392 = torch.aten.view %1388, %1391 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.weight : tensor<5120x1280xf16>
    %1393 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_1508 = torch.constant.int 0
    %int1_1509 = torch.constant.int 1
    %1394 = torch.aten.transpose.int %1393, %int0_1508, %int1_1509 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.bias : tensor<5120xf16>
    %1395 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1510 = torch.constant.int 6
    %1396 = torch.prims.convert_element_type %1395, %int6_1510 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1511 = torch.constant.int 6
    %1397 = torch.prims.convert_element_type %1392, %int6_1511 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1512 = torch.constant.int 6
    %1398 = torch.prims.convert_element_type %1394, %int6_1512 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %1399 = torch.aten.mm %1397, %1398 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_1513 = torch.constant.int 1
    %1400 = torch.aten.mul.Scalar %1399, %int1_1513 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_1514 = torch.constant.int 1
    %1401 = torch.aten.mul.Scalar %1396, %int1_1514 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1515 = torch.constant.int 1
    %1402 = torch.aten.add.Tensor %1400, %1401, %int1_1515 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_1516 = torch.constant.int 5
    %1403 = torch.prims.convert_element_type %1402, %int5_1516 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_1517 = torch.constant.int 1
    %int64_1518 = torch.constant.int 64
    %int5120_1519 = torch.constant.int 5120
    %1404 = torch.prim.ListConstruct %int1_1517, %int64_1518, %int5120_1519 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1405 = torch.aten.view %1403, %1404 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_1520 = torch.constant.str "none"
    %1406 = torch.aten.gelu %1405, %str_1520 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_1521 = torch.constant.int 64
    %int5120_1522 = torch.constant.int 5120
    %1407 = torch.prim.ListConstruct %int64_1521, %int5120_1522 : (!torch.int, !torch.int) -> !torch.list<int>
    %1408 = torch.aten.view %1406, %1407 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.weight : tensor<1280x5120xf16>
    %1409 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1523 = torch.constant.int 0
    %int1_1524 = torch.constant.int 1
    %1410 = torch.aten.transpose.int %1409, %int0_1523, %int1_1524 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.bias : tensor<1280xf16>
    %1411 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.8.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1525 = torch.constant.int 6
    %1412 = torch.prims.convert_element_type %1411, %int6_1525 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1526 = torch.constant.int 6
    %1413 = torch.prims.convert_element_type %1408, %int6_1526 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_1527 = torch.constant.int 6
    %1414 = torch.prims.convert_element_type %1410, %int6_1527 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1415 = torch.aten.mm %1413, %1414 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1528 = torch.constant.int 1
    %1416 = torch.aten.mul.Scalar %1415, %int1_1528 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1529 = torch.constant.int 1
    %1417 = torch.aten.mul.Scalar %1412, %int1_1529 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1530 = torch.constant.int 1
    %1418 = torch.aten.add.Tensor %1416, %1417, %int1_1530 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1531 = torch.constant.int 5
    %1419 = torch.prims.convert_element_type %1418, %int5_1531 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1532 = torch.constant.int 1
    %int64_1533 = torch.constant.int 64
    %int1280_1534 = torch.constant.int 1280
    %1420 = torch.prim.ListConstruct %int1_1532, %int64_1533, %int1280_1534 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1421 = torch.aten.view %1419, %1420 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1535 = torch.constant.int 1
    %1422 = torch.aten.add.Tensor %1377, %1421, %int1_1535 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1536 = torch.constant.int 6
    %1423 = torch.prims.convert_element_type %1422, %int6_1536 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1537 = torch.constant.int 2
    %1424 = torch.prim.ListConstruct %int2_1537 : (!torch.int) -> !torch.list<int>
    %int0_1538 = torch.constant.int 0
    %true_1539 = torch.constant.bool true
    %result0_1540, %result1_1541 = torch.aten.var_mean.correction %1423, %1424, %int0_1538, %true_1539 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1542 = torch.constant.float 1.000000e-05
    %int1_1543 = torch.constant.int 1
    %1425 = torch.aten.add.Scalar %result0_1540, %float1.000000e-05_1542, %int1_1543 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1426 = torch.aten.rsqrt %1425 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1544 = torch.constant.int 1
    %1427 = torch.aten.sub.Tensor %1422, %result1_1541, %int1_1544 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1428 = torch.aten.mul.Tensor %1427, %1426 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.weight : tensor<1280xf16>
    %1429 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1430 = torch.aten.mul.Tensor %1428, %1429 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.bias : tensor<1280xf16>
    %1431 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1545 = torch.constant.int 1
    %1432 = torch.aten.add.Tensor %1430, %1431, %int1_1545 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1546 = torch.constant.int 5
    %1433 = torch.prims.convert_element_type %1432, %int5_1546 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1547 = torch.constant.int 5
    %1434 = torch.prims.convert_element_type %result1_1541, %int5_1547 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1548 = torch.constant.int 5
    %1435 = torch.prims.convert_element_type %1426, %int5_1548 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1549 = torch.constant.int 64
    %int1280_1550 = torch.constant.int 1280
    %1436 = torch.prim.ListConstruct %int64_1549, %int1280_1550 : (!torch.int, !torch.int) -> !torch.list<int>
    %1437 = torch.aten.view %1433, %1436 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %1438 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1551 = torch.constant.int 0
    %int1_1552 = torch.constant.int 1
    %1439 = torch.aten.transpose.int %1438, %int0_1551, %int1_1552 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.bias : tensor<1280xf16>
    %1440 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1553 = torch.constant.int 6
    %1441 = torch.prims.convert_element_type %1440, %int6_1553 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1554 = torch.constant.int 6
    %1442 = torch.prims.convert_element_type %1437, %int6_1554 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1555 = torch.constant.int 6
    %1443 = torch.prims.convert_element_type %1439, %int6_1555 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1444 = torch.aten.mm %1442, %1443 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1556 = torch.constant.int 1
    %1445 = torch.aten.mul.Scalar %1444, %int1_1556 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1557 = torch.constant.int 1
    %1446 = torch.aten.mul.Scalar %1441, %int1_1557 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1558 = torch.constant.int 1
    %1447 = torch.aten.add.Tensor %1445, %1446, %int1_1558 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1559 = torch.constant.int 5
    %1448 = torch.prims.convert_element_type %1447, %int5_1559 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1560 = torch.constant.int 1
    %int64_1561 = torch.constant.int 64
    %int1280_1562 = torch.constant.int 1280
    %1449 = torch.prim.ListConstruct %int1_1560, %int64_1561, %int1280_1562 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1450 = torch.aten.view %1448, %1449 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_1563 = torch.constant.float 1.250000e-01
    %1451 = torch.aten.mul.Scalar %1450, %float1.250000e-01_1563 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_1564 = torch.constant.int 64
    %int1280_1565 = torch.constant.int 1280
    %1452 = torch.prim.ListConstruct %int64_1564, %int1280_1565 : (!torch.int, !torch.int) -> !torch.list<int>
    %1453 = torch.aten.view %1433, %1452 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %1454 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1566 = torch.constant.int 0
    %int1_1567 = torch.constant.int 1
    %1455 = torch.aten.transpose.int %1454, %int0_1566, %int1_1567 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.bias : tensor<1280xf16>
    %1456 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1568 = torch.constant.int 6
    %1457 = torch.prims.convert_element_type %1456, %int6_1568 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1569 = torch.constant.int 6
    %1458 = torch.prims.convert_element_type %1453, %int6_1569 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1570 = torch.constant.int 6
    %1459 = torch.prims.convert_element_type %1455, %int6_1570 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1460 = torch.aten.mm %1458, %1459 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1571 = torch.constant.int 1
    %1461 = torch.aten.mul.Scalar %1460, %int1_1571 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1572 = torch.constant.int 1
    %1462 = torch.aten.mul.Scalar %1457, %int1_1572 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1573 = torch.constant.int 1
    %1463 = torch.aten.add.Tensor %1461, %1462, %int1_1573 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1574 = torch.constant.int 5
    %1464 = torch.prims.convert_element_type %1463, %int5_1574 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1575 = torch.constant.int 1
    %int64_1576 = torch.constant.int 64
    %int1280_1577 = torch.constant.int 1280
    %1465 = torch.prim.ListConstruct %int1_1575, %int64_1576, %int1280_1577 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1466 = torch.aten.view %1464, %1465 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1578 = torch.constant.int 1
    %int-1_1579 = torch.constant.int -1
    %int20_1580 = torch.constant.int 20
    %int64_1581 = torch.constant.int 64
    %1467 = torch.prim.ListConstruct %int1_1578, %int-1_1579, %int20_1580, %int64_1581 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1468 = torch.aten.view %1466, %1467 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1582 = torch.constant.int 1
    %int2_1583 = torch.constant.int 2
    %1469 = torch.aten.transpose.int %1468, %int1_1582, %int2_1583 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1584 = torch.constant.int 0
    %1470 = torch.aten.clone %1469, %int0_1584 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_1585 = torch.constant.int 64
    %int1280_1586 = torch.constant.int 1280
    %1471 = torch.prim.ListConstruct %int64_1585, %int1280_1586 : (!torch.int, !torch.int) -> !torch.list<int>
    %1472 = torch.aten.view %1433, %1471 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1473 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1587 = torch.constant.int 0
    %int1_1588 = torch.constant.int 1
    %1474 = torch.aten.transpose.int %1473, %int0_1587, %int1_1588 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.bias : tensor<1280xf16>
    %1475 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1589 = torch.constant.int 6
    %1476 = torch.prims.convert_element_type %1475, %int6_1589 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1590 = torch.constant.int 6
    %1477 = torch.prims.convert_element_type %1472, %int6_1590 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1591 = torch.constant.int 6
    %1478 = torch.prims.convert_element_type %1474, %int6_1591 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1479 = torch.aten.mm %1477, %1478 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1592 = torch.constant.int 1
    %1480 = torch.aten.mul.Scalar %1479, %int1_1592 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1593 = torch.constant.int 1
    %1481 = torch.aten.mul.Scalar %1476, %int1_1593 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1594 = torch.constant.int 1
    %1482 = torch.aten.add.Tensor %1480, %1481, %int1_1594 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1595 = torch.constant.int 5
    %1483 = torch.prims.convert_element_type %1482, %int5_1595 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1596 = torch.constant.int 1
    %int64_1597 = torch.constant.int 64
    %int1280_1598 = torch.constant.int 1280
    %1484 = torch.prim.ListConstruct %int1_1596, %int64_1597, %int1280_1598 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1485 = torch.aten.view %1483, %1484 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1599 = torch.constant.int 1
    %int-1_1600 = torch.constant.int -1
    %int20_1601 = torch.constant.int 20
    %int64_1602 = torch.constant.int 64
    %1486 = torch.prim.ListConstruct %int1_1599, %int-1_1600, %int20_1601, %int64_1602 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1487 = torch.aten.view %1485, %1486 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1603 = torch.constant.int 1
    %int2_1604 = torch.constant.int 2
    %1488 = torch.aten.transpose.int %1487, %int1_1603, %int2_1604 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1605 = torch.constant.int 0
    %1489 = torch.aten.clone %1488, %int0_1605 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1606 = torch.constant.int 1
    %int64_1607 = torch.constant.int 64
    %int20_1608 = torch.constant.int 20
    %int64_1609 = torch.constant.int 64
    %1490 = torch.prim.ListConstruct %int1_1606, %int64_1607, %int20_1608, %int64_1609 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1491 = torch.aten.view %1451, %1490 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1610 = torch.constant.int 1
    %int2_1611 = torch.constant.int 2
    %1492 = torch.aten.transpose.int %1491, %int1_1610, %int2_1611 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1612 = torch.constant.int 0
    %1493 = torch.aten.clone %1492, %int0_1612 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1613 = torch.constant.int 20
    %int-1_1614 = torch.constant.int -1
    %int64_1615 = torch.constant.int 64
    %1494 = torch.prim.ListConstruct %int20_1613, %int-1_1614, %int64_1615 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1495 = torch.aten.view %1493, %1494 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1616 = torch.constant.int 20
    %int-1_1617 = torch.constant.int -1
    %int64_1618 = torch.constant.int 64
    %1496 = torch.prim.ListConstruct %int20_1616, %int-1_1617, %int64_1618 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1497 = torch.aten.view %1470, %1496 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1619 = torch.constant.int 20
    %int-1_1620 = torch.constant.int -1
    %int64_1621 = torch.constant.int 64
    %1498 = torch.prim.ListConstruct %int20_1619, %int-1_1620, %int64_1621 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1499 = torch.aten.view %1489, %1498 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_1622 = torch.constant.int 1
    %int2_1623 = torch.constant.int 2
    %1500 = torch.aten.transpose.int %1497, %int1_1622, %int2_1623 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1501 = torch.aten.bmm %1495, %1500 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1624 = torch.constant.int 1
    %int20_1625 = torch.constant.int 20
    %int64_1626 = torch.constant.int 64
    %int64_1627 = torch.constant.int 64
    %1502 = torch.prim.ListConstruct %int1_1624, %int20_1625, %int64_1626, %int64_1627 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1503 = torch.aten.view %1501, %1502 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1628 = torch.constant.int 1
    %1504 = torch.aten.add.Tensor %1503, %27, %int1_1628 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1629 = torch.constant.int 20
    %int64_1630 = torch.constant.int 64
    %int64_1631 = torch.constant.int 64
    %1505 = torch.prim.ListConstruct %int20_1629, %int64_1630, %int64_1631 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1506 = torch.aten.view %1504, %1505 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_1632 = torch.constant.int -1
    %false_1633 = torch.constant.bool false
    %1507 = torch.aten._softmax %1506, %int-1_1632, %false_1633 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1508 = torch.aten.detach %1507 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_1634 = torch.constant.none
    %1509 = torch.aten.clone %1507, %none_1634 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1510 = torch.aten.bmm %1509, %1499 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1635 = torch.constant.int 1
    %int20_1636 = torch.constant.int 20
    %int64_1637 = torch.constant.int 64
    %int64_1638 = torch.constant.int 64
    %1511 = torch.prim.ListConstruct %int1_1635, %int20_1636, %int64_1637, %int64_1638 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1512 = torch.aten.view %1510, %1511 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1639 = torch.constant.int 1
    %int2_1640 = torch.constant.int 2
    %1513 = torch.aten.transpose.int %1512, %int1_1639, %int2_1640 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_1641 = torch.constant.int 0
    %1514 = torch.aten.clone %1513, %int0_1641 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1642 = torch.constant.int 1
    %int64_1643 = torch.constant.int 64
    %int1280_1644 = torch.constant.int 1280
    %1515 = torch.prim.ListConstruct %int1_1642, %int64_1643, %int1280_1644 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1516 = torch.aten._unsafe_view %1514, %1515 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_1645 = torch.constant.int 64
    %int1280_1646 = torch.constant.int 1280
    %1517 = torch.prim.ListConstruct %int64_1645, %int1280_1646 : (!torch.int, !torch.int) -> !torch.list<int>
    %1518 = torch.aten.view %1516, %1517 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1519 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1647 = torch.constant.int 0
    %int1_1648 = torch.constant.int 1
    %1520 = torch.aten.transpose.int %1519, %int0_1647, %int1_1648 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.bias : tensor<1280xf16>
    %1521 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1649 = torch.constant.int 6
    %1522 = torch.prims.convert_element_type %1521, %int6_1649 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1650 = torch.constant.int 6
    %1523 = torch.prims.convert_element_type %1518, %int6_1650 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1651 = torch.constant.int 6
    %1524 = torch.prims.convert_element_type %1520, %int6_1651 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1525 = torch.aten.mm %1523, %1524 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1652 = torch.constant.int 1
    %1526 = torch.aten.mul.Scalar %1525, %int1_1652 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1653 = torch.constant.int 1
    %1527 = torch.aten.mul.Scalar %1522, %int1_1653 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1654 = torch.constant.int 1
    %1528 = torch.aten.add.Tensor %1526, %1527, %int1_1654 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1655 = torch.constant.int 5
    %1529 = torch.prims.convert_element_type %1528, %int5_1655 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1656 = torch.constant.int 1
    %int64_1657 = torch.constant.int 64
    %int1280_1658 = torch.constant.int 1280
    %1530 = torch.prim.ListConstruct %int1_1656, %int64_1657, %int1280_1658 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1531 = torch.aten.view %1529, %1530 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1659 = torch.constant.int 1
    %1532 = torch.aten.add.Tensor %1422, %1531, %int1_1659 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1660 = torch.constant.int 6
    %1533 = torch.prims.convert_element_type %1532, %int6_1660 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1661 = torch.constant.int 2
    %1534 = torch.prim.ListConstruct %int2_1661 : (!torch.int) -> !torch.list<int>
    %int0_1662 = torch.constant.int 0
    %true_1663 = torch.constant.bool true
    %result0_1664, %result1_1665 = torch.aten.var_mean.correction %1533, %1534, %int0_1662, %true_1663 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1666 = torch.constant.float 1.000000e-05
    %int1_1667 = torch.constant.int 1
    %1535 = torch.aten.add.Scalar %result0_1664, %float1.000000e-05_1666, %int1_1667 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1536 = torch.aten.rsqrt %1535 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1668 = torch.constant.int 1
    %1537 = torch.aten.sub.Tensor %1532, %result1_1665, %int1_1668 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1538 = torch.aten.mul.Tensor %1537, %1536 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.weight : tensor<1280xf16>
    %1539 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1540 = torch.aten.mul.Tensor %1538, %1539 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.bias : tensor<1280xf16>
    %1541 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1669 = torch.constant.int 1
    %1542 = torch.aten.add.Tensor %1540, %1541, %int1_1669 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1670 = torch.constant.int 5
    %1543 = torch.prims.convert_element_type %1542, %int5_1670 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1671 = torch.constant.int 5
    %1544 = torch.prims.convert_element_type %result1_1665, %int5_1671 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1672 = torch.constant.int 5
    %1545 = torch.prims.convert_element_type %1536, %int5_1672 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1673 = torch.constant.int 64
    %int1280_1674 = torch.constant.int 1280
    %1546 = torch.prim.ListConstruct %int64_1673, %int1280_1674 : (!torch.int, !torch.int) -> !torch.list<int>
    %1547 = torch.aten.view %1543, %1546 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.weight : tensor<5120x1280xf16>
    %1548 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_1675 = torch.constant.int 0
    %int1_1676 = torch.constant.int 1
    %1549 = torch.aten.transpose.int %1548, %int0_1675, %int1_1676 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.bias : tensor<5120xf16>
    %1550 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1677 = torch.constant.int 6
    %1551 = torch.prims.convert_element_type %1550, %int6_1677 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1678 = torch.constant.int 6
    %1552 = torch.prims.convert_element_type %1547, %int6_1678 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1679 = torch.constant.int 6
    %1553 = torch.prims.convert_element_type %1549, %int6_1679 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %1554 = torch.aten.mm %1552, %1553 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_1680 = torch.constant.int 1
    %1555 = torch.aten.mul.Scalar %1554, %int1_1680 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_1681 = torch.constant.int 1
    %1556 = torch.aten.mul.Scalar %1551, %int1_1681 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1682 = torch.constant.int 1
    %1557 = torch.aten.add.Tensor %1555, %1556, %int1_1682 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_1683 = torch.constant.int 5
    %1558 = torch.prims.convert_element_type %1557, %int5_1683 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_1684 = torch.constant.int 1
    %int64_1685 = torch.constant.int 64
    %int5120_1686 = torch.constant.int 5120
    %1559 = torch.prim.ListConstruct %int1_1684, %int64_1685, %int5120_1686 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1560 = torch.aten.view %1558, %1559 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_1687 = torch.constant.str "none"
    %1561 = torch.aten.gelu %1560, %str_1687 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_1688 = torch.constant.int 64
    %int5120_1689 = torch.constant.int 5120
    %1562 = torch.prim.ListConstruct %int64_1688, %int5120_1689 : (!torch.int, !torch.int) -> !torch.list<int>
    %1563 = torch.aten.view %1561, %1562 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.weight : tensor<1280x5120xf16>
    %1564 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1690 = torch.constant.int 0
    %int1_1691 = torch.constant.int 1
    %1565 = torch.aten.transpose.int %1564, %int0_1690, %int1_1691 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.bias : tensor<1280xf16>
    %1566 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.9.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1692 = torch.constant.int 6
    %1567 = torch.prims.convert_element_type %1566, %int6_1692 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1693 = torch.constant.int 6
    %1568 = torch.prims.convert_element_type %1563, %int6_1693 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_1694 = torch.constant.int 6
    %1569 = torch.prims.convert_element_type %1565, %int6_1694 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1570 = torch.aten.mm %1568, %1569 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1695 = torch.constant.int 1
    %1571 = torch.aten.mul.Scalar %1570, %int1_1695 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1696 = torch.constant.int 1
    %1572 = torch.aten.mul.Scalar %1567, %int1_1696 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1697 = torch.constant.int 1
    %1573 = torch.aten.add.Tensor %1571, %1572, %int1_1697 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1698 = torch.constant.int 5
    %1574 = torch.prims.convert_element_type %1573, %int5_1698 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1699 = torch.constant.int 1
    %int64_1700 = torch.constant.int 64
    %int1280_1701 = torch.constant.int 1280
    %1575 = torch.prim.ListConstruct %int1_1699, %int64_1700, %int1280_1701 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1576 = torch.aten.view %1574, %1575 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1702 = torch.constant.int 1
    %1577 = torch.aten.add.Tensor %1532, %1576, %int1_1702 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1703 = torch.constant.int 6
    %1578 = torch.prims.convert_element_type %1577, %int6_1703 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1704 = torch.constant.int 2
    %1579 = torch.prim.ListConstruct %int2_1704 : (!torch.int) -> !torch.list<int>
    %int0_1705 = torch.constant.int 0
    %true_1706 = torch.constant.bool true
    %result0_1707, %result1_1708 = torch.aten.var_mean.correction %1578, %1579, %int0_1705, %true_1706 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1709 = torch.constant.float 1.000000e-05
    %int1_1710 = torch.constant.int 1
    %1580 = torch.aten.add.Scalar %result0_1707, %float1.000000e-05_1709, %int1_1710 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1581 = torch.aten.rsqrt %1580 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1711 = torch.constant.int 1
    %1582 = torch.aten.sub.Tensor %1577, %result1_1708, %int1_1711 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1583 = torch.aten.mul.Tensor %1582, %1581 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.weight : tensor<1280xf16>
    %1584 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1585 = torch.aten.mul.Tensor %1583, %1584 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.bias : tensor<1280xf16>
    %1586 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1712 = torch.constant.int 1
    %1587 = torch.aten.add.Tensor %1585, %1586, %int1_1712 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1713 = torch.constant.int 5
    %1588 = torch.prims.convert_element_type %1587, %int5_1713 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1714 = torch.constant.int 5
    %1589 = torch.prims.convert_element_type %result1_1708, %int5_1714 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1715 = torch.constant.int 5
    %1590 = torch.prims.convert_element_type %1581, %int5_1715 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1716 = torch.constant.int 64
    %int1280_1717 = torch.constant.int 1280
    %1591 = torch.prim.ListConstruct %int64_1716, %int1280_1717 : (!torch.int, !torch.int) -> !torch.list<int>
    %1592 = torch.aten.view %1588, %1591 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %1593 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1718 = torch.constant.int 0
    %int1_1719 = torch.constant.int 1
    %1594 = torch.aten.transpose.int %1593, %int0_1718, %int1_1719 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.bias : tensor<1280xf16>
    %1595 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1720 = torch.constant.int 6
    %1596 = torch.prims.convert_element_type %1595, %int6_1720 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1721 = torch.constant.int 6
    %1597 = torch.prims.convert_element_type %1592, %int6_1721 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1722 = torch.constant.int 6
    %1598 = torch.prims.convert_element_type %1594, %int6_1722 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1599 = torch.aten.mm %1597, %1598 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1723 = torch.constant.int 1
    %1600 = torch.aten.mul.Scalar %1599, %int1_1723 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1724 = torch.constant.int 1
    %1601 = torch.aten.mul.Scalar %1596, %int1_1724 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1725 = torch.constant.int 1
    %1602 = torch.aten.add.Tensor %1600, %1601, %int1_1725 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1726 = torch.constant.int 5
    %1603 = torch.prims.convert_element_type %1602, %int5_1726 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1727 = torch.constant.int 1
    %int64_1728 = torch.constant.int 64
    %int1280_1729 = torch.constant.int 1280
    %1604 = torch.prim.ListConstruct %int1_1727, %int64_1728, %int1280_1729 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1605 = torch.aten.view %1603, %1604 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_1730 = torch.constant.float 1.250000e-01
    %1606 = torch.aten.mul.Scalar %1605, %float1.250000e-01_1730 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_1731 = torch.constant.int 64
    %int1280_1732 = torch.constant.int 1280
    %1607 = torch.prim.ListConstruct %int64_1731, %int1280_1732 : (!torch.int, !torch.int) -> !torch.list<int>
    %1608 = torch.aten.view %1588, %1607 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %1609 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1733 = torch.constant.int 0
    %int1_1734 = torch.constant.int 1
    %1610 = torch.aten.transpose.int %1609, %int0_1733, %int1_1734 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.bias : tensor<1280xf16>
    %1611 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1735 = torch.constant.int 6
    %1612 = torch.prims.convert_element_type %1611, %int6_1735 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1736 = torch.constant.int 6
    %1613 = torch.prims.convert_element_type %1608, %int6_1736 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1737 = torch.constant.int 6
    %1614 = torch.prims.convert_element_type %1610, %int6_1737 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1615 = torch.aten.mm %1613, %1614 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1738 = torch.constant.int 1
    %1616 = torch.aten.mul.Scalar %1615, %int1_1738 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1739 = torch.constant.int 1
    %1617 = torch.aten.mul.Scalar %1612, %int1_1739 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1740 = torch.constant.int 1
    %1618 = torch.aten.add.Tensor %1616, %1617, %int1_1740 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1741 = torch.constant.int 5
    %1619 = torch.prims.convert_element_type %1618, %int5_1741 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1742 = torch.constant.int 1
    %int64_1743 = torch.constant.int 64
    %int1280_1744 = torch.constant.int 1280
    %1620 = torch.prim.ListConstruct %int1_1742, %int64_1743, %int1280_1744 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1621 = torch.aten.view %1619, %1620 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1745 = torch.constant.int 1
    %int-1_1746 = torch.constant.int -1
    %int20_1747 = torch.constant.int 20
    %int64_1748 = torch.constant.int 64
    %1622 = torch.prim.ListConstruct %int1_1745, %int-1_1746, %int20_1747, %int64_1748 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1623 = torch.aten.view %1621, %1622 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1749 = torch.constant.int 1
    %int2_1750 = torch.constant.int 2
    %1624 = torch.aten.transpose.int %1623, %int1_1749, %int2_1750 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1751 = torch.constant.int 0
    %1625 = torch.aten.clone %1624, %int0_1751 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_1752 = torch.constant.int 64
    %int1280_1753 = torch.constant.int 1280
    %1626 = torch.prim.ListConstruct %int64_1752, %int1280_1753 : (!torch.int, !torch.int) -> !torch.list<int>
    %1627 = torch.aten.view %1588, %1626 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1628 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1754 = torch.constant.int 0
    %int1_1755 = torch.constant.int 1
    %1629 = torch.aten.transpose.int %1628, %int0_1754, %int1_1755 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.bias : tensor<1280xf16>
    %1630 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1756 = torch.constant.int 6
    %1631 = torch.prims.convert_element_type %1630, %int6_1756 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1757 = torch.constant.int 6
    %1632 = torch.prims.convert_element_type %1627, %int6_1757 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1758 = torch.constant.int 6
    %1633 = torch.prims.convert_element_type %1629, %int6_1758 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1634 = torch.aten.mm %1632, %1633 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1759 = torch.constant.int 1
    %1635 = torch.aten.mul.Scalar %1634, %int1_1759 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1760 = torch.constant.int 1
    %1636 = torch.aten.mul.Scalar %1631, %int1_1760 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1761 = torch.constant.int 1
    %1637 = torch.aten.add.Tensor %1635, %1636, %int1_1761 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1762 = torch.constant.int 5
    %1638 = torch.prims.convert_element_type %1637, %int5_1762 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1763 = torch.constant.int 1
    %int64_1764 = torch.constant.int 64
    %int1280_1765 = torch.constant.int 1280
    %1639 = torch.prim.ListConstruct %int1_1763, %int64_1764, %int1280_1765 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1640 = torch.aten.view %1638, %1639 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1766 = torch.constant.int 1
    %int-1_1767 = torch.constant.int -1
    %int20_1768 = torch.constant.int 20
    %int64_1769 = torch.constant.int 64
    %1641 = torch.prim.ListConstruct %int1_1766, %int-1_1767, %int20_1768, %int64_1769 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1642 = torch.aten.view %1640, %1641 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1770 = torch.constant.int 1
    %int2_1771 = torch.constant.int 2
    %1643 = torch.aten.transpose.int %1642, %int1_1770, %int2_1771 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1772 = torch.constant.int 0
    %1644 = torch.aten.clone %1643, %int0_1772 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1773 = torch.constant.int 1
    %int64_1774 = torch.constant.int 64
    %int20_1775 = torch.constant.int 20
    %int64_1776 = torch.constant.int 64
    %1645 = torch.prim.ListConstruct %int1_1773, %int64_1774, %int20_1775, %int64_1776 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1646 = torch.aten.view %1606, %1645 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1777 = torch.constant.int 1
    %int2_1778 = torch.constant.int 2
    %1647 = torch.aten.transpose.int %1646, %int1_1777, %int2_1778 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1779 = torch.constant.int 0
    %1648 = torch.aten.clone %1647, %int0_1779 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1780 = torch.constant.int 20
    %int-1_1781 = torch.constant.int -1
    %int64_1782 = torch.constant.int 64
    %1649 = torch.prim.ListConstruct %int20_1780, %int-1_1781, %int64_1782 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1650 = torch.aten.view %1648, %1649 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1783 = torch.constant.int 20
    %int-1_1784 = torch.constant.int -1
    %int64_1785 = torch.constant.int 64
    %1651 = torch.prim.ListConstruct %int20_1783, %int-1_1784, %int64_1785 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1652 = torch.aten.view %1625, %1651 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1786 = torch.constant.int 20
    %int-1_1787 = torch.constant.int -1
    %int64_1788 = torch.constant.int 64
    %1653 = torch.prim.ListConstruct %int20_1786, %int-1_1787, %int64_1788 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1654 = torch.aten.view %1644, %1653 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_1789 = torch.constant.int 1
    %int2_1790 = torch.constant.int 2
    %1655 = torch.aten.transpose.int %1652, %int1_1789, %int2_1790 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1656 = torch.aten.bmm %1650, %1655 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1791 = torch.constant.int 1
    %int20_1792 = torch.constant.int 20
    %int64_1793 = torch.constant.int 64
    %int64_1794 = torch.constant.int 64
    %1657 = torch.prim.ListConstruct %int1_1791, %int20_1792, %int64_1793, %int64_1794 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1658 = torch.aten.view %1656, %1657 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1795 = torch.constant.int 1
    %1659 = torch.aten.add.Tensor %1658, %27, %int1_1795 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1796 = torch.constant.int 20
    %int64_1797 = torch.constant.int 64
    %int64_1798 = torch.constant.int 64
    %1660 = torch.prim.ListConstruct %int20_1796, %int64_1797, %int64_1798 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1661 = torch.aten.view %1659, %1660 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_1799 = torch.constant.int -1
    %false_1800 = torch.constant.bool false
    %1662 = torch.aten._softmax %1661, %int-1_1799, %false_1800 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1663 = torch.aten.detach %1662 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_1801 = torch.constant.none
    %1664 = torch.aten.clone %1662, %none_1801 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1665 = torch.aten.bmm %1664, %1654 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1802 = torch.constant.int 1
    %int20_1803 = torch.constant.int 20
    %int64_1804 = torch.constant.int 64
    %int64_1805 = torch.constant.int 64
    %1666 = torch.prim.ListConstruct %int1_1802, %int20_1803, %int64_1804, %int64_1805 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1667 = torch.aten.view %1665, %1666 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1806 = torch.constant.int 1
    %int2_1807 = torch.constant.int 2
    %1668 = torch.aten.transpose.int %1667, %int1_1806, %int2_1807 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_1808 = torch.constant.int 0
    %1669 = torch.aten.clone %1668, %int0_1808 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1809 = torch.constant.int 1
    %int64_1810 = torch.constant.int 64
    %int1280_1811 = torch.constant.int 1280
    %1670 = torch.prim.ListConstruct %int1_1809, %int64_1810, %int1280_1811 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1671 = torch.aten._unsafe_view %1669, %1670 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_1812 = torch.constant.int 64
    %int1280_1813 = torch.constant.int 1280
    %1672 = torch.prim.ListConstruct %int64_1812, %int1280_1813 : (!torch.int, !torch.int) -> !torch.list<int>
    %1673 = torch.aten.view %1671, %1672 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1674 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1814 = torch.constant.int 0
    %int1_1815 = torch.constant.int 1
    %1675 = torch.aten.transpose.int %1674, %int0_1814, %int1_1815 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.bias : tensor<1280xf16>
    %1676 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1816 = torch.constant.int 6
    %1677 = torch.prims.convert_element_type %1676, %int6_1816 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1817 = torch.constant.int 6
    %1678 = torch.prims.convert_element_type %1673, %int6_1817 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1818 = torch.constant.int 6
    %1679 = torch.prims.convert_element_type %1675, %int6_1818 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1680 = torch.aten.mm %1678, %1679 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1819 = torch.constant.int 1
    %1681 = torch.aten.mul.Scalar %1680, %int1_1819 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1820 = torch.constant.int 1
    %1682 = torch.aten.mul.Scalar %1677, %int1_1820 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1821 = torch.constant.int 1
    %1683 = torch.aten.add.Tensor %1681, %1682, %int1_1821 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1822 = torch.constant.int 5
    %1684 = torch.prims.convert_element_type %1683, %int5_1822 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1823 = torch.constant.int 1
    %int64_1824 = torch.constant.int 64
    %int1280_1825 = torch.constant.int 1280
    %1685 = torch.prim.ListConstruct %int1_1823, %int64_1824, %int1280_1825 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1686 = torch.aten.view %1684, %1685 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1826 = torch.constant.int 1
    %1687 = torch.aten.add.Tensor %1577, %1686, %int1_1826 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1827 = torch.constant.int 6
    %1688 = torch.prims.convert_element_type %1687, %int6_1827 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1828 = torch.constant.int 2
    %1689 = torch.prim.ListConstruct %int2_1828 : (!torch.int) -> !torch.list<int>
    %int0_1829 = torch.constant.int 0
    %true_1830 = torch.constant.bool true
    %result0_1831, %result1_1832 = torch.aten.var_mean.correction %1688, %1689, %int0_1829, %true_1830 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1833 = torch.constant.float 1.000000e-05
    %int1_1834 = torch.constant.int 1
    %1690 = torch.aten.add.Scalar %result0_1831, %float1.000000e-05_1833, %int1_1834 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1691 = torch.aten.rsqrt %1690 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1835 = torch.constant.int 1
    %1692 = torch.aten.sub.Tensor %1687, %result1_1832, %int1_1835 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1693 = torch.aten.mul.Tensor %1692, %1691 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.weight : tensor<1280xf16>
    %1694 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1695 = torch.aten.mul.Tensor %1693, %1694 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.bias : tensor<1280xf16>
    %1696 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1836 = torch.constant.int 1
    %1697 = torch.aten.add.Tensor %1695, %1696, %int1_1836 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1837 = torch.constant.int 5
    %1698 = torch.prims.convert_element_type %1697, %int5_1837 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1838 = torch.constant.int 5
    %1699 = torch.prims.convert_element_type %result1_1832, %int5_1838 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1839 = torch.constant.int 5
    %1700 = torch.prims.convert_element_type %1691, %int5_1839 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1840 = torch.constant.int 64
    %int1280_1841 = torch.constant.int 1280
    %1701 = torch.prim.ListConstruct %int64_1840, %int1280_1841 : (!torch.int, !torch.int) -> !torch.list<int>
    %1702 = torch.aten.view %1698, %1701 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.weight : tensor<5120x1280xf16>
    %1703 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_1842 = torch.constant.int 0
    %int1_1843 = torch.constant.int 1
    %1704 = torch.aten.transpose.int %1703, %int0_1842, %int1_1843 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.bias : tensor<5120xf16>
    %1705 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1844 = torch.constant.int 6
    %1706 = torch.prims.convert_element_type %1705, %int6_1844 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1845 = torch.constant.int 6
    %1707 = torch.prims.convert_element_type %1702, %int6_1845 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1846 = torch.constant.int 6
    %1708 = torch.prims.convert_element_type %1704, %int6_1846 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %1709 = torch.aten.mm %1707, %1708 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_1847 = torch.constant.int 1
    %1710 = torch.aten.mul.Scalar %1709, %int1_1847 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_1848 = torch.constant.int 1
    %1711 = torch.aten.mul.Scalar %1706, %int1_1848 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1849 = torch.constant.int 1
    %1712 = torch.aten.add.Tensor %1710, %1711, %int1_1849 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_1850 = torch.constant.int 5
    %1713 = torch.prims.convert_element_type %1712, %int5_1850 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_1851 = torch.constant.int 1
    %int64_1852 = torch.constant.int 64
    %int5120_1853 = torch.constant.int 5120
    %1714 = torch.prim.ListConstruct %int1_1851, %int64_1852, %int5120_1853 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1715 = torch.aten.view %1713, %1714 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_1854 = torch.constant.str "none"
    %1716 = torch.aten.gelu %1715, %str_1854 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_1855 = torch.constant.int 64
    %int5120_1856 = torch.constant.int 5120
    %1717 = torch.prim.ListConstruct %int64_1855, %int5120_1856 : (!torch.int, !torch.int) -> !torch.list<int>
    %1718 = torch.aten.view %1716, %1717 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.weight : tensor<1280x5120xf16>
    %1719 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1857 = torch.constant.int 0
    %int1_1858 = torch.constant.int 1
    %1720 = torch.aten.transpose.int %1719, %int0_1857, %int1_1858 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.bias : tensor<1280xf16>
    %1721 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.10.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1859 = torch.constant.int 6
    %1722 = torch.prims.convert_element_type %1721, %int6_1859 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1860 = torch.constant.int 6
    %1723 = torch.prims.convert_element_type %1718, %int6_1860 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_1861 = torch.constant.int 6
    %1724 = torch.prims.convert_element_type %1720, %int6_1861 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1725 = torch.aten.mm %1723, %1724 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1862 = torch.constant.int 1
    %1726 = torch.aten.mul.Scalar %1725, %int1_1862 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1863 = torch.constant.int 1
    %1727 = torch.aten.mul.Scalar %1722, %int1_1863 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1864 = torch.constant.int 1
    %1728 = torch.aten.add.Tensor %1726, %1727, %int1_1864 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1865 = torch.constant.int 5
    %1729 = torch.prims.convert_element_type %1728, %int5_1865 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1866 = torch.constant.int 1
    %int64_1867 = torch.constant.int 64
    %int1280_1868 = torch.constant.int 1280
    %1730 = torch.prim.ListConstruct %int1_1866, %int64_1867, %int1280_1868 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1731 = torch.aten.view %1729, %1730 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1869 = torch.constant.int 1
    %1732 = torch.aten.add.Tensor %1687, %1731, %int1_1869 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1870 = torch.constant.int 6
    %1733 = torch.prims.convert_element_type %1732, %int6_1870 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1871 = torch.constant.int 2
    %1734 = torch.prim.ListConstruct %int2_1871 : (!torch.int) -> !torch.list<int>
    %int0_1872 = torch.constant.int 0
    %true_1873 = torch.constant.bool true
    %result0_1874, %result1_1875 = torch.aten.var_mean.correction %1733, %1734, %int0_1872, %true_1873 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_1876 = torch.constant.float 1.000000e-05
    %int1_1877 = torch.constant.int 1
    %1735 = torch.aten.add.Scalar %result0_1874, %float1.000000e-05_1876, %int1_1877 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1736 = torch.aten.rsqrt %1735 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_1878 = torch.constant.int 1
    %1737 = torch.aten.sub.Tensor %1732, %result1_1875, %int1_1878 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1738 = torch.aten.mul.Tensor %1737, %1736 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.weight : tensor<1280xf16>
    %1739 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1740 = torch.aten.mul.Tensor %1738, %1739 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.bias : tensor<1280xf16>
    %1741 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1879 = torch.constant.int 1
    %1742 = torch.aten.add.Tensor %1740, %1741, %int1_1879 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_1880 = torch.constant.int 5
    %1743 = torch.prims.convert_element_type %1742, %int5_1880 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_1881 = torch.constant.int 5
    %1744 = torch.prims.convert_element_type %result1_1875, %int5_1881 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_1882 = torch.constant.int 5
    %1745 = torch.prims.convert_element_type %1736, %int5_1882 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_1883 = torch.constant.int 64
    %int1280_1884 = torch.constant.int 1280
    %1746 = torch.prim.ListConstruct %int64_1883, %int1280_1884 : (!torch.int, !torch.int) -> !torch.list<int>
    %1747 = torch.aten.view %1743, %1746 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %1748 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1885 = torch.constant.int 0
    %int1_1886 = torch.constant.int 1
    %1749 = torch.aten.transpose.int %1748, %int0_1885, %int1_1886 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.bias : tensor<1280xf16>
    %1750 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1887 = torch.constant.int 6
    %1751 = torch.prims.convert_element_type %1750, %int6_1887 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1888 = torch.constant.int 6
    %1752 = torch.prims.convert_element_type %1747, %int6_1888 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1889 = torch.constant.int 6
    %1753 = torch.prims.convert_element_type %1749, %int6_1889 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1754 = torch.aten.mm %1752, %1753 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1890 = torch.constant.int 1
    %1755 = torch.aten.mul.Scalar %1754, %int1_1890 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1891 = torch.constant.int 1
    %1756 = torch.aten.mul.Scalar %1751, %int1_1891 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1892 = torch.constant.int 1
    %1757 = torch.aten.add.Tensor %1755, %1756, %int1_1892 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1893 = torch.constant.int 5
    %1758 = torch.prims.convert_element_type %1757, %int5_1893 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1894 = torch.constant.int 1
    %int64_1895 = torch.constant.int 64
    %int1280_1896 = torch.constant.int 1280
    %1759 = torch.prim.ListConstruct %int1_1894, %int64_1895, %int1280_1896 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1760 = torch.aten.view %1758, %1759 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_1897 = torch.constant.float 1.250000e-01
    %1761 = torch.aten.mul.Scalar %1760, %float1.250000e-01_1897 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_1898 = torch.constant.int 64
    %int1280_1899 = torch.constant.int 1280
    %1762 = torch.prim.ListConstruct %int64_1898, %int1280_1899 : (!torch.int, !torch.int) -> !torch.list<int>
    %1763 = torch.aten.view %1743, %1762 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %1764 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1900 = torch.constant.int 0
    %int1_1901 = torch.constant.int 1
    %1765 = torch.aten.transpose.int %1764, %int0_1900, %int1_1901 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.bias : tensor<1280xf16>
    %1766 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1902 = torch.constant.int 6
    %1767 = torch.prims.convert_element_type %1766, %int6_1902 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1903 = torch.constant.int 6
    %1768 = torch.prims.convert_element_type %1763, %int6_1903 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1904 = torch.constant.int 6
    %1769 = torch.prims.convert_element_type %1765, %int6_1904 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1770 = torch.aten.mm %1768, %1769 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1905 = torch.constant.int 1
    %1771 = torch.aten.mul.Scalar %1770, %int1_1905 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1906 = torch.constant.int 1
    %1772 = torch.aten.mul.Scalar %1767, %int1_1906 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1907 = torch.constant.int 1
    %1773 = torch.aten.add.Tensor %1771, %1772, %int1_1907 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1908 = torch.constant.int 5
    %1774 = torch.prims.convert_element_type %1773, %int5_1908 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1909 = torch.constant.int 1
    %int64_1910 = torch.constant.int 64
    %int1280_1911 = torch.constant.int 1280
    %1775 = torch.prim.ListConstruct %int1_1909, %int64_1910, %int1280_1911 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1776 = torch.aten.view %1774, %1775 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1912 = torch.constant.int 1
    %int-1_1913 = torch.constant.int -1
    %int20_1914 = torch.constant.int 20
    %int64_1915 = torch.constant.int 64
    %1777 = torch.prim.ListConstruct %int1_1912, %int-1_1913, %int20_1914, %int64_1915 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1778 = torch.aten.view %1776, %1777 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1916 = torch.constant.int 1
    %int2_1917 = torch.constant.int 2
    %1779 = torch.aten.transpose.int %1778, %int1_1916, %int2_1917 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1918 = torch.constant.int 0
    %1780 = torch.aten.clone %1779, %int0_1918 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_1919 = torch.constant.int 64
    %int1280_1920 = torch.constant.int 1280
    %1781 = torch.prim.ListConstruct %int64_1919, %int1280_1920 : (!torch.int, !torch.int) -> !torch.list<int>
    %1782 = torch.aten.view %1743, %1781 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1783 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1921 = torch.constant.int 0
    %int1_1922 = torch.constant.int 1
    %1784 = torch.aten.transpose.int %1783, %int0_1921, %int1_1922 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.bias : tensor<1280xf16>
    %1785 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1923 = torch.constant.int 6
    %1786 = torch.prims.convert_element_type %1785, %int6_1923 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1924 = torch.constant.int 6
    %1787 = torch.prims.convert_element_type %1782, %int6_1924 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1925 = torch.constant.int 6
    %1788 = torch.prims.convert_element_type %1784, %int6_1925 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1789 = torch.aten.mm %1787, %1788 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1926 = torch.constant.int 1
    %1790 = torch.aten.mul.Scalar %1789, %int1_1926 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1927 = torch.constant.int 1
    %1791 = torch.aten.mul.Scalar %1786, %int1_1927 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1928 = torch.constant.int 1
    %1792 = torch.aten.add.Tensor %1790, %1791, %int1_1928 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1929 = torch.constant.int 5
    %1793 = torch.prims.convert_element_type %1792, %int5_1929 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1930 = torch.constant.int 1
    %int64_1931 = torch.constant.int 64
    %int1280_1932 = torch.constant.int 1280
    %1794 = torch.prim.ListConstruct %int1_1930, %int64_1931, %int1280_1932 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1795 = torch.aten.view %1793, %1794 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1933 = torch.constant.int 1
    %int-1_1934 = torch.constant.int -1
    %int20_1935 = torch.constant.int 20
    %int64_1936 = torch.constant.int 64
    %1796 = torch.prim.ListConstruct %int1_1933, %int-1_1934, %int20_1935, %int64_1936 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1797 = torch.aten.view %1795, %1796 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1937 = torch.constant.int 1
    %int2_1938 = torch.constant.int 2
    %1798 = torch.aten.transpose.int %1797, %int1_1937, %int2_1938 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1939 = torch.constant.int 0
    %1799 = torch.aten.clone %1798, %int0_1939 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1940 = torch.constant.int 1
    %int64_1941 = torch.constant.int 64
    %int20_1942 = torch.constant.int 20
    %int64_1943 = torch.constant.int 64
    %1800 = torch.prim.ListConstruct %int1_1940, %int64_1941, %int20_1942, %int64_1943 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1801 = torch.aten.view %1761, %1800 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1944 = torch.constant.int 1
    %int2_1945 = torch.constant.int 2
    %1802 = torch.aten.transpose.int %1801, %int1_1944, %int2_1945 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_1946 = torch.constant.int 0
    %1803 = torch.aten.clone %1802, %int0_1946 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1947 = torch.constant.int 20
    %int-1_1948 = torch.constant.int -1
    %int64_1949 = torch.constant.int 64
    %1804 = torch.prim.ListConstruct %int20_1947, %int-1_1948, %int64_1949 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1805 = torch.aten.view %1803, %1804 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1950 = torch.constant.int 20
    %int-1_1951 = torch.constant.int -1
    %int64_1952 = torch.constant.int 64
    %1806 = torch.prim.ListConstruct %int20_1950, %int-1_1951, %int64_1952 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1807 = torch.aten.view %1780, %1806 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_1953 = torch.constant.int 20
    %int-1_1954 = torch.constant.int -1
    %int64_1955 = torch.constant.int 64
    %1808 = torch.prim.ListConstruct %int20_1953, %int-1_1954, %int64_1955 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1809 = torch.aten.view %1799, %1808 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_1956 = torch.constant.int 1
    %int2_1957 = torch.constant.int 2
    %1810 = torch.aten.transpose.int %1807, %int1_1956, %int2_1957 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1811 = torch.aten.bmm %1805, %1810 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1958 = torch.constant.int 1
    %int20_1959 = torch.constant.int 20
    %int64_1960 = torch.constant.int 64
    %int64_1961 = torch.constant.int 64
    %1812 = torch.prim.ListConstruct %int1_1958, %int20_1959, %int64_1960, %int64_1961 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1813 = torch.aten.view %1811, %1812 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1962 = torch.constant.int 1
    %1814 = torch.aten.add.Tensor %1813, %27, %int1_1962 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_1963 = torch.constant.int 20
    %int64_1964 = torch.constant.int 64
    %int64_1965 = torch.constant.int 64
    %1815 = torch.prim.ListConstruct %int20_1963, %int64_1964, %int64_1965 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1816 = torch.aten.view %1814, %1815 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_1966 = torch.constant.int -1
    %false_1967 = torch.constant.bool false
    %1817 = torch.aten._softmax %1816, %int-1_1966, %false_1967 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1818 = torch.aten.detach %1817 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_1968 = torch.constant.none
    %1819 = torch.aten.clone %1817, %none_1968 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1820 = torch.aten.bmm %1819, %1809 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_1969 = torch.constant.int 1
    %int20_1970 = torch.constant.int 20
    %int64_1971 = torch.constant.int 64
    %int64_1972 = torch.constant.int 64
    %1821 = torch.prim.ListConstruct %int1_1969, %int20_1970, %int64_1971, %int64_1972 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1822 = torch.aten.view %1820, %1821 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_1973 = torch.constant.int 1
    %int2_1974 = torch.constant.int 2
    %1823 = torch.aten.transpose.int %1822, %int1_1973, %int2_1974 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_1975 = torch.constant.int 0
    %1824 = torch.aten.clone %1823, %int0_1975 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_1976 = torch.constant.int 1
    %int64_1977 = torch.constant.int 64
    %int1280_1978 = torch.constant.int 1280
    %1825 = torch.prim.ListConstruct %int1_1976, %int64_1977, %int1280_1978 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1826 = torch.aten._unsafe_view %1824, %1825 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_1979 = torch.constant.int 64
    %int1280_1980 = torch.constant.int 1280
    %1827 = torch.prim.ListConstruct %int64_1979, %int1280_1980 : (!torch.int, !torch.int) -> !torch.list<int>
    %1828 = torch.aten.view %1826, %1827 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1829 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1981 = torch.constant.int 0
    %int1_1982 = torch.constant.int 1
    %1830 = torch.aten.transpose.int %1829, %int0_1981, %int1_1982 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.bias : tensor<1280xf16>
    %1831 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1983 = torch.constant.int 6
    %1832 = torch.prims.convert_element_type %1831, %int6_1983 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1984 = torch.constant.int 6
    %1833 = torch.prims.convert_element_type %1828, %int6_1984 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_1985 = torch.constant.int 6
    %1834 = torch.prims.convert_element_type %1830, %int6_1985 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1835 = torch.aten.mm %1833, %1834 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_1986 = torch.constant.int 1
    %1836 = torch.aten.mul.Scalar %1835, %int1_1986 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_1987 = torch.constant.int 1
    %1837 = torch.aten.mul.Scalar %1832, %int1_1987 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1988 = torch.constant.int 1
    %1838 = torch.aten.add.Tensor %1836, %1837, %int1_1988 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_1989 = torch.constant.int 5
    %1839 = torch.prims.convert_element_type %1838, %int5_1989 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_1990 = torch.constant.int 1
    %int64_1991 = torch.constant.int 64
    %int1280_1992 = torch.constant.int 1280
    %1840 = torch.prim.ListConstruct %int1_1990, %int64_1991, %int1280_1992 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1841 = torch.aten.view %1839, %1840 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_1993 = torch.constant.int 1
    %1842 = torch.aten.add.Tensor %1732, %1841, %int1_1993 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_1994 = torch.constant.int 6
    %1843 = torch.prims.convert_element_type %1842, %int6_1994 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_1995 = torch.constant.int 2
    %1844 = torch.prim.ListConstruct %int2_1995 : (!torch.int) -> !torch.list<int>
    %int0_1996 = torch.constant.int 0
    %true_1997 = torch.constant.bool true
    %result0_1998, %result1_1999 = torch.aten.var_mean.correction %1843, %1844, %int0_1996, %true_1997 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2000 = torch.constant.float 1.000000e-05
    %int1_2001 = torch.constant.int 1
    %1845 = torch.aten.add.Scalar %result0_1998, %float1.000000e-05_2000, %int1_2001 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1846 = torch.aten.rsqrt %1845 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2002 = torch.constant.int 1
    %1847 = torch.aten.sub.Tensor %1842, %result1_1999, %int1_2002 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1848 = torch.aten.mul.Tensor %1847, %1846 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.weight : tensor<1280xf16>
    %1849 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1850 = torch.aten.mul.Tensor %1848, %1849 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.bias : tensor<1280xf16>
    %1851 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2003 = torch.constant.int 1
    %1852 = torch.aten.add.Tensor %1850, %1851, %int1_2003 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2004 = torch.constant.int 5
    %1853 = torch.prims.convert_element_type %1852, %int5_2004 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2005 = torch.constant.int 5
    %1854 = torch.prims.convert_element_type %result1_1999, %int5_2005 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2006 = torch.constant.int 5
    %1855 = torch.prims.convert_element_type %1846, %int5_2006 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2007 = torch.constant.int 64
    %int1280_2008 = torch.constant.int 1280
    %1856 = torch.prim.ListConstruct %int64_2007, %int1280_2008 : (!torch.int, !torch.int) -> !torch.list<int>
    %1857 = torch.aten.view %1853, %1856 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.weight : tensor<5120x1280xf16>
    %1858 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_2009 = torch.constant.int 0
    %int1_2010 = torch.constant.int 1
    %1859 = torch.aten.transpose.int %1858, %int0_2009, %int1_2010 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.bias : tensor<5120xf16>
    %1860 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_2011 = torch.constant.int 6
    %1861 = torch.prims.convert_element_type %1860, %int6_2011 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_2012 = torch.constant.int 6
    %1862 = torch.prims.convert_element_type %1857, %int6_2012 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2013 = torch.constant.int 6
    %1863 = torch.prims.convert_element_type %1859, %int6_2013 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %1864 = torch.aten.mm %1862, %1863 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_2014 = torch.constant.int 1
    %1865 = torch.aten.mul.Scalar %1864, %int1_2014 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_2015 = torch.constant.int 1
    %1866 = torch.aten.mul.Scalar %1861, %int1_2015 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_2016 = torch.constant.int 1
    %1867 = torch.aten.add.Tensor %1865, %1866, %int1_2016 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_2017 = torch.constant.int 5
    %1868 = torch.prims.convert_element_type %1867, %int5_2017 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_2018 = torch.constant.int 1
    %int64_2019 = torch.constant.int 64
    %int5120_2020 = torch.constant.int 5120
    %1869 = torch.prim.ListConstruct %int1_2018, %int64_2019, %int5120_2020 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1870 = torch.aten.view %1868, %1869 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_2021 = torch.constant.str "none"
    %1871 = torch.aten.gelu %1870, %str_2021 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_2022 = torch.constant.int 64
    %int5120_2023 = torch.constant.int 5120
    %1872 = torch.prim.ListConstruct %int64_2022, %int5120_2023 : (!torch.int, !torch.int) -> !torch.list<int>
    %1873 = torch.aten.view %1871, %1872 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.weight : tensor<1280x5120xf16>
    %1874 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2024 = torch.constant.int 0
    %int1_2025 = torch.constant.int 1
    %1875 = torch.aten.transpose.int %1874, %int0_2024, %int1_2025 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.bias : tensor<1280xf16>
    %1876 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.11.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2026 = torch.constant.int 6
    %1877 = torch.prims.convert_element_type %1876, %int6_2026 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2027 = torch.constant.int 6
    %1878 = torch.prims.convert_element_type %1873, %int6_2027 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_2028 = torch.constant.int 6
    %1879 = torch.prims.convert_element_type %1875, %int6_2028 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1880 = torch.aten.mm %1878, %1879 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2029 = torch.constant.int 1
    %1881 = torch.aten.mul.Scalar %1880, %int1_2029 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2030 = torch.constant.int 1
    %1882 = torch.aten.mul.Scalar %1877, %int1_2030 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2031 = torch.constant.int 1
    %1883 = torch.aten.add.Tensor %1881, %1882, %int1_2031 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2032 = torch.constant.int 5
    %1884 = torch.prims.convert_element_type %1883, %int5_2032 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2033 = torch.constant.int 1
    %int64_2034 = torch.constant.int 64
    %int1280_2035 = torch.constant.int 1280
    %1885 = torch.prim.ListConstruct %int1_2033, %int64_2034, %int1280_2035 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1886 = torch.aten.view %1884, %1885 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2036 = torch.constant.int 1
    %1887 = torch.aten.add.Tensor %1842, %1886, %int1_2036 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2037 = torch.constant.int 6
    %1888 = torch.prims.convert_element_type %1887, %int6_2037 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2038 = torch.constant.int 2
    %1889 = torch.prim.ListConstruct %int2_2038 : (!torch.int) -> !torch.list<int>
    %int0_2039 = torch.constant.int 0
    %true_2040 = torch.constant.bool true
    %result0_2041, %result1_2042 = torch.aten.var_mean.correction %1888, %1889, %int0_2039, %true_2040 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2043 = torch.constant.float 1.000000e-05
    %int1_2044 = torch.constant.int 1
    %1890 = torch.aten.add.Scalar %result0_2041, %float1.000000e-05_2043, %int1_2044 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %1891 = torch.aten.rsqrt %1890 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2045 = torch.constant.int 1
    %1892 = torch.aten.sub.Tensor %1887, %result1_2042, %int1_2045 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %1893 = torch.aten.mul.Tensor %1892, %1891 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.weight : tensor<1280xf16>
    %1894 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1895 = torch.aten.mul.Tensor %1893, %1894 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.bias : tensor<1280xf16>
    %1896 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2046 = torch.constant.int 1
    %1897 = torch.aten.add.Tensor %1895, %1896, %int1_2046 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2047 = torch.constant.int 5
    %1898 = torch.prims.convert_element_type %1897, %int5_2047 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2048 = torch.constant.int 5
    %1899 = torch.prims.convert_element_type %result1_2042, %int5_2048 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2049 = torch.constant.int 5
    %1900 = torch.prims.convert_element_type %1891, %int5_2049 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2050 = torch.constant.int 64
    %int1280_2051 = torch.constant.int 1280
    %1901 = torch.prim.ListConstruct %int64_2050, %int1280_2051 : (!torch.int, !torch.int) -> !torch.list<int>
    %1902 = torch.aten.view %1898, %1901 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %1903 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2052 = torch.constant.int 0
    %int1_2053 = torch.constant.int 1
    %1904 = torch.aten.transpose.int %1903, %int0_2052, %int1_2053 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.bias : tensor<1280xf16>
    %1905 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2054 = torch.constant.int 6
    %1906 = torch.prims.convert_element_type %1905, %int6_2054 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2055 = torch.constant.int 6
    %1907 = torch.prims.convert_element_type %1902, %int6_2055 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2056 = torch.constant.int 6
    %1908 = torch.prims.convert_element_type %1904, %int6_2056 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1909 = torch.aten.mm %1907, %1908 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2057 = torch.constant.int 1
    %1910 = torch.aten.mul.Scalar %1909, %int1_2057 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2058 = torch.constant.int 1
    %1911 = torch.aten.mul.Scalar %1906, %int1_2058 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2059 = torch.constant.int 1
    %1912 = torch.aten.add.Tensor %1910, %1911, %int1_2059 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2060 = torch.constant.int 5
    %1913 = torch.prims.convert_element_type %1912, %int5_2060 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2061 = torch.constant.int 1
    %int64_2062 = torch.constant.int 64
    %int1280_2063 = torch.constant.int 1280
    %1914 = torch.prim.ListConstruct %int1_2061, %int64_2062, %int1280_2063 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1915 = torch.aten.view %1913, %1914 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_2064 = torch.constant.float 1.250000e-01
    %1916 = torch.aten.mul.Scalar %1915, %float1.250000e-01_2064 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_2065 = torch.constant.int 64
    %int1280_2066 = torch.constant.int 1280
    %1917 = torch.prim.ListConstruct %int64_2065, %int1280_2066 : (!torch.int, !torch.int) -> !torch.list<int>
    %1918 = torch.aten.view %1898, %1917 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %1919 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2067 = torch.constant.int 0
    %int1_2068 = torch.constant.int 1
    %1920 = torch.aten.transpose.int %1919, %int0_2067, %int1_2068 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.bias : tensor<1280xf16>
    %1921 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2069 = torch.constant.int 6
    %1922 = torch.prims.convert_element_type %1921, %int6_2069 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2070 = torch.constant.int 6
    %1923 = torch.prims.convert_element_type %1918, %int6_2070 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2071 = torch.constant.int 6
    %1924 = torch.prims.convert_element_type %1920, %int6_2071 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1925 = torch.aten.mm %1923, %1924 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2072 = torch.constant.int 1
    %1926 = torch.aten.mul.Scalar %1925, %int1_2072 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2073 = torch.constant.int 1
    %1927 = torch.aten.mul.Scalar %1922, %int1_2073 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2074 = torch.constant.int 1
    %1928 = torch.aten.add.Tensor %1926, %1927, %int1_2074 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2075 = torch.constant.int 5
    %1929 = torch.prims.convert_element_type %1928, %int5_2075 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2076 = torch.constant.int 1
    %int64_2077 = torch.constant.int 64
    %int1280_2078 = torch.constant.int 1280
    %1930 = torch.prim.ListConstruct %int1_2076, %int64_2077, %int1280_2078 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1931 = torch.aten.view %1929, %1930 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2079 = torch.constant.int 1
    %int-1_2080 = torch.constant.int -1
    %int20_2081 = torch.constant.int 20
    %int64_2082 = torch.constant.int 64
    %1932 = torch.prim.ListConstruct %int1_2079, %int-1_2080, %int20_2081, %int64_2082 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1933 = torch.aten.view %1931, %1932 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2083 = torch.constant.int 1
    %int2_2084 = torch.constant.int 2
    %1934 = torch.aten.transpose.int %1933, %int1_2083, %int2_2084 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2085 = torch.constant.int 0
    %1935 = torch.aten.clone %1934, %int0_2085 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_2086 = torch.constant.int 64
    %int1280_2087 = torch.constant.int 1280
    %1936 = torch.prim.ListConstruct %int64_2086, %int1280_2087 : (!torch.int, !torch.int) -> !torch.list<int>
    %1937 = torch.aten.view %1898, %1936 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %1938 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2088 = torch.constant.int 0
    %int1_2089 = torch.constant.int 1
    %1939 = torch.aten.transpose.int %1938, %int0_2088, %int1_2089 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.bias : tensor<1280xf16>
    %1940 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2090 = torch.constant.int 6
    %1941 = torch.prims.convert_element_type %1940, %int6_2090 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2091 = torch.constant.int 6
    %1942 = torch.prims.convert_element_type %1937, %int6_2091 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2092 = torch.constant.int 6
    %1943 = torch.prims.convert_element_type %1939, %int6_2092 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1944 = torch.aten.mm %1942, %1943 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2093 = torch.constant.int 1
    %1945 = torch.aten.mul.Scalar %1944, %int1_2093 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2094 = torch.constant.int 1
    %1946 = torch.aten.mul.Scalar %1941, %int1_2094 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2095 = torch.constant.int 1
    %1947 = torch.aten.add.Tensor %1945, %1946, %int1_2095 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2096 = torch.constant.int 5
    %1948 = torch.prims.convert_element_type %1947, %int5_2096 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2097 = torch.constant.int 1
    %int64_2098 = torch.constant.int 64
    %int1280_2099 = torch.constant.int 1280
    %1949 = torch.prim.ListConstruct %int1_2097, %int64_2098, %int1280_2099 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1950 = torch.aten.view %1948, %1949 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2100 = torch.constant.int 1
    %int-1_2101 = torch.constant.int -1
    %int20_2102 = torch.constant.int 20
    %int64_2103 = torch.constant.int 64
    %1951 = torch.prim.ListConstruct %int1_2100, %int-1_2101, %int20_2102, %int64_2103 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1952 = torch.aten.view %1950, %1951 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2104 = torch.constant.int 1
    %int2_2105 = torch.constant.int 2
    %1953 = torch.aten.transpose.int %1952, %int1_2104, %int2_2105 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2106 = torch.constant.int 0
    %1954 = torch.aten.clone %1953, %int0_2106 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2107 = torch.constant.int 1
    %int64_2108 = torch.constant.int 64
    %int20_2109 = torch.constant.int 20
    %int64_2110 = torch.constant.int 64
    %1955 = torch.prim.ListConstruct %int1_2107, %int64_2108, %int20_2109, %int64_2110 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1956 = torch.aten.view %1916, %1955 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2111 = torch.constant.int 1
    %int2_2112 = torch.constant.int 2
    %1957 = torch.aten.transpose.int %1956, %int1_2111, %int2_2112 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2113 = torch.constant.int 0
    %1958 = torch.aten.clone %1957, %int0_2113 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2114 = torch.constant.int 20
    %int-1_2115 = torch.constant.int -1
    %int64_2116 = torch.constant.int 64
    %1959 = torch.prim.ListConstruct %int20_2114, %int-1_2115, %int64_2116 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1960 = torch.aten.view %1958, %1959 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2117 = torch.constant.int 20
    %int-1_2118 = torch.constant.int -1
    %int64_2119 = torch.constant.int 64
    %1961 = torch.prim.ListConstruct %int20_2117, %int-1_2118, %int64_2119 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1962 = torch.aten.view %1935, %1961 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2120 = torch.constant.int 20
    %int-1_2121 = torch.constant.int -1
    %int64_2122 = torch.constant.int 64
    %1963 = torch.prim.ListConstruct %int20_2120, %int-1_2121, %int64_2122 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1964 = torch.aten.view %1954, %1963 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_2123 = torch.constant.int 1
    %int2_2124 = torch.constant.int 2
    %1965 = torch.aten.transpose.int %1962, %int1_2123, %int2_2124 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %1966 = torch.aten.bmm %1960, %1965 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2125 = torch.constant.int 1
    %int20_2126 = torch.constant.int 20
    %int64_2127 = torch.constant.int 64
    %int64_2128 = torch.constant.int 64
    %1967 = torch.prim.ListConstruct %int1_2125, %int20_2126, %int64_2127, %int64_2128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1968 = torch.aten.view %1966, %1967 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2129 = torch.constant.int 1
    %1969 = torch.aten.add.Tensor %1968, %27, %int1_2129 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2130 = torch.constant.int 20
    %int64_2131 = torch.constant.int 64
    %int64_2132 = torch.constant.int 64
    %1970 = torch.prim.ListConstruct %int20_2130, %int64_2131, %int64_2132 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1971 = torch.aten.view %1969, %1970 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_2133 = torch.constant.int -1
    %false_2134 = torch.constant.bool false
    %1972 = torch.aten._softmax %1971, %int-1_2133, %false_2134 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %1973 = torch.aten.detach %1972 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_2135 = torch.constant.none
    %1974 = torch.aten.clone %1972, %none_2135 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %1975 = torch.aten.bmm %1974, %1964 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2136 = torch.constant.int 1
    %int20_2137 = torch.constant.int 20
    %int64_2138 = torch.constant.int 64
    %int64_2139 = torch.constant.int 64
    %1976 = torch.prim.ListConstruct %int1_2136, %int20_2137, %int64_2138, %int64_2139 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1977 = torch.aten.view %1975, %1976 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2140 = torch.constant.int 1
    %int2_2141 = torch.constant.int 2
    %1978 = torch.aten.transpose.int %1977, %int1_2140, %int2_2141 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_2142 = torch.constant.int 0
    %1979 = torch.aten.clone %1978, %int0_2142 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2143 = torch.constant.int 1
    %int64_2144 = torch.constant.int 64
    %int1280_2145 = torch.constant.int 1280
    %1980 = torch.prim.ListConstruct %int1_2143, %int64_2144, %int1280_2145 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1981 = torch.aten._unsafe_view %1979, %1980 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_2146 = torch.constant.int 64
    %int1280_2147 = torch.constant.int 1280
    %1982 = torch.prim.ListConstruct %int64_2146, %int1280_2147 : (!torch.int, !torch.int) -> !torch.list<int>
    %1983 = torch.aten.view %1981, %1982 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %1984 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2148 = torch.constant.int 0
    %int1_2149 = torch.constant.int 1
    %1985 = torch.aten.transpose.int %1984, %int0_2148, %int1_2149 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.bias : tensor<1280xf16>
    %1986 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2150 = torch.constant.int 6
    %1987 = torch.prims.convert_element_type %1986, %int6_2150 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2151 = torch.constant.int 6
    %1988 = torch.prims.convert_element_type %1983, %int6_2151 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2152 = torch.constant.int 6
    %1989 = torch.prims.convert_element_type %1985, %int6_2152 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1990 = torch.aten.mm %1988, %1989 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2153 = torch.constant.int 1
    %1991 = torch.aten.mul.Scalar %1990, %int1_2153 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2154 = torch.constant.int 1
    %1992 = torch.aten.mul.Scalar %1987, %int1_2154 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2155 = torch.constant.int 1
    %1993 = torch.aten.add.Tensor %1991, %1992, %int1_2155 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2156 = torch.constant.int 5
    %1994 = torch.prims.convert_element_type %1993, %int5_2156 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2157 = torch.constant.int 1
    %int64_2158 = torch.constant.int 64
    %int1280_2159 = torch.constant.int 1280
    %1995 = torch.prim.ListConstruct %int1_2157, %int64_2158, %int1280_2159 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1996 = torch.aten.view %1994, %1995 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2160 = torch.constant.int 1
    %1997 = torch.aten.add.Tensor %1887, %1996, %int1_2160 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2161 = torch.constant.int 6
    %1998 = torch.prims.convert_element_type %1997, %int6_2161 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2162 = torch.constant.int 2
    %1999 = torch.prim.ListConstruct %int2_2162 : (!torch.int) -> !torch.list<int>
    %int0_2163 = torch.constant.int 0
    %true_2164 = torch.constant.bool true
    %result0_2165, %result1_2166 = torch.aten.var_mean.correction %1998, %1999, %int0_2163, %true_2164 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2167 = torch.constant.float 1.000000e-05
    %int1_2168 = torch.constant.int 1
    %2000 = torch.aten.add.Scalar %result0_2165, %float1.000000e-05_2167, %int1_2168 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2001 = torch.aten.rsqrt %2000 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2169 = torch.constant.int 1
    %2002 = torch.aten.sub.Tensor %1997, %result1_2166, %int1_2169 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2003 = torch.aten.mul.Tensor %2002, %2001 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.weight : tensor<1280xf16>
    %2004 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2005 = torch.aten.mul.Tensor %2003, %2004 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.bias : tensor<1280xf16>
    %2006 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2170 = torch.constant.int 1
    %2007 = torch.aten.add.Tensor %2005, %2006, %int1_2170 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2171 = torch.constant.int 5
    %2008 = torch.prims.convert_element_type %2007, %int5_2171 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2172 = torch.constant.int 5
    %2009 = torch.prims.convert_element_type %result1_2166, %int5_2172 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2173 = torch.constant.int 5
    %2010 = torch.prims.convert_element_type %2001, %int5_2173 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2174 = torch.constant.int 64
    %int1280_2175 = torch.constant.int 1280
    %2011 = torch.prim.ListConstruct %int64_2174, %int1280_2175 : (!torch.int, !torch.int) -> !torch.list<int>
    %2012 = torch.aten.view %2008, %2011 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.weight : tensor<5120x1280xf16>
    %2013 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_2176 = torch.constant.int 0
    %int1_2177 = torch.constant.int 1
    %2014 = torch.aten.transpose.int %2013, %int0_2176, %int1_2177 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.bias : tensor<5120xf16>
    %2015 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_2178 = torch.constant.int 6
    %2016 = torch.prims.convert_element_type %2015, %int6_2178 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_2179 = torch.constant.int 6
    %2017 = torch.prims.convert_element_type %2012, %int6_2179 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2180 = torch.constant.int 6
    %2018 = torch.prims.convert_element_type %2014, %int6_2180 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2019 = torch.aten.mm %2017, %2018 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_2181 = torch.constant.int 1
    %2020 = torch.aten.mul.Scalar %2019, %int1_2181 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_2182 = torch.constant.int 1
    %2021 = torch.aten.mul.Scalar %2016, %int1_2182 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_2183 = torch.constant.int 1
    %2022 = torch.aten.add.Tensor %2020, %2021, %int1_2183 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_2184 = torch.constant.int 5
    %2023 = torch.prims.convert_element_type %2022, %int5_2184 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_2185 = torch.constant.int 1
    %int64_2186 = torch.constant.int 64
    %int5120_2187 = torch.constant.int 5120
    %2024 = torch.prim.ListConstruct %int1_2185, %int64_2186, %int5120_2187 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2025 = torch.aten.view %2023, %2024 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_2188 = torch.constant.str "none"
    %2026 = torch.aten.gelu %2025, %str_2188 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_2189 = torch.constant.int 64
    %int5120_2190 = torch.constant.int 5120
    %2027 = torch.prim.ListConstruct %int64_2189, %int5120_2190 : (!torch.int, !torch.int) -> !torch.list<int>
    %2028 = torch.aten.view %2026, %2027 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.weight : tensor<1280x5120xf16>
    %2029 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2191 = torch.constant.int 0
    %int1_2192 = torch.constant.int 1
    %2030 = torch.aten.transpose.int %2029, %int0_2191, %int1_2192 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.bias : tensor<1280xf16>
    %2031 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.12.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2193 = torch.constant.int 6
    %2032 = torch.prims.convert_element_type %2031, %int6_2193 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2194 = torch.constant.int 6
    %2033 = torch.prims.convert_element_type %2028, %int6_2194 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_2195 = torch.constant.int 6
    %2034 = torch.prims.convert_element_type %2030, %int6_2195 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2035 = torch.aten.mm %2033, %2034 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2196 = torch.constant.int 1
    %2036 = torch.aten.mul.Scalar %2035, %int1_2196 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2197 = torch.constant.int 1
    %2037 = torch.aten.mul.Scalar %2032, %int1_2197 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2198 = torch.constant.int 1
    %2038 = torch.aten.add.Tensor %2036, %2037, %int1_2198 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2199 = torch.constant.int 5
    %2039 = torch.prims.convert_element_type %2038, %int5_2199 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2200 = torch.constant.int 1
    %int64_2201 = torch.constant.int 64
    %int1280_2202 = torch.constant.int 1280
    %2040 = torch.prim.ListConstruct %int1_2200, %int64_2201, %int1280_2202 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2041 = torch.aten.view %2039, %2040 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2203 = torch.constant.int 1
    %2042 = torch.aten.add.Tensor %1997, %2041, %int1_2203 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2204 = torch.constant.int 6
    %2043 = torch.prims.convert_element_type %2042, %int6_2204 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2205 = torch.constant.int 2
    %2044 = torch.prim.ListConstruct %int2_2205 : (!torch.int) -> !torch.list<int>
    %int0_2206 = torch.constant.int 0
    %true_2207 = torch.constant.bool true
    %result0_2208, %result1_2209 = torch.aten.var_mean.correction %2043, %2044, %int0_2206, %true_2207 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2210 = torch.constant.float 1.000000e-05
    %int1_2211 = torch.constant.int 1
    %2045 = torch.aten.add.Scalar %result0_2208, %float1.000000e-05_2210, %int1_2211 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2046 = torch.aten.rsqrt %2045 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2212 = torch.constant.int 1
    %2047 = torch.aten.sub.Tensor %2042, %result1_2209, %int1_2212 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2048 = torch.aten.mul.Tensor %2047, %2046 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.weight : tensor<1280xf16>
    %2049 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2050 = torch.aten.mul.Tensor %2048, %2049 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.bias : tensor<1280xf16>
    %2051 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2213 = torch.constant.int 1
    %2052 = torch.aten.add.Tensor %2050, %2051, %int1_2213 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2214 = torch.constant.int 5
    %2053 = torch.prims.convert_element_type %2052, %int5_2214 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2215 = torch.constant.int 5
    %2054 = torch.prims.convert_element_type %result1_2209, %int5_2215 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2216 = torch.constant.int 5
    %2055 = torch.prims.convert_element_type %2046, %int5_2216 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2217 = torch.constant.int 64
    %int1280_2218 = torch.constant.int 1280
    %2056 = torch.prim.ListConstruct %int64_2217, %int1280_2218 : (!torch.int, !torch.int) -> !torch.list<int>
    %2057 = torch.aten.view %2053, %2056 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2058 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2219 = torch.constant.int 0
    %int1_2220 = torch.constant.int 1
    %2059 = torch.aten.transpose.int %2058, %int0_2219, %int1_2220 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.bias : tensor<1280xf16>
    %2060 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2221 = torch.constant.int 6
    %2061 = torch.prims.convert_element_type %2060, %int6_2221 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2222 = torch.constant.int 6
    %2062 = torch.prims.convert_element_type %2057, %int6_2222 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2223 = torch.constant.int 6
    %2063 = torch.prims.convert_element_type %2059, %int6_2223 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2064 = torch.aten.mm %2062, %2063 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2224 = torch.constant.int 1
    %2065 = torch.aten.mul.Scalar %2064, %int1_2224 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2225 = torch.constant.int 1
    %2066 = torch.aten.mul.Scalar %2061, %int1_2225 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2226 = torch.constant.int 1
    %2067 = torch.aten.add.Tensor %2065, %2066, %int1_2226 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2227 = torch.constant.int 5
    %2068 = torch.prims.convert_element_type %2067, %int5_2227 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2228 = torch.constant.int 1
    %int64_2229 = torch.constant.int 64
    %int1280_2230 = torch.constant.int 1280
    %2069 = torch.prim.ListConstruct %int1_2228, %int64_2229, %int1280_2230 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2070 = torch.aten.view %2068, %2069 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_2231 = torch.constant.float 1.250000e-01
    %2071 = torch.aten.mul.Scalar %2070, %float1.250000e-01_2231 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_2232 = torch.constant.int 64
    %int1280_2233 = torch.constant.int 1280
    %2072 = torch.prim.ListConstruct %int64_2232, %int1280_2233 : (!torch.int, !torch.int) -> !torch.list<int>
    %2073 = torch.aten.view %2053, %2072 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %2074 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2234 = torch.constant.int 0
    %int1_2235 = torch.constant.int 1
    %2075 = torch.aten.transpose.int %2074, %int0_2234, %int1_2235 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.bias : tensor<1280xf16>
    %2076 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2236 = torch.constant.int 6
    %2077 = torch.prims.convert_element_type %2076, %int6_2236 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2237 = torch.constant.int 6
    %2078 = torch.prims.convert_element_type %2073, %int6_2237 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2238 = torch.constant.int 6
    %2079 = torch.prims.convert_element_type %2075, %int6_2238 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2080 = torch.aten.mm %2078, %2079 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2239 = torch.constant.int 1
    %2081 = torch.aten.mul.Scalar %2080, %int1_2239 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2240 = torch.constant.int 1
    %2082 = torch.aten.mul.Scalar %2077, %int1_2240 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2241 = torch.constant.int 1
    %2083 = torch.aten.add.Tensor %2081, %2082, %int1_2241 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2242 = torch.constant.int 5
    %2084 = torch.prims.convert_element_type %2083, %int5_2242 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2243 = torch.constant.int 1
    %int64_2244 = torch.constant.int 64
    %int1280_2245 = torch.constant.int 1280
    %2085 = torch.prim.ListConstruct %int1_2243, %int64_2244, %int1280_2245 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2086 = torch.aten.view %2084, %2085 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2246 = torch.constant.int 1
    %int-1_2247 = torch.constant.int -1
    %int20_2248 = torch.constant.int 20
    %int64_2249 = torch.constant.int 64
    %2087 = torch.prim.ListConstruct %int1_2246, %int-1_2247, %int20_2248, %int64_2249 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2088 = torch.aten.view %2086, %2087 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2250 = torch.constant.int 1
    %int2_2251 = torch.constant.int 2
    %2089 = torch.aten.transpose.int %2088, %int1_2250, %int2_2251 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2252 = torch.constant.int 0
    %2090 = torch.aten.clone %2089, %int0_2252 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_2253 = torch.constant.int 64
    %int1280_2254 = torch.constant.int 1280
    %2091 = torch.prim.ListConstruct %int64_2253, %int1280_2254 : (!torch.int, !torch.int) -> !torch.list<int>
    %2092 = torch.aten.view %2053, %2091 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %2093 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2255 = torch.constant.int 0
    %int1_2256 = torch.constant.int 1
    %2094 = torch.aten.transpose.int %2093, %int0_2255, %int1_2256 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.bias : tensor<1280xf16>
    %2095 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2257 = torch.constant.int 6
    %2096 = torch.prims.convert_element_type %2095, %int6_2257 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2258 = torch.constant.int 6
    %2097 = torch.prims.convert_element_type %2092, %int6_2258 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2259 = torch.constant.int 6
    %2098 = torch.prims.convert_element_type %2094, %int6_2259 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2099 = torch.aten.mm %2097, %2098 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2260 = torch.constant.int 1
    %2100 = torch.aten.mul.Scalar %2099, %int1_2260 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2261 = torch.constant.int 1
    %2101 = torch.aten.mul.Scalar %2096, %int1_2261 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2262 = torch.constant.int 1
    %2102 = torch.aten.add.Tensor %2100, %2101, %int1_2262 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2263 = torch.constant.int 5
    %2103 = torch.prims.convert_element_type %2102, %int5_2263 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2264 = torch.constant.int 1
    %int64_2265 = torch.constant.int 64
    %int1280_2266 = torch.constant.int 1280
    %2104 = torch.prim.ListConstruct %int1_2264, %int64_2265, %int1280_2266 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2105 = torch.aten.view %2103, %2104 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2267 = torch.constant.int 1
    %int-1_2268 = torch.constant.int -1
    %int20_2269 = torch.constant.int 20
    %int64_2270 = torch.constant.int 64
    %2106 = torch.prim.ListConstruct %int1_2267, %int-1_2268, %int20_2269, %int64_2270 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2107 = torch.aten.view %2105, %2106 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2271 = torch.constant.int 1
    %int2_2272 = torch.constant.int 2
    %2108 = torch.aten.transpose.int %2107, %int1_2271, %int2_2272 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2273 = torch.constant.int 0
    %2109 = torch.aten.clone %2108, %int0_2273 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2274 = torch.constant.int 1
    %int64_2275 = torch.constant.int 64
    %int20_2276 = torch.constant.int 20
    %int64_2277 = torch.constant.int 64
    %2110 = torch.prim.ListConstruct %int1_2274, %int64_2275, %int20_2276, %int64_2277 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2111 = torch.aten.view %2071, %2110 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2278 = torch.constant.int 1
    %int2_2279 = torch.constant.int 2
    %2112 = torch.aten.transpose.int %2111, %int1_2278, %int2_2279 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2280 = torch.constant.int 0
    %2113 = torch.aten.clone %2112, %int0_2280 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2281 = torch.constant.int 20
    %int-1_2282 = torch.constant.int -1
    %int64_2283 = torch.constant.int 64
    %2114 = torch.prim.ListConstruct %int20_2281, %int-1_2282, %int64_2283 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2115 = torch.aten.view %2113, %2114 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2284 = torch.constant.int 20
    %int-1_2285 = torch.constant.int -1
    %int64_2286 = torch.constant.int 64
    %2116 = torch.prim.ListConstruct %int20_2284, %int-1_2285, %int64_2286 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2117 = torch.aten.view %2090, %2116 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2287 = torch.constant.int 20
    %int-1_2288 = torch.constant.int -1
    %int64_2289 = torch.constant.int 64
    %2118 = torch.prim.ListConstruct %int20_2287, %int-1_2288, %int64_2289 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2119 = torch.aten.view %2109, %2118 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_2290 = torch.constant.int 1
    %int2_2291 = torch.constant.int 2
    %2120 = torch.aten.transpose.int %2117, %int1_2290, %int2_2291 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %2121 = torch.aten.bmm %2115, %2120 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2292 = torch.constant.int 1
    %int20_2293 = torch.constant.int 20
    %int64_2294 = torch.constant.int 64
    %int64_2295 = torch.constant.int 64
    %2122 = torch.prim.ListConstruct %int1_2292, %int20_2293, %int64_2294, %int64_2295 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2123 = torch.aten.view %2121, %2122 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2296 = torch.constant.int 1
    %2124 = torch.aten.add.Tensor %2123, %27, %int1_2296 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2297 = torch.constant.int 20
    %int64_2298 = torch.constant.int 64
    %int64_2299 = torch.constant.int 64
    %2125 = torch.prim.ListConstruct %int20_2297, %int64_2298, %int64_2299 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2126 = torch.aten.view %2124, %2125 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_2300 = torch.constant.int -1
    %false_2301 = torch.constant.bool false
    %2127 = torch.aten._softmax %2126, %int-1_2300, %false_2301 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %2128 = torch.aten.detach %2127 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_2302 = torch.constant.none
    %2129 = torch.aten.clone %2127, %none_2302 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %2130 = torch.aten.bmm %2129, %2119 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2303 = torch.constant.int 1
    %int20_2304 = torch.constant.int 20
    %int64_2305 = torch.constant.int 64
    %int64_2306 = torch.constant.int 64
    %2131 = torch.prim.ListConstruct %int1_2303, %int20_2304, %int64_2305, %int64_2306 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2132 = torch.aten.view %2130, %2131 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2307 = torch.constant.int 1
    %int2_2308 = torch.constant.int 2
    %2133 = torch.aten.transpose.int %2132, %int1_2307, %int2_2308 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_2309 = torch.constant.int 0
    %2134 = torch.aten.clone %2133, %int0_2309 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2310 = torch.constant.int 1
    %int64_2311 = torch.constant.int 64
    %int1280_2312 = torch.constant.int 1280
    %2135 = torch.prim.ListConstruct %int1_2310, %int64_2311, %int1280_2312 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2136 = torch.aten._unsafe_view %2134, %2135 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_2313 = torch.constant.int 64
    %int1280_2314 = torch.constant.int 1280
    %2137 = torch.prim.ListConstruct %int64_2313, %int1280_2314 : (!torch.int, !torch.int) -> !torch.list<int>
    %2138 = torch.aten.view %2136, %2137 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %2139 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2315 = torch.constant.int 0
    %int1_2316 = torch.constant.int 1
    %2140 = torch.aten.transpose.int %2139, %int0_2315, %int1_2316 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.bias : tensor<1280xf16>
    %2141 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2317 = torch.constant.int 6
    %2142 = torch.prims.convert_element_type %2141, %int6_2317 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2318 = torch.constant.int 6
    %2143 = torch.prims.convert_element_type %2138, %int6_2318 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2319 = torch.constant.int 6
    %2144 = torch.prims.convert_element_type %2140, %int6_2319 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2145 = torch.aten.mm %2143, %2144 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2320 = torch.constant.int 1
    %2146 = torch.aten.mul.Scalar %2145, %int1_2320 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2321 = torch.constant.int 1
    %2147 = torch.aten.mul.Scalar %2142, %int1_2321 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2322 = torch.constant.int 1
    %2148 = torch.aten.add.Tensor %2146, %2147, %int1_2322 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2323 = torch.constant.int 5
    %2149 = torch.prims.convert_element_type %2148, %int5_2323 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2324 = torch.constant.int 1
    %int64_2325 = torch.constant.int 64
    %int1280_2326 = torch.constant.int 1280
    %2150 = torch.prim.ListConstruct %int1_2324, %int64_2325, %int1280_2326 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2151 = torch.aten.view %2149, %2150 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2327 = torch.constant.int 1
    %2152 = torch.aten.add.Tensor %2042, %2151, %int1_2327 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2328 = torch.constant.int 6
    %2153 = torch.prims.convert_element_type %2152, %int6_2328 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2329 = torch.constant.int 2
    %2154 = torch.prim.ListConstruct %int2_2329 : (!torch.int) -> !torch.list<int>
    %int0_2330 = torch.constant.int 0
    %true_2331 = torch.constant.bool true
    %result0_2332, %result1_2333 = torch.aten.var_mean.correction %2153, %2154, %int0_2330, %true_2331 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2334 = torch.constant.float 1.000000e-05
    %int1_2335 = torch.constant.int 1
    %2155 = torch.aten.add.Scalar %result0_2332, %float1.000000e-05_2334, %int1_2335 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2156 = torch.aten.rsqrt %2155 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2336 = torch.constant.int 1
    %2157 = torch.aten.sub.Tensor %2152, %result1_2333, %int1_2336 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2158 = torch.aten.mul.Tensor %2157, %2156 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.weight : tensor<1280xf16>
    %2159 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2160 = torch.aten.mul.Tensor %2158, %2159 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.bias : tensor<1280xf16>
    %2161 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2337 = torch.constant.int 1
    %2162 = torch.aten.add.Tensor %2160, %2161, %int1_2337 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2338 = torch.constant.int 5
    %2163 = torch.prims.convert_element_type %2162, %int5_2338 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2339 = torch.constant.int 5
    %2164 = torch.prims.convert_element_type %result1_2333, %int5_2339 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2340 = torch.constant.int 5
    %2165 = torch.prims.convert_element_type %2156, %int5_2340 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2341 = torch.constant.int 64
    %int1280_2342 = torch.constant.int 1280
    %2166 = torch.prim.ListConstruct %int64_2341, %int1280_2342 : (!torch.int, !torch.int) -> !torch.list<int>
    %2167 = torch.aten.view %2163, %2166 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.weight : tensor<5120x1280xf16>
    %2168 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_2343 = torch.constant.int 0
    %int1_2344 = torch.constant.int 1
    %2169 = torch.aten.transpose.int %2168, %int0_2343, %int1_2344 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.bias : tensor<5120xf16>
    %2170 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_2345 = torch.constant.int 6
    %2171 = torch.prims.convert_element_type %2170, %int6_2345 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_2346 = torch.constant.int 6
    %2172 = torch.prims.convert_element_type %2167, %int6_2346 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2347 = torch.constant.int 6
    %2173 = torch.prims.convert_element_type %2169, %int6_2347 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2174 = torch.aten.mm %2172, %2173 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_2348 = torch.constant.int 1
    %2175 = torch.aten.mul.Scalar %2174, %int1_2348 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_2349 = torch.constant.int 1
    %2176 = torch.aten.mul.Scalar %2171, %int1_2349 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_2350 = torch.constant.int 1
    %2177 = torch.aten.add.Tensor %2175, %2176, %int1_2350 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_2351 = torch.constant.int 5
    %2178 = torch.prims.convert_element_type %2177, %int5_2351 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_2352 = torch.constant.int 1
    %int64_2353 = torch.constant.int 64
    %int5120_2354 = torch.constant.int 5120
    %2179 = torch.prim.ListConstruct %int1_2352, %int64_2353, %int5120_2354 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2180 = torch.aten.view %2178, %2179 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_2355 = torch.constant.str "none"
    %2181 = torch.aten.gelu %2180, %str_2355 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_2356 = torch.constant.int 64
    %int5120_2357 = torch.constant.int 5120
    %2182 = torch.prim.ListConstruct %int64_2356, %int5120_2357 : (!torch.int, !torch.int) -> !torch.list<int>
    %2183 = torch.aten.view %2181, %2182 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.weight : tensor<1280x5120xf16>
    %2184 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2358 = torch.constant.int 0
    %int1_2359 = torch.constant.int 1
    %2185 = torch.aten.transpose.int %2184, %int0_2358, %int1_2359 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.bias : tensor<1280xf16>
    %2186 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.13.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2360 = torch.constant.int 6
    %2187 = torch.prims.convert_element_type %2186, %int6_2360 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2361 = torch.constant.int 6
    %2188 = torch.prims.convert_element_type %2183, %int6_2361 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_2362 = torch.constant.int 6
    %2189 = torch.prims.convert_element_type %2185, %int6_2362 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2190 = torch.aten.mm %2188, %2189 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2363 = torch.constant.int 1
    %2191 = torch.aten.mul.Scalar %2190, %int1_2363 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2364 = torch.constant.int 1
    %2192 = torch.aten.mul.Scalar %2187, %int1_2364 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2365 = torch.constant.int 1
    %2193 = torch.aten.add.Tensor %2191, %2192, %int1_2365 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2366 = torch.constant.int 5
    %2194 = torch.prims.convert_element_type %2193, %int5_2366 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2367 = torch.constant.int 1
    %int64_2368 = torch.constant.int 64
    %int1280_2369 = torch.constant.int 1280
    %2195 = torch.prim.ListConstruct %int1_2367, %int64_2368, %int1280_2369 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2196 = torch.aten.view %2194, %2195 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2370 = torch.constant.int 1
    %2197 = torch.aten.add.Tensor %2152, %2196, %int1_2370 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2371 = torch.constant.int 6
    %2198 = torch.prims.convert_element_type %2197, %int6_2371 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2372 = torch.constant.int 2
    %2199 = torch.prim.ListConstruct %int2_2372 : (!torch.int) -> !torch.list<int>
    %int0_2373 = torch.constant.int 0
    %true_2374 = torch.constant.bool true
    %result0_2375, %result1_2376 = torch.aten.var_mean.correction %2198, %2199, %int0_2373, %true_2374 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2377 = torch.constant.float 1.000000e-05
    %int1_2378 = torch.constant.int 1
    %2200 = torch.aten.add.Scalar %result0_2375, %float1.000000e-05_2377, %int1_2378 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2201 = torch.aten.rsqrt %2200 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2379 = torch.constant.int 1
    %2202 = torch.aten.sub.Tensor %2197, %result1_2376, %int1_2379 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2203 = torch.aten.mul.Tensor %2202, %2201 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.weight : tensor<1280xf16>
    %2204 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2205 = torch.aten.mul.Tensor %2203, %2204 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.bias : tensor<1280xf16>
    %2206 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2380 = torch.constant.int 1
    %2207 = torch.aten.add.Tensor %2205, %2206, %int1_2380 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2381 = torch.constant.int 5
    %2208 = torch.prims.convert_element_type %2207, %int5_2381 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2382 = torch.constant.int 5
    %2209 = torch.prims.convert_element_type %result1_2376, %int5_2382 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2383 = torch.constant.int 5
    %2210 = torch.prims.convert_element_type %2201, %int5_2383 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2384 = torch.constant.int 64
    %int1280_2385 = torch.constant.int 1280
    %2211 = torch.prim.ListConstruct %int64_2384, %int1280_2385 : (!torch.int, !torch.int) -> !torch.list<int>
    %2212 = torch.aten.view %2208, %2211 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2213 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2386 = torch.constant.int 0
    %int1_2387 = torch.constant.int 1
    %2214 = torch.aten.transpose.int %2213, %int0_2386, %int1_2387 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.bias : tensor<1280xf16>
    %2215 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2388 = torch.constant.int 6
    %2216 = torch.prims.convert_element_type %2215, %int6_2388 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2389 = torch.constant.int 6
    %2217 = torch.prims.convert_element_type %2212, %int6_2389 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2390 = torch.constant.int 6
    %2218 = torch.prims.convert_element_type %2214, %int6_2390 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2219 = torch.aten.mm %2217, %2218 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2391 = torch.constant.int 1
    %2220 = torch.aten.mul.Scalar %2219, %int1_2391 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2392 = torch.constant.int 1
    %2221 = torch.aten.mul.Scalar %2216, %int1_2392 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2393 = torch.constant.int 1
    %2222 = torch.aten.add.Tensor %2220, %2221, %int1_2393 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2394 = torch.constant.int 5
    %2223 = torch.prims.convert_element_type %2222, %int5_2394 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2395 = torch.constant.int 1
    %int64_2396 = torch.constant.int 64
    %int1280_2397 = torch.constant.int 1280
    %2224 = torch.prim.ListConstruct %int1_2395, %int64_2396, %int1280_2397 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2225 = torch.aten.view %2223, %2224 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_2398 = torch.constant.float 1.250000e-01
    %2226 = torch.aten.mul.Scalar %2225, %float1.250000e-01_2398 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_2399 = torch.constant.int 64
    %int1280_2400 = torch.constant.int 1280
    %2227 = torch.prim.ListConstruct %int64_2399, %int1280_2400 : (!torch.int, !torch.int) -> !torch.list<int>
    %2228 = torch.aten.view %2208, %2227 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %2229 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2401 = torch.constant.int 0
    %int1_2402 = torch.constant.int 1
    %2230 = torch.aten.transpose.int %2229, %int0_2401, %int1_2402 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.bias : tensor<1280xf16>
    %2231 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2403 = torch.constant.int 6
    %2232 = torch.prims.convert_element_type %2231, %int6_2403 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2404 = torch.constant.int 6
    %2233 = torch.prims.convert_element_type %2228, %int6_2404 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2405 = torch.constant.int 6
    %2234 = torch.prims.convert_element_type %2230, %int6_2405 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2235 = torch.aten.mm %2233, %2234 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2406 = torch.constant.int 1
    %2236 = torch.aten.mul.Scalar %2235, %int1_2406 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2407 = torch.constant.int 1
    %2237 = torch.aten.mul.Scalar %2232, %int1_2407 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2408 = torch.constant.int 1
    %2238 = torch.aten.add.Tensor %2236, %2237, %int1_2408 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2409 = torch.constant.int 5
    %2239 = torch.prims.convert_element_type %2238, %int5_2409 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2410 = torch.constant.int 1
    %int64_2411 = torch.constant.int 64
    %int1280_2412 = torch.constant.int 1280
    %2240 = torch.prim.ListConstruct %int1_2410, %int64_2411, %int1280_2412 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2241 = torch.aten.view %2239, %2240 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2413 = torch.constant.int 1
    %int-1_2414 = torch.constant.int -1
    %int20_2415 = torch.constant.int 20
    %int64_2416 = torch.constant.int 64
    %2242 = torch.prim.ListConstruct %int1_2413, %int-1_2414, %int20_2415, %int64_2416 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2243 = torch.aten.view %2241, %2242 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2417 = torch.constant.int 1
    %int2_2418 = torch.constant.int 2
    %2244 = torch.aten.transpose.int %2243, %int1_2417, %int2_2418 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2419 = torch.constant.int 0
    %2245 = torch.aten.clone %2244, %int0_2419 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_2420 = torch.constant.int 64
    %int1280_2421 = torch.constant.int 1280
    %2246 = torch.prim.ListConstruct %int64_2420, %int1280_2421 : (!torch.int, !torch.int) -> !torch.list<int>
    %2247 = torch.aten.view %2208, %2246 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %2248 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2422 = torch.constant.int 0
    %int1_2423 = torch.constant.int 1
    %2249 = torch.aten.transpose.int %2248, %int0_2422, %int1_2423 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.bias : tensor<1280xf16>
    %2250 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2424 = torch.constant.int 6
    %2251 = torch.prims.convert_element_type %2250, %int6_2424 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2425 = torch.constant.int 6
    %2252 = torch.prims.convert_element_type %2247, %int6_2425 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2426 = torch.constant.int 6
    %2253 = torch.prims.convert_element_type %2249, %int6_2426 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2254 = torch.aten.mm %2252, %2253 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2427 = torch.constant.int 1
    %2255 = torch.aten.mul.Scalar %2254, %int1_2427 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2428 = torch.constant.int 1
    %2256 = torch.aten.mul.Scalar %2251, %int1_2428 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2429 = torch.constant.int 1
    %2257 = torch.aten.add.Tensor %2255, %2256, %int1_2429 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2430 = torch.constant.int 5
    %2258 = torch.prims.convert_element_type %2257, %int5_2430 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2431 = torch.constant.int 1
    %int64_2432 = torch.constant.int 64
    %int1280_2433 = torch.constant.int 1280
    %2259 = torch.prim.ListConstruct %int1_2431, %int64_2432, %int1280_2433 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2260 = torch.aten.view %2258, %2259 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2434 = torch.constant.int 1
    %int-1_2435 = torch.constant.int -1
    %int20_2436 = torch.constant.int 20
    %int64_2437 = torch.constant.int 64
    %2261 = torch.prim.ListConstruct %int1_2434, %int-1_2435, %int20_2436, %int64_2437 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2262 = torch.aten.view %2260, %2261 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2438 = torch.constant.int 1
    %int2_2439 = torch.constant.int 2
    %2263 = torch.aten.transpose.int %2262, %int1_2438, %int2_2439 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2440 = torch.constant.int 0
    %2264 = torch.aten.clone %2263, %int0_2440 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2441 = torch.constant.int 1
    %int64_2442 = torch.constant.int 64
    %int20_2443 = torch.constant.int 20
    %int64_2444 = torch.constant.int 64
    %2265 = torch.prim.ListConstruct %int1_2441, %int64_2442, %int20_2443, %int64_2444 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2266 = torch.aten.view %2226, %2265 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2445 = torch.constant.int 1
    %int2_2446 = torch.constant.int 2
    %2267 = torch.aten.transpose.int %2266, %int1_2445, %int2_2446 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2447 = torch.constant.int 0
    %2268 = torch.aten.clone %2267, %int0_2447 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2448 = torch.constant.int 20
    %int-1_2449 = torch.constant.int -1
    %int64_2450 = torch.constant.int 64
    %2269 = torch.prim.ListConstruct %int20_2448, %int-1_2449, %int64_2450 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2270 = torch.aten.view %2268, %2269 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2451 = torch.constant.int 20
    %int-1_2452 = torch.constant.int -1
    %int64_2453 = torch.constant.int 64
    %2271 = torch.prim.ListConstruct %int20_2451, %int-1_2452, %int64_2453 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2272 = torch.aten.view %2245, %2271 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2454 = torch.constant.int 20
    %int-1_2455 = torch.constant.int -1
    %int64_2456 = torch.constant.int 64
    %2273 = torch.prim.ListConstruct %int20_2454, %int-1_2455, %int64_2456 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2274 = torch.aten.view %2264, %2273 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_2457 = torch.constant.int 1
    %int2_2458 = torch.constant.int 2
    %2275 = torch.aten.transpose.int %2272, %int1_2457, %int2_2458 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %2276 = torch.aten.bmm %2270, %2275 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2459 = torch.constant.int 1
    %int20_2460 = torch.constant.int 20
    %int64_2461 = torch.constant.int 64
    %int64_2462 = torch.constant.int 64
    %2277 = torch.prim.ListConstruct %int1_2459, %int20_2460, %int64_2461, %int64_2462 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2278 = torch.aten.view %2276, %2277 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2463 = torch.constant.int 1
    %2279 = torch.aten.add.Tensor %2278, %27, %int1_2463 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2464 = torch.constant.int 20
    %int64_2465 = torch.constant.int 64
    %int64_2466 = torch.constant.int 64
    %2280 = torch.prim.ListConstruct %int20_2464, %int64_2465, %int64_2466 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2281 = torch.aten.view %2279, %2280 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_2467 = torch.constant.int -1
    %false_2468 = torch.constant.bool false
    %2282 = torch.aten._softmax %2281, %int-1_2467, %false_2468 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %2283 = torch.aten.detach %2282 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_2469 = torch.constant.none
    %2284 = torch.aten.clone %2282, %none_2469 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %2285 = torch.aten.bmm %2284, %2274 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2470 = torch.constant.int 1
    %int20_2471 = torch.constant.int 20
    %int64_2472 = torch.constant.int 64
    %int64_2473 = torch.constant.int 64
    %2286 = torch.prim.ListConstruct %int1_2470, %int20_2471, %int64_2472, %int64_2473 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2287 = torch.aten.view %2285, %2286 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2474 = torch.constant.int 1
    %int2_2475 = torch.constant.int 2
    %2288 = torch.aten.transpose.int %2287, %int1_2474, %int2_2475 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_2476 = torch.constant.int 0
    %2289 = torch.aten.clone %2288, %int0_2476 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2477 = torch.constant.int 1
    %int64_2478 = torch.constant.int 64
    %int1280_2479 = torch.constant.int 1280
    %2290 = torch.prim.ListConstruct %int1_2477, %int64_2478, %int1280_2479 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2291 = torch.aten._unsafe_view %2289, %2290 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_2480 = torch.constant.int 64
    %int1280_2481 = torch.constant.int 1280
    %2292 = torch.prim.ListConstruct %int64_2480, %int1280_2481 : (!torch.int, !torch.int) -> !torch.list<int>
    %2293 = torch.aten.view %2291, %2292 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %2294 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2482 = torch.constant.int 0
    %int1_2483 = torch.constant.int 1
    %2295 = torch.aten.transpose.int %2294, %int0_2482, %int1_2483 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.bias : tensor<1280xf16>
    %2296 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2484 = torch.constant.int 6
    %2297 = torch.prims.convert_element_type %2296, %int6_2484 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2485 = torch.constant.int 6
    %2298 = torch.prims.convert_element_type %2293, %int6_2485 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2486 = torch.constant.int 6
    %2299 = torch.prims.convert_element_type %2295, %int6_2486 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2300 = torch.aten.mm %2298, %2299 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2487 = torch.constant.int 1
    %2301 = torch.aten.mul.Scalar %2300, %int1_2487 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2488 = torch.constant.int 1
    %2302 = torch.aten.mul.Scalar %2297, %int1_2488 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2489 = torch.constant.int 1
    %2303 = torch.aten.add.Tensor %2301, %2302, %int1_2489 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2490 = torch.constant.int 5
    %2304 = torch.prims.convert_element_type %2303, %int5_2490 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2491 = torch.constant.int 1
    %int64_2492 = torch.constant.int 64
    %int1280_2493 = torch.constant.int 1280
    %2305 = torch.prim.ListConstruct %int1_2491, %int64_2492, %int1280_2493 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2306 = torch.aten.view %2304, %2305 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2494 = torch.constant.int 1
    %2307 = torch.aten.add.Tensor %2197, %2306, %int1_2494 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2495 = torch.constant.int 6
    %2308 = torch.prims.convert_element_type %2307, %int6_2495 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2496 = torch.constant.int 2
    %2309 = torch.prim.ListConstruct %int2_2496 : (!torch.int) -> !torch.list<int>
    %int0_2497 = torch.constant.int 0
    %true_2498 = torch.constant.bool true
    %result0_2499, %result1_2500 = torch.aten.var_mean.correction %2308, %2309, %int0_2497, %true_2498 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2501 = torch.constant.float 1.000000e-05
    %int1_2502 = torch.constant.int 1
    %2310 = torch.aten.add.Scalar %result0_2499, %float1.000000e-05_2501, %int1_2502 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2311 = torch.aten.rsqrt %2310 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2503 = torch.constant.int 1
    %2312 = torch.aten.sub.Tensor %2307, %result1_2500, %int1_2503 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2313 = torch.aten.mul.Tensor %2312, %2311 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.weight : tensor<1280xf16>
    %2314 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2315 = torch.aten.mul.Tensor %2313, %2314 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.bias : tensor<1280xf16>
    %2316 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2504 = torch.constant.int 1
    %2317 = torch.aten.add.Tensor %2315, %2316, %int1_2504 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2505 = torch.constant.int 5
    %2318 = torch.prims.convert_element_type %2317, %int5_2505 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2506 = torch.constant.int 5
    %2319 = torch.prims.convert_element_type %result1_2500, %int5_2506 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2507 = torch.constant.int 5
    %2320 = torch.prims.convert_element_type %2311, %int5_2507 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2508 = torch.constant.int 64
    %int1280_2509 = torch.constant.int 1280
    %2321 = torch.prim.ListConstruct %int64_2508, %int1280_2509 : (!torch.int, !torch.int) -> !torch.list<int>
    %2322 = torch.aten.view %2318, %2321 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.weight : tensor<5120x1280xf16>
    %2323 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_2510 = torch.constant.int 0
    %int1_2511 = torch.constant.int 1
    %2324 = torch.aten.transpose.int %2323, %int0_2510, %int1_2511 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.bias : tensor<5120xf16>
    %2325 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_2512 = torch.constant.int 6
    %2326 = torch.prims.convert_element_type %2325, %int6_2512 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_2513 = torch.constant.int 6
    %2327 = torch.prims.convert_element_type %2322, %int6_2513 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2514 = torch.constant.int 6
    %2328 = torch.prims.convert_element_type %2324, %int6_2514 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2329 = torch.aten.mm %2327, %2328 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_2515 = torch.constant.int 1
    %2330 = torch.aten.mul.Scalar %2329, %int1_2515 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_2516 = torch.constant.int 1
    %2331 = torch.aten.mul.Scalar %2326, %int1_2516 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_2517 = torch.constant.int 1
    %2332 = torch.aten.add.Tensor %2330, %2331, %int1_2517 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_2518 = torch.constant.int 5
    %2333 = torch.prims.convert_element_type %2332, %int5_2518 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_2519 = torch.constant.int 1
    %int64_2520 = torch.constant.int 64
    %int5120_2521 = torch.constant.int 5120
    %2334 = torch.prim.ListConstruct %int1_2519, %int64_2520, %int5120_2521 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2335 = torch.aten.view %2333, %2334 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_2522 = torch.constant.str "none"
    %2336 = torch.aten.gelu %2335, %str_2522 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_2523 = torch.constant.int 64
    %int5120_2524 = torch.constant.int 5120
    %2337 = torch.prim.ListConstruct %int64_2523, %int5120_2524 : (!torch.int, !torch.int) -> !torch.list<int>
    %2338 = torch.aten.view %2336, %2337 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.weight : tensor<1280x5120xf16>
    %2339 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2525 = torch.constant.int 0
    %int1_2526 = torch.constant.int 1
    %2340 = torch.aten.transpose.int %2339, %int0_2525, %int1_2526 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.bias : tensor<1280xf16>
    %2341 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.14.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2527 = torch.constant.int 6
    %2342 = torch.prims.convert_element_type %2341, %int6_2527 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2528 = torch.constant.int 6
    %2343 = torch.prims.convert_element_type %2338, %int6_2528 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_2529 = torch.constant.int 6
    %2344 = torch.prims.convert_element_type %2340, %int6_2529 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2345 = torch.aten.mm %2343, %2344 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2530 = torch.constant.int 1
    %2346 = torch.aten.mul.Scalar %2345, %int1_2530 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2531 = torch.constant.int 1
    %2347 = torch.aten.mul.Scalar %2342, %int1_2531 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2532 = torch.constant.int 1
    %2348 = torch.aten.add.Tensor %2346, %2347, %int1_2532 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2533 = torch.constant.int 5
    %2349 = torch.prims.convert_element_type %2348, %int5_2533 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2534 = torch.constant.int 1
    %int64_2535 = torch.constant.int 64
    %int1280_2536 = torch.constant.int 1280
    %2350 = torch.prim.ListConstruct %int1_2534, %int64_2535, %int1280_2536 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2351 = torch.aten.view %2349, %2350 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2537 = torch.constant.int 1
    %2352 = torch.aten.add.Tensor %2307, %2351, %int1_2537 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2538 = torch.constant.int 6
    %2353 = torch.prims.convert_element_type %2352, %int6_2538 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2539 = torch.constant.int 2
    %2354 = torch.prim.ListConstruct %int2_2539 : (!torch.int) -> !torch.list<int>
    %int0_2540 = torch.constant.int 0
    %true_2541 = torch.constant.bool true
    %result0_2542, %result1_2543 = torch.aten.var_mean.correction %2353, %2354, %int0_2540, %true_2541 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2544 = torch.constant.float 1.000000e-05
    %int1_2545 = torch.constant.int 1
    %2355 = torch.aten.add.Scalar %result0_2542, %float1.000000e-05_2544, %int1_2545 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2356 = torch.aten.rsqrt %2355 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2546 = torch.constant.int 1
    %2357 = torch.aten.sub.Tensor %2352, %result1_2543, %int1_2546 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2358 = torch.aten.mul.Tensor %2357, %2356 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.weight : tensor<1280xf16>
    %2359 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2360 = torch.aten.mul.Tensor %2358, %2359 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.bias : tensor<1280xf16>
    %2361 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2547 = torch.constant.int 1
    %2362 = torch.aten.add.Tensor %2360, %2361, %int1_2547 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2548 = torch.constant.int 5
    %2363 = torch.prims.convert_element_type %2362, %int5_2548 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2549 = torch.constant.int 5
    %2364 = torch.prims.convert_element_type %result1_2543, %int5_2549 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2550 = torch.constant.int 5
    %2365 = torch.prims.convert_element_type %2356, %int5_2550 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2551 = torch.constant.int 64
    %int1280_2552 = torch.constant.int 1280
    %2366 = torch.prim.ListConstruct %int64_2551, %int1280_2552 : (!torch.int, !torch.int) -> !torch.list<int>
    %2367 = torch.aten.view %2363, %2366 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2368 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2553 = torch.constant.int 0
    %int1_2554 = torch.constant.int 1
    %2369 = torch.aten.transpose.int %2368, %int0_2553, %int1_2554 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.bias : tensor<1280xf16>
    %2370 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2555 = torch.constant.int 6
    %2371 = torch.prims.convert_element_type %2370, %int6_2555 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2556 = torch.constant.int 6
    %2372 = torch.prims.convert_element_type %2367, %int6_2556 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2557 = torch.constant.int 6
    %2373 = torch.prims.convert_element_type %2369, %int6_2557 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2374 = torch.aten.mm %2372, %2373 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2558 = torch.constant.int 1
    %2375 = torch.aten.mul.Scalar %2374, %int1_2558 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2559 = torch.constant.int 1
    %2376 = torch.aten.mul.Scalar %2371, %int1_2559 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2560 = torch.constant.int 1
    %2377 = torch.aten.add.Tensor %2375, %2376, %int1_2560 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2561 = torch.constant.int 5
    %2378 = torch.prims.convert_element_type %2377, %int5_2561 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2562 = torch.constant.int 1
    %int64_2563 = torch.constant.int 64
    %int1280_2564 = torch.constant.int 1280
    %2379 = torch.prim.ListConstruct %int1_2562, %int64_2563, %int1280_2564 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2380 = torch.aten.view %2378, %2379 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_2565 = torch.constant.float 1.250000e-01
    %2381 = torch.aten.mul.Scalar %2380, %float1.250000e-01_2565 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_2566 = torch.constant.int 64
    %int1280_2567 = torch.constant.int 1280
    %2382 = torch.prim.ListConstruct %int64_2566, %int1280_2567 : (!torch.int, !torch.int) -> !torch.list<int>
    %2383 = torch.aten.view %2363, %2382 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %2384 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2568 = torch.constant.int 0
    %int1_2569 = torch.constant.int 1
    %2385 = torch.aten.transpose.int %2384, %int0_2568, %int1_2569 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.bias : tensor<1280xf16>
    %2386 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2570 = torch.constant.int 6
    %2387 = torch.prims.convert_element_type %2386, %int6_2570 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2571 = torch.constant.int 6
    %2388 = torch.prims.convert_element_type %2383, %int6_2571 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2572 = torch.constant.int 6
    %2389 = torch.prims.convert_element_type %2385, %int6_2572 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2390 = torch.aten.mm %2388, %2389 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2573 = torch.constant.int 1
    %2391 = torch.aten.mul.Scalar %2390, %int1_2573 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2574 = torch.constant.int 1
    %2392 = torch.aten.mul.Scalar %2387, %int1_2574 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2575 = torch.constant.int 1
    %2393 = torch.aten.add.Tensor %2391, %2392, %int1_2575 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2576 = torch.constant.int 5
    %2394 = torch.prims.convert_element_type %2393, %int5_2576 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2577 = torch.constant.int 1
    %int64_2578 = torch.constant.int 64
    %int1280_2579 = torch.constant.int 1280
    %2395 = torch.prim.ListConstruct %int1_2577, %int64_2578, %int1280_2579 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2396 = torch.aten.view %2394, %2395 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2580 = torch.constant.int 1
    %int-1_2581 = torch.constant.int -1
    %int20_2582 = torch.constant.int 20
    %int64_2583 = torch.constant.int 64
    %2397 = torch.prim.ListConstruct %int1_2580, %int-1_2581, %int20_2582, %int64_2583 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2398 = torch.aten.view %2396, %2397 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2584 = torch.constant.int 1
    %int2_2585 = torch.constant.int 2
    %2399 = torch.aten.transpose.int %2398, %int1_2584, %int2_2585 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2586 = torch.constant.int 0
    %2400 = torch.aten.clone %2399, %int0_2586 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_2587 = torch.constant.int 64
    %int1280_2588 = torch.constant.int 1280
    %2401 = torch.prim.ListConstruct %int64_2587, %int1280_2588 : (!torch.int, !torch.int) -> !torch.list<int>
    %2402 = torch.aten.view %2363, %2401 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %2403 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2589 = torch.constant.int 0
    %int1_2590 = torch.constant.int 1
    %2404 = torch.aten.transpose.int %2403, %int0_2589, %int1_2590 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.bias : tensor<1280xf16>
    %2405 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2591 = torch.constant.int 6
    %2406 = torch.prims.convert_element_type %2405, %int6_2591 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2592 = torch.constant.int 6
    %2407 = torch.prims.convert_element_type %2402, %int6_2592 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2593 = torch.constant.int 6
    %2408 = torch.prims.convert_element_type %2404, %int6_2593 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2409 = torch.aten.mm %2407, %2408 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2594 = torch.constant.int 1
    %2410 = torch.aten.mul.Scalar %2409, %int1_2594 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2595 = torch.constant.int 1
    %2411 = torch.aten.mul.Scalar %2406, %int1_2595 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2596 = torch.constant.int 1
    %2412 = torch.aten.add.Tensor %2410, %2411, %int1_2596 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2597 = torch.constant.int 5
    %2413 = torch.prims.convert_element_type %2412, %int5_2597 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2598 = torch.constant.int 1
    %int64_2599 = torch.constant.int 64
    %int1280_2600 = torch.constant.int 1280
    %2414 = torch.prim.ListConstruct %int1_2598, %int64_2599, %int1280_2600 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2415 = torch.aten.view %2413, %2414 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2601 = torch.constant.int 1
    %int-1_2602 = torch.constant.int -1
    %int20_2603 = torch.constant.int 20
    %int64_2604 = torch.constant.int 64
    %2416 = torch.prim.ListConstruct %int1_2601, %int-1_2602, %int20_2603, %int64_2604 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2417 = torch.aten.view %2415, %2416 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2605 = torch.constant.int 1
    %int2_2606 = torch.constant.int 2
    %2418 = torch.aten.transpose.int %2417, %int1_2605, %int2_2606 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2607 = torch.constant.int 0
    %2419 = torch.aten.clone %2418, %int0_2607 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2608 = torch.constant.int 1
    %int64_2609 = torch.constant.int 64
    %int20_2610 = torch.constant.int 20
    %int64_2611 = torch.constant.int 64
    %2420 = torch.prim.ListConstruct %int1_2608, %int64_2609, %int20_2610, %int64_2611 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2421 = torch.aten.view %2381, %2420 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2612 = torch.constant.int 1
    %int2_2613 = torch.constant.int 2
    %2422 = torch.aten.transpose.int %2421, %int1_2612, %int2_2613 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2614 = torch.constant.int 0
    %2423 = torch.aten.clone %2422, %int0_2614 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2615 = torch.constant.int 20
    %int-1_2616 = torch.constant.int -1
    %int64_2617 = torch.constant.int 64
    %2424 = torch.prim.ListConstruct %int20_2615, %int-1_2616, %int64_2617 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2425 = torch.aten.view %2423, %2424 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2618 = torch.constant.int 20
    %int-1_2619 = torch.constant.int -1
    %int64_2620 = torch.constant.int 64
    %2426 = torch.prim.ListConstruct %int20_2618, %int-1_2619, %int64_2620 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2427 = torch.aten.view %2400, %2426 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2621 = torch.constant.int 20
    %int-1_2622 = torch.constant.int -1
    %int64_2623 = torch.constant.int 64
    %2428 = torch.prim.ListConstruct %int20_2621, %int-1_2622, %int64_2623 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2429 = torch.aten.view %2419, %2428 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_2624 = torch.constant.int 1
    %int2_2625 = torch.constant.int 2
    %2430 = torch.aten.transpose.int %2427, %int1_2624, %int2_2625 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %2431 = torch.aten.bmm %2425, %2430 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2626 = torch.constant.int 1
    %int20_2627 = torch.constant.int 20
    %int64_2628 = torch.constant.int 64
    %int64_2629 = torch.constant.int 64
    %2432 = torch.prim.ListConstruct %int1_2626, %int20_2627, %int64_2628, %int64_2629 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2433 = torch.aten.view %2431, %2432 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2630 = torch.constant.int 1
    %2434 = torch.aten.add.Tensor %2433, %27, %int1_2630 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2631 = torch.constant.int 20
    %int64_2632 = torch.constant.int 64
    %int64_2633 = torch.constant.int 64
    %2435 = torch.prim.ListConstruct %int20_2631, %int64_2632, %int64_2633 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2436 = torch.aten.view %2434, %2435 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_2634 = torch.constant.int -1
    %false_2635 = torch.constant.bool false
    %2437 = torch.aten._softmax %2436, %int-1_2634, %false_2635 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %2438 = torch.aten.detach %2437 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_2636 = torch.constant.none
    %2439 = torch.aten.clone %2437, %none_2636 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %2440 = torch.aten.bmm %2439, %2429 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2637 = torch.constant.int 1
    %int20_2638 = torch.constant.int 20
    %int64_2639 = torch.constant.int 64
    %int64_2640 = torch.constant.int 64
    %2441 = torch.prim.ListConstruct %int1_2637, %int20_2638, %int64_2639, %int64_2640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2442 = torch.aten.view %2440, %2441 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2641 = torch.constant.int 1
    %int2_2642 = torch.constant.int 2
    %2443 = torch.aten.transpose.int %2442, %int1_2641, %int2_2642 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_2643 = torch.constant.int 0
    %2444 = torch.aten.clone %2443, %int0_2643 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2644 = torch.constant.int 1
    %int64_2645 = torch.constant.int 64
    %int1280_2646 = torch.constant.int 1280
    %2445 = torch.prim.ListConstruct %int1_2644, %int64_2645, %int1280_2646 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2446 = torch.aten._unsafe_view %2444, %2445 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_2647 = torch.constant.int 64
    %int1280_2648 = torch.constant.int 1280
    %2447 = torch.prim.ListConstruct %int64_2647, %int1280_2648 : (!torch.int, !torch.int) -> !torch.list<int>
    %2448 = torch.aten.view %2446, %2447 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %2449 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2649 = torch.constant.int 0
    %int1_2650 = torch.constant.int 1
    %2450 = torch.aten.transpose.int %2449, %int0_2649, %int1_2650 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.bias : tensor<1280xf16>
    %2451 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2651 = torch.constant.int 6
    %2452 = torch.prims.convert_element_type %2451, %int6_2651 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2652 = torch.constant.int 6
    %2453 = torch.prims.convert_element_type %2448, %int6_2652 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2653 = torch.constant.int 6
    %2454 = torch.prims.convert_element_type %2450, %int6_2653 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2455 = torch.aten.mm %2453, %2454 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2654 = torch.constant.int 1
    %2456 = torch.aten.mul.Scalar %2455, %int1_2654 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2655 = torch.constant.int 1
    %2457 = torch.aten.mul.Scalar %2452, %int1_2655 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2656 = torch.constant.int 1
    %2458 = torch.aten.add.Tensor %2456, %2457, %int1_2656 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2657 = torch.constant.int 5
    %2459 = torch.prims.convert_element_type %2458, %int5_2657 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2658 = torch.constant.int 1
    %int64_2659 = torch.constant.int 64
    %int1280_2660 = torch.constant.int 1280
    %2460 = torch.prim.ListConstruct %int1_2658, %int64_2659, %int1280_2660 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2461 = torch.aten.view %2459, %2460 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2661 = torch.constant.int 1
    %2462 = torch.aten.add.Tensor %2352, %2461, %int1_2661 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2662 = torch.constant.int 6
    %2463 = torch.prims.convert_element_type %2462, %int6_2662 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2663 = torch.constant.int 2
    %2464 = torch.prim.ListConstruct %int2_2663 : (!torch.int) -> !torch.list<int>
    %int0_2664 = torch.constant.int 0
    %true_2665 = torch.constant.bool true
    %result0_2666, %result1_2667 = torch.aten.var_mean.correction %2463, %2464, %int0_2664, %true_2665 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2668 = torch.constant.float 1.000000e-05
    %int1_2669 = torch.constant.int 1
    %2465 = torch.aten.add.Scalar %result0_2666, %float1.000000e-05_2668, %int1_2669 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2466 = torch.aten.rsqrt %2465 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2670 = torch.constant.int 1
    %2467 = torch.aten.sub.Tensor %2462, %result1_2667, %int1_2670 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2468 = torch.aten.mul.Tensor %2467, %2466 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.weight : tensor<1280xf16>
    %2469 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2470 = torch.aten.mul.Tensor %2468, %2469 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.bias : tensor<1280xf16>
    %2471 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2671 = torch.constant.int 1
    %2472 = torch.aten.add.Tensor %2470, %2471, %int1_2671 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2672 = torch.constant.int 5
    %2473 = torch.prims.convert_element_type %2472, %int5_2672 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2673 = torch.constant.int 5
    %2474 = torch.prims.convert_element_type %result1_2667, %int5_2673 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2674 = torch.constant.int 5
    %2475 = torch.prims.convert_element_type %2466, %int5_2674 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2675 = torch.constant.int 64
    %int1280_2676 = torch.constant.int 1280
    %2476 = torch.prim.ListConstruct %int64_2675, %int1280_2676 : (!torch.int, !torch.int) -> !torch.list<int>
    %2477 = torch.aten.view %2473, %2476 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.weight : tensor<5120x1280xf16>
    %2478 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_2677 = torch.constant.int 0
    %int1_2678 = torch.constant.int 1
    %2479 = torch.aten.transpose.int %2478, %int0_2677, %int1_2678 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.bias : tensor<5120xf16>
    %2480 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_2679 = torch.constant.int 6
    %2481 = torch.prims.convert_element_type %2480, %int6_2679 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_2680 = torch.constant.int 6
    %2482 = torch.prims.convert_element_type %2477, %int6_2680 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2681 = torch.constant.int 6
    %2483 = torch.prims.convert_element_type %2479, %int6_2681 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2484 = torch.aten.mm %2482, %2483 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_2682 = torch.constant.int 1
    %2485 = torch.aten.mul.Scalar %2484, %int1_2682 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_2683 = torch.constant.int 1
    %2486 = torch.aten.mul.Scalar %2481, %int1_2683 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_2684 = torch.constant.int 1
    %2487 = torch.aten.add.Tensor %2485, %2486, %int1_2684 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_2685 = torch.constant.int 5
    %2488 = torch.prims.convert_element_type %2487, %int5_2685 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_2686 = torch.constant.int 1
    %int64_2687 = torch.constant.int 64
    %int5120_2688 = torch.constant.int 5120
    %2489 = torch.prim.ListConstruct %int1_2686, %int64_2687, %int5120_2688 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2490 = torch.aten.view %2488, %2489 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_2689 = torch.constant.str "none"
    %2491 = torch.aten.gelu %2490, %str_2689 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_2690 = torch.constant.int 64
    %int5120_2691 = torch.constant.int 5120
    %2492 = torch.prim.ListConstruct %int64_2690, %int5120_2691 : (!torch.int, !torch.int) -> !torch.list<int>
    %2493 = torch.aten.view %2491, %2492 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.weight : tensor<1280x5120xf16>
    %2494 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2692 = torch.constant.int 0
    %int1_2693 = torch.constant.int 1
    %2495 = torch.aten.transpose.int %2494, %int0_2692, %int1_2693 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.bias : tensor<1280xf16>
    %2496 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.15.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2694 = torch.constant.int 6
    %2497 = torch.prims.convert_element_type %2496, %int6_2694 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2695 = torch.constant.int 6
    %2498 = torch.prims.convert_element_type %2493, %int6_2695 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_2696 = torch.constant.int 6
    %2499 = torch.prims.convert_element_type %2495, %int6_2696 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2500 = torch.aten.mm %2498, %2499 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2697 = torch.constant.int 1
    %2501 = torch.aten.mul.Scalar %2500, %int1_2697 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2698 = torch.constant.int 1
    %2502 = torch.aten.mul.Scalar %2497, %int1_2698 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2699 = torch.constant.int 1
    %2503 = torch.aten.add.Tensor %2501, %2502, %int1_2699 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2700 = torch.constant.int 5
    %2504 = torch.prims.convert_element_type %2503, %int5_2700 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2701 = torch.constant.int 1
    %int64_2702 = torch.constant.int 64
    %int1280_2703 = torch.constant.int 1280
    %2505 = torch.prim.ListConstruct %int1_2701, %int64_2702, %int1280_2703 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2506 = torch.aten.view %2504, %2505 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2704 = torch.constant.int 1
    %2507 = torch.aten.add.Tensor %2462, %2506, %int1_2704 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2705 = torch.constant.int 6
    %2508 = torch.prims.convert_element_type %2507, %int6_2705 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2706 = torch.constant.int 2
    %2509 = torch.prim.ListConstruct %int2_2706 : (!torch.int) -> !torch.list<int>
    %int0_2707 = torch.constant.int 0
    %true_2708 = torch.constant.bool true
    %result0_2709, %result1_2710 = torch.aten.var_mean.correction %2508, %2509, %int0_2707, %true_2708 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2711 = torch.constant.float 1.000000e-05
    %int1_2712 = torch.constant.int 1
    %2510 = torch.aten.add.Scalar %result0_2709, %float1.000000e-05_2711, %int1_2712 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2511 = torch.aten.rsqrt %2510 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2713 = torch.constant.int 1
    %2512 = torch.aten.sub.Tensor %2507, %result1_2710, %int1_2713 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2513 = torch.aten.mul.Tensor %2512, %2511 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.weight : tensor<1280xf16>
    %2514 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2515 = torch.aten.mul.Tensor %2513, %2514 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.bias : tensor<1280xf16>
    %2516 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2714 = torch.constant.int 1
    %2517 = torch.aten.add.Tensor %2515, %2516, %int1_2714 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2715 = torch.constant.int 5
    %2518 = torch.prims.convert_element_type %2517, %int5_2715 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2716 = torch.constant.int 5
    %2519 = torch.prims.convert_element_type %result1_2710, %int5_2716 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2717 = torch.constant.int 5
    %2520 = torch.prims.convert_element_type %2511, %int5_2717 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2718 = torch.constant.int 64
    %int1280_2719 = torch.constant.int 1280
    %2521 = torch.prim.ListConstruct %int64_2718, %int1280_2719 : (!torch.int, !torch.int) -> !torch.list<int>
    %2522 = torch.aten.view %2518, %2521 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2523 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2720 = torch.constant.int 0
    %int1_2721 = torch.constant.int 1
    %2524 = torch.aten.transpose.int %2523, %int0_2720, %int1_2721 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.bias : tensor<1280xf16>
    %2525 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2722 = torch.constant.int 6
    %2526 = torch.prims.convert_element_type %2525, %int6_2722 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2723 = torch.constant.int 6
    %2527 = torch.prims.convert_element_type %2522, %int6_2723 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2724 = torch.constant.int 6
    %2528 = torch.prims.convert_element_type %2524, %int6_2724 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2529 = torch.aten.mm %2527, %2528 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2725 = torch.constant.int 1
    %2530 = torch.aten.mul.Scalar %2529, %int1_2725 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2726 = torch.constant.int 1
    %2531 = torch.aten.mul.Scalar %2526, %int1_2726 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2727 = torch.constant.int 1
    %2532 = torch.aten.add.Tensor %2530, %2531, %int1_2727 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2728 = torch.constant.int 5
    %2533 = torch.prims.convert_element_type %2532, %int5_2728 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2729 = torch.constant.int 1
    %int64_2730 = torch.constant.int 64
    %int1280_2731 = torch.constant.int 1280
    %2534 = torch.prim.ListConstruct %int1_2729, %int64_2730, %int1280_2731 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2535 = torch.aten.view %2533, %2534 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_2732 = torch.constant.float 1.250000e-01
    %2536 = torch.aten.mul.Scalar %2535, %float1.250000e-01_2732 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_2733 = torch.constant.int 64
    %int1280_2734 = torch.constant.int 1280
    %2537 = torch.prim.ListConstruct %int64_2733, %int1280_2734 : (!torch.int, !torch.int) -> !torch.list<int>
    %2538 = torch.aten.view %2518, %2537 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %2539 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2735 = torch.constant.int 0
    %int1_2736 = torch.constant.int 1
    %2540 = torch.aten.transpose.int %2539, %int0_2735, %int1_2736 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.bias : tensor<1280xf16>
    %2541 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2737 = torch.constant.int 6
    %2542 = torch.prims.convert_element_type %2541, %int6_2737 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2738 = torch.constant.int 6
    %2543 = torch.prims.convert_element_type %2538, %int6_2738 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2739 = torch.constant.int 6
    %2544 = torch.prims.convert_element_type %2540, %int6_2739 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2545 = torch.aten.mm %2543, %2544 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2740 = torch.constant.int 1
    %2546 = torch.aten.mul.Scalar %2545, %int1_2740 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2741 = torch.constant.int 1
    %2547 = torch.aten.mul.Scalar %2542, %int1_2741 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2742 = torch.constant.int 1
    %2548 = torch.aten.add.Tensor %2546, %2547, %int1_2742 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2743 = torch.constant.int 5
    %2549 = torch.prims.convert_element_type %2548, %int5_2743 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2744 = torch.constant.int 1
    %int64_2745 = torch.constant.int 64
    %int1280_2746 = torch.constant.int 1280
    %2550 = torch.prim.ListConstruct %int1_2744, %int64_2745, %int1280_2746 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2551 = torch.aten.view %2549, %2550 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2747 = torch.constant.int 1
    %int-1_2748 = torch.constant.int -1
    %int20_2749 = torch.constant.int 20
    %int64_2750 = torch.constant.int 64
    %2552 = torch.prim.ListConstruct %int1_2747, %int-1_2748, %int20_2749, %int64_2750 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2553 = torch.aten.view %2551, %2552 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2751 = torch.constant.int 1
    %int2_2752 = torch.constant.int 2
    %2554 = torch.aten.transpose.int %2553, %int1_2751, %int2_2752 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2753 = torch.constant.int 0
    %2555 = torch.aten.clone %2554, %int0_2753 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_2754 = torch.constant.int 64
    %int1280_2755 = torch.constant.int 1280
    %2556 = torch.prim.ListConstruct %int64_2754, %int1280_2755 : (!torch.int, !torch.int) -> !torch.list<int>
    %2557 = torch.aten.view %2518, %2556 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %2558 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2756 = torch.constant.int 0
    %int1_2757 = torch.constant.int 1
    %2559 = torch.aten.transpose.int %2558, %int0_2756, %int1_2757 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.bias : tensor<1280xf16>
    %2560 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2758 = torch.constant.int 6
    %2561 = torch.prims.convert_element_type %2560, %int6_2758 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2759 = torch.constant.int 6
    %2562 = torch.prims.convert_element_type %2557, %int6_2759 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2760 = torch.constant.int 6
    %2563 = torch.prims.convert_element_type %2559, %int6_2760 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2564 = torch.aten.mm %2562, %2563 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2761 = torch.constant.int 1
    %2565 = torch.aten.mul.Scalar %2564, %int1_2761 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2762 = torch.constant.int 1
    %2566 = torch.aten.mul.Scalar %2561, %int1_2762 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2763 = torch.constant.int 1
    %2567 = torch.aten.add.Tensor %2565, %2566, %int1_2763 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2764 = torch.constant.int 5
    %2568 = torch.prims.convert_element_type %2567, %int5_2764 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2765 = torch.constant.int 1
    %int64_2766 = torch.constant.int 64
    %int1280_2767 = torch.constant.int 1280
    %2569 = torch.prim.ListConstruct %int1_2765, %int64_2766, %int1280_2767 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2570 = torch.aten.view %2568, %2569 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2768 = torch.constant.int 1
    %int-1_2769 = torch.constant.int -1
    %int20_2770 = torch.constant.int 20
    %int64_2771 = torch.constant.int 64
    %2571 = torch.prim.ListConstruct %int1_2768, %int-1_2769, %int20_2770, %int64_2771 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2572 = torch.aten.view %2570, %2571 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2772 = torch.constant.int 1
    %int2_2773 = torch.constant.int 2
    %2573 = torch.aten.transpose.int %2572, %int1_2772, %int2_2773 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2774 = torch.constant.int 0
    %2574 = torch.aten.clone %2573, %int0_2774 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2775 = torch.constant.int 1
    %int64_2776 = torch.constant.int 64
    %int20_2777 = torch.constant.int 20
    %int64_2778 = torch.constant.int 64
    %2575 = torch.prim.ListConstruct %int1_2775, %int64_2776, %int20_2777, %int64_2778 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2576 = torch.aten.view %2536, %2575 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2779 = torch.constant.int 1
    %int2_2780 = torch.constant.int 2
    %2577 = torch.aten.transpose.int %2576, %int1_2779, %int2_2780 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2781 = torch.constant.int 0
    %2578 = torch.aten.clone %2577, %int0_2781 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2782 = torch.constant.int 20
    %int-1_2783 = torch.constant.int -1
    %int64_2784 = torch.constant.int 64
    %2579 = torch.prim.ListConstruct %int20_2782, %int-1_2783, %int64_2784 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2580 = torch.aten.view %2578, %2579 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2785 = torch.constant.int 20
    %int-1_2786 = torch.constant.int -1
    %int64_2787 = torch.constant.int 64
    %2581 = torch.prim.ListConstruct %int20_2785, %int-1_2786, %int64_2787 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2582 = torch.aten.view %2555, %2581 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2788 = torch.constant.int 20
    %int-1_2789 = torch.constant.int -1
    %int64_2790 = torch.constant.int 64
    %2583 = torch.prim.ListConstruct %int20_2788, %int-1_2789, %int64_2790 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2584 = torch.aten.view %2574, %2583 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_2791 = torch.constant.int 1
    %int2_2792 = torch.constant.int 2
    %2585 = torch.aten.transpose.int %2582, %int1_2791, %int2_2792 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %2586 = torch.aten.bmm %2580, %2585 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2793 = torch.constant.int 1
    %int20_2794 = torch.constant.int 20
    %int64_2795 = torch.constant.int 64
    %int64_2796 = torch.constant.int 64
    %2587 = torch.prim.ListConstruct %int1_2793, %int20_2794, %int64_2795, %int64_2796 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2588 = torch.aten.view %2586, %2587 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2797 = torch.constant.int 1
    %2589 = torch.aten.add.Tensor %2588, %27, %int1_2797 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2798 = torch.constant.int 20
    %int64_2799 = torch.constant.int 64
    %int64_2800 = torch.constant.int 64
    %2590 = torch.prim.ListConstruct %int20_2798, %int64_2799, %int64_2800 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2591 = torch.aten.view %2589, %2590 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_2801 = torch.constant.int -1
    %false_2802 = torch.constant.bool false
    %2592 = torch.aten._softmax %2591, %int-1_2801, %false_2802 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %2593 = torch.aten.detach %2592 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_2803 = torch.constant.none
    %2594 = torch.aten.clone %2592, %none_2803 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %2595 = torch.aten.bmm %2594, %2584 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2804 = torch.constant.int 1
    %int20_2805 = torch.constant.int 20
    %int64_2806 = torch.constant.int 64
    %int64_2807 = torch.constant.int 64
    %2596 = torch.prim.ListConstruct %int1_2804, %int20_2805, %int64_2806, %int64_2807 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2597 = torch.aten.view %2595, %2596 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2808 = torch.constant.int 1
    %int2_2809 = torch.constant.int 2
    %2598 = torch.aten.transpose.int %2597, %int1_2808, %int2_2809 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_2810 = torch.constant.int 0
    %2599 = torch.aten.clone %2598, %int0_2810 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2811 = torch.constant.int 1
    %int64_2812 = torch.constant.int 64
    %int1280_2813 = torch.constant.int 1280
    %2600 = torch.prim.ListConstruct %int1_2811, %int64_2812, %int1280_2813 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2601 = torch.aten._unsafe_view %2599, %2600 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_2814 = torch.constant.int 64
    %int1280_2815 = torch.constant.int 1280
    %2602 = torch.prim.ListConstruct %int64_2814, %int1280_2815 : (!torch.int, !torch.int) -> !torch.list<int>
    %2603 = torch.aten.view %2601, %2602 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %2604 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2816 = torch.constant.int 0
    %int1_2817 = torch.constant.int 1
    %2605 = torch.aten.transpose.int %2604, %int0_2816, %int1_2817 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.bias : tensor<1280xf16>
    %2606 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2818 = torch.constant.int 6
    %2607 = torch.prims.convert_element_type %2606, %int6_2818 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2819 = torch.constant.int 6
    %2608 = torch.prims.convert_element_type %2603, %int6_2819 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2820 = torch.constant.int 6
    %2609 = torch.prims.convert_element_type %2605, %int6_2820 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2610 = torch.aten.mm %2608, %2609 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2821 = torch.constant.int 1
    %2611 = torch.aten.mul.Scalar %2610, %int1_2821 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2822 = torch.constant.int 1
    %2612 = torch.aten.mul.Scalar %2607, %int1_2822 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2823 = torch.constant.int 1
    %2613 = torch.aten.add.Tensor %2611, %2612, %int1_2823 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2824 = torch.constant.int 5
    %2614 = torch.prims.convert_element_type %2613, %int5_2824 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2825 = torch.constant.int 1
    %int64_2826 = torch.constant.int 64
    %int1280_2827 = torch.constant.int 1280
    %2615 = torch.prim.ListConstruct %int1_2825, %int64_2826, %int1280_2827 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2616 = torch.aten.view %2614, %2615 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2828 = torch.constant.int 1
    %2617 = torch.aten.add.Tensor %2507, %2616, %int1_2828 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2829 = torch.constant.int 6
    %2618 = torch.prims.convert_element_type %2617, %int6_2829 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2830 = torch.constant.int 2
    %2619 = torch.prim.ListConstruct %int2_2830 : (!torch.int) -> !torch.list<int>
    %int0_2831 = torch.constant.int 0
    %true_2832 = torch.constant.bool true
    %result0_2833, %result1_2834 = torch.aten.var_mean.correction %2618, %2619, %int0_2831, %true_2832 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2835 = torch.constant.float 1.000000e-05
    %int1_2836 = torch.constant.int 1
    %2620 = torch.aten.add.Scalar %result0_2833, %float1.000000e-05_2835, %int1_2836 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2621 = torch.aten.rsqrt %2620 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2837 = torch.constant.int 1
    %2622 = torch.aten.sub.Tensor %2617, %result1_2834, %int1_2837 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2623 = torch.aten.mul.Tensor %2622, %2621 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.weight : tensor<1280xf16>
    %2624 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2625 = torch.aten.mul.Tensor %2623, %2624 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.bias : tensor<1280xf16>
    %2626 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2838 = torch.constant.int 1
    %2627 = torch.aten.add.Tensor %2625, %2626, %int1_2838 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2839 = torch.constant.int 5
    %2628 = torch.prims.convert_element_type %2627, %int5_2839 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2840 = torch.constant.int 5
    %2629 = torch.prims.convert_element_type %result1_2834, %int5_2840 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2841 = torch.constant.int 5
    %2630 = torch.prims.convert_element_type %2621, %int5_2841 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2842 = torch.constant.int 64
    %int1280_2843 = torch.constant.int 1280
    %2631 = torch.prim.ListConstruct %int64_2842, %int1280_2843 : (!torch.int, !torch.int) -> !torch.list<int>
    %2632 = torch.aten.view %2628, %2631 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.weight : tensor<5120x1280xf16>
    %2633 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_2844 = torch.constant.int 0
    %int1_2845 = torch.constant.int 1
    %2634 = torch.aten.transpose.int %2633, %int0_2844, %int1_2845 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.bias : tensor<5120xf16>
    %2635 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_2846 = torch.constant.int 6
    %2636 = torch.prims.convert_element_type %2635, %int6_2846 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_2847 = torch.constant.int 6
    %2637 = torch.prims.convert_element_type %2632, %int6_2847 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2848 = torch.constant.int 6
    %2638 = torch.prims.convert_element_type %2634, %int6_2848 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2639 = torch.aten.mm %2637, %2638 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_2849 = torch.constant.int 1
    %2640 = torch.aten.mul.Scalar %2639, %int1_2849 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_2850 = torch.constant.int 1
    %2641 = torch.aten.mul.Scalar %2636, %int1_2850 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_2851 = torch.constant.int 1
    %2642 = torch.aten.add.Tensor %2640, %2641, %int1_2851 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_2852 = torch.constant.int 5
    %2643 = torch.prims.convert_element_type %2642, %int5_2852 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_2853 = torch.constant.int 1
    %int64_2854 = torch.constant.int 64
    %int5120_2855 = torch.constant.int 5120
    %2644 = torch.prim.ListConstruct %int1_2853, %int64_2854, %int5120_2855 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2645 = torch.aten.view %2643, %2644 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_2856 = torch.constant.str "none"
    %2646 = torch.aten.gelu %2645, %str_2856 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_2857 = torch.constant.int 64
    %int5120_2858 = torch.constant.int 5120
    %2647 = torch.prim.ListConstruct %int64_2857, %int5120_2858 : (!torch.int, !torch.int) -> !torch.list<int>
    %2648 = torch.aten.view %2646, %2647 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.weight : tensor<1280x5120xf16>
    %2649 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2859 = torch.constant.int 0
    %int1_2860 = torch.constant.int 1
    %2650 = torch.aten.transpose.int %2649, %int0_2859, %int1_2860 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.bias : tensor<1280xf16>
    %2651 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.16.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2861 = torch.constant.int 6
    %2652 = torch.prims.convert_element_type %2651, %int6_2861 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2862 = torch.constant.int 6
    %2653 = torch.prims.convert_element_type %2648, %int6_2862 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_2863 = torch.constant.int 6
    %2654 = torch.prims.convert_element_type %2650, %int6_2863 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2655 = torch.aten.mm %2653, %2654 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2864 = torch.constant.int 1
    %2656 = torch.aten.mul.Scalar %2655, %int1_2864 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2865 = torch.constant.int 1
    %2657 = torch.aten.mul.Scalar %2652, %int1_2865 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2866 = torch.constant.int 1
    %2658 = torch.aten.add.Tensor %2656, %2657, %int1_2866 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2867 = torch.constant.int 5
    %2659 = torch.prims.convert_element_type %2658, %int5_2867 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2868 = torch.constant.int 1
    %int64_2869 = torch.constant.int 64
    %int1280_2870 = torch.constant.int 1280
    %2660 = torch.prim.ListConstruct %int1_2868, %int64_2869, %int1280_2870 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2661 = torch.aten.view %2659, %2660 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2871 = torch.constant.int 1
    %2662 = torch.aten.add.Tensor %2617, %2661, %int1_2871 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2872 = torch.constant.int 6
    %2663 = torch.prims.convert_element_type %2662, %int6_2872 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2873 = torch.constant.int 2
    %2664 = torch.prim.ListConstruct %int2_2873 : (!torch.int) -> !torch.list<int>
    %int0_2874 = torch.constant.int 0
    %true_2875 = torch.constant.bool true
    %result0_2876, %result1_2877 = torch.aten.var_mean.correction %2663, %2664, %int0_2874, %true_2875 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_2878 = torch.constant.float 1.000000e-05
    %int1_2879 = torch.constant.int 1
    %2665 = torch.aten.add.Scalar %result0_2876, %float1.000000e-05_2878, %int1_2879 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2666 = torch.aten.rsqrt %2665 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_2880 = torch.constant.int 1
    %2667 = torch.aten.sub.Tensor %2662, %result1_2877, %int1_2880 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2668 = torch.aten.mul.Tensor %2667, %2666 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.weight : tensor<1280xf16>
    %2669 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2670 = torch.aten.mul.Tensor %2668, %2669 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.bias : tensor<1280xf16>
    %2671 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2881 = torch.constant.int 1
    %2672 = torch.aten.add.Tensor %2670, %2671, %int1_2881 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_2882 = torch.constant.int 5
    %2673 = torch.prims.convert_element_type %2672, %int5_2882 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_2883 = torch.constant.int 5
    %2674 = torch.prims.convert_element_type %result1_2877, %int5_2883 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_2884 = torch.constant.int 5
    %2675 = torch.prims.convert_element_type %2666, %int5_2884 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_2885 = torch.constant.int 64
    %int1280_2886 = torch.constant.int 1280
    %2676 = torch.prim.ListConstruct %int64_2885, %int1280_2886 : (!torch.int, !torch.int) -> !torch.list<int>
    %2677 = torch.aten.view %2673, %2676 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2678 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2887 = torch.constant.int 0
    %int1_2888 = torch.constant.int 1
    %2679 = torch.aten.transpose.int %2678, %int0_2887, %int1_2888 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.bias : tensor<1280xf16>
    %2680 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2889 = torch.constant.int 6
    %2681 = torch.prims.convert_element_type %2680, %int6_2889 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2890 = torch.constant.int 6
    %2682 = torch.prims.convert_element_type %2677, %int6_2890 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2891 = torch.constant.int 6
    %2683 = torch.prims.convert_element_type %2679, %int6_2891 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2684 = torch.aten.mm %2682, %2683 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2892 = torch.constant.int 1
    %2685 = torch.aten.mul.Scalar %2684, %int1_2892 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2893 = torch.constant.int 1
    %2686 = torch.aten.mul.Scalar %2681, %int1_2893 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2894 = torch.constant.int 1
    %2687 = torch.aten.add.Tensor %2685, %2686, %int1_2894 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2895 = torch.constant.int 5
    %2688 = torch.prims.convert_element_type %2687, %int5_2895 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2896 = torch.constant.int 1
    %int64_2897 = torch.constant.int 64
    %int1280_2898 = torch.constant.int 1280
    %2689 = torch.prim.ListConstruct %int1_2896, %int64_2897, %int1280_2898 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2690 = torch.aten.view %2688, %2689 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_2899 = torch.constant.float 1.250000e-01
    %2691 = torch.aten.mul.Scalar %2690, %float1.250000e-01_2899 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_2900 = torch.constant.int 64
    %int1280_2901 = torch.constant.int 1280
    %2692 = torch.prim.ListConstruct %int64_2900, %int1280_2901 : (!torch.int, !torch.int) -> !torch.list<int>
    %2693 = torch.aten.view %2673, %2692 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %2694 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2902 = torch.constant.int 0
    %int1_2903 = torch.constant.int 1
    %2695 = torch.aten.transpose.int %2694, %int0_2902, %int1_2903 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.bias : tensor<1280xf16>
    %2696 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2904 = torch.constant.int 6
    %2697 = torch.prims.convert_element_type %2696, %int6_2904 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2905 = torch.constant.int 6
    %2698 = torch.prims.convert_element_type %2693, %int6_2905 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2906 = torch.constant.int 6
    %2699 = torch.prims.convert_element_type %2695, %int6_2906 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2700 = torch.aten.mm %2698, %2699 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2907 = torch.constant.int 1
    %2701 = torch.aten.mul.Scalar %2700, %int1_2907 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2908 = torch.constant.int 1
    %2702 = torch.aten.mul.Scalar %2697, %int1_2908 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2909 = torch.constant.int 1
    %2703 = torch.aten.add.Tensor %2701, %2702, %int1_2909 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2910 = torch.constant.int 5
    %2704 = torch.prims.convert_element_type %2703, %int5_2910 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2911 = torch.constant.int 1
    %int64_2912 = torch.constant.int 64
    %int1280_2913 = torch.constant.int 1280
    %2705 = torch.prim.ListConstruct %int1_2911, %int64_2912, %int1280_2913 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2706 = torch.aten.view %2704, %2705 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2914 = torch.constant.int 1
    %int-1_2915 = torch.constant.int -1
    %int20_2916 = torch.constant.int 20
    %int64_2917 = torch.constant.int 64
    %2707 = torch.prim.ListConstruct %int1_2914, %int-1_2915, %int20_2916, %int64_2917 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2708 = torch.aten.view %2706, %2707 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2918 = torch.constant.int 1
    %int2_2919 = torch.constant.int 2
    %2709 = torch.aten.transpose.int %2708, %int1_2918, %int2_2919 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2920 = torch.constant.int 0
    %2710 = torch.aten.clone %2709, %int0_2920 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_2921 = torch.constant.int 64
    %int1280_2922 = torch.constant.int 1280
    %2711 = torch.prim.ListConstruct %int64_2921, %int1280_2922 : (!torch.int, !torch.int) -> !torch.list<int>
    %2712 = torch.aten.view %2673, %2711 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %2713 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2923 = torch.constant.int 0
    %int1_2924 = torch.constant.int 1
    %2714 = torch.aten.transpose.int %2713, %int0_2923, %int1_2924 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.bias : tensor<1280xf16>
    %2715 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2925 = torch.constant.int 6
    %2716 = torch.prims.convert_element_type %2715, %int6_2925 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2926 = torch.constant.int 6
    %2717 = torch.prims.convert_element_type %2712, %int6_2926 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2927 = torch.constant.int 6
    %2718 = torch.prims.convert_element_type %2714, %int6_2927 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2719 = torch.aten.mm %2717, %2718 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2928 = torch.constant.int 1
    %2720 = torch.aten.mul.Scalar %2719, %int1_2928 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2929 = torch.constant.int 1
    %2721 = torch.aten.mul.Scalar %2716, %int1_2929 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2930 = torch.constant.int 1
    %2722 = torch.aten.add.Tensor %2720, %2721, %int1_2930 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2931 = torch.constant.int 5
    %2723 = torch.prims.convert_element_type %2722, %int5_2931 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2932 = torch.constant.int 1
    %int64_2933 = torch.constant.int 64
    %int1280_2934 = torch.constant.int 1280
    %2724 = torch.prim.ListConstruct %int1_2932, %int64_2933, %int1280_2934 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2725 = torch.aten.view %2723, %2724 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2935 = torch.constant.int 1
    %int-1_2936 = torch.constant.int -1
    %int20_2937 = torch.constant.int 20
    %int64_2938 = torch.constant.int 64
    %2726 = torch.prim.ListConstruct %int1_2935, %int-1_2936, %int20_2937, %int64_2938 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2727 = torch.aten.view %2725, %2726 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2939 = torch.constant.int 1
    %int2_2940 = torch.constant.int 2
    %2728 = torch.aten.transpose.int %2727, %int1_2939, %int2_2940 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2941 = torch.constant.int 0
    %2729 = torch.aten.clone %2728, %int0_2941 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2942 = torch.constant.int 1
    %int64_2943 = torch.constant.int 64
    %int20_2944 = torch.constant.int 20
    %int64_2945 = torch.constant.int 64
    %2730 = torch.prim.ListConstruct %int1_2942, %int64_2943, %int20_2944, %int64_2945 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2731 = torch.aten.view %2691, %2730 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2946 = torch.constant.int 1
    %int2_2947 = torch.constant.int 2
    %2732 = torch.aten.transpose.int %2731, %int1_2946, %int2_2947 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_2948 = torch.constant.int 0
    %2733 = torch.aten.clone %2732, %int0_2948 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2949 = torch.constant.int 20
    %int-1_2950 = torch.constant.int -1
    %int64_2951 = torch.constant.int 64
    %2734 = torch.prim.ListConstruct %int20_2949, %int-1_2950, %int64_2951 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2735 = torch.aten.view %2733, %2734 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2952 = torch.constant.int 20
    %int-1_2953 = torch.constant.int -1
    %int64_2954 = torch.constant.int 64
    %2736 = torch.prim.ListConstruct %int20_2952, %int-1_2953, %int64_2954 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2737 = torch.aten.view %2710, %2736 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_2955 = torch.constant.int 20
    %int-1_2956 = torch.constant.int -1
    %int64_2957 = torch.constant.int 64
    %2738 = torch.prim.ListConstruct %int20_2955, %int-1_2956, %int64_2957 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2739 = torch.aten.view %2729, %2738 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_2958 = torch.constant.int 1
    %int2_2959 = torch.constant.int 2
    %2740 = torch.aten.transpose.int %2737, %int1_2958, %int2_2959 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %2741 = torch.aten.bmm %2735, %2740 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2960 = torch.constant.int 1
    %int20_2961 = torch.constant.int 20
    %int64_2962 = torch.constant.int 64
    %int64_2963 = torch.constant.int 64
    %2742 = torch.prim.ListConstruct %int1_2960, %int20_2961, %int64_2962, %int64_2963 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2743 = torch.aten.view %2741, %2742 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2964 = torch.constant.int 1
    %2744 = torch.aten.add.Tensor %2743, %27, %int1_2964 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_2965 = torch.constant.int 20
    %int64_2966 = torch.constant.int 64
    %int64_2967 = torch.constant.int 64
    %2745 = torch.prim.ListConstruct %int20_2965, %int64_2966, %int64_2967 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2746 = torch.aten.view %2744, %2745 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_2968 = torch.constant.int -1
    %false_2969 = torch.constant.bool false
    %2747 = torch.aten._softmax %2746, %int-1_2968, %false_2969 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %2748 = torch.aten.detach %2747 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_2970 = torch.constant.none
    %2749 = torch.aten.clone %2747, %none_2970 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %2750 = torch.aten.bmm %2749, %2739 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_2971 = torch.constant.int 1
    %int20_2972 = torch.constant.int 20
    %int64_2973 = torch.constant.int 64
    %int64_2974 = torch.constant.int 64
    %2751 = torch.prim.ListConstruct %int1_2971, %int20_2972, %int64_2973, %int64_2974 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2752 = torch.aten.view %2750, %2751 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_2975 = torch.constant.int 1
    %int2_2976 = torch.constant.int 2
    %2753 = torch.aten.transpose.int %2752, %int1_2975, %int2_2976 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_2977 = torch.constant.int 0
    %2754 = torch.aten.clone %2753, %int0_2977 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_2978 = torch.constant.int 1
    %int64_2979 = torch.constant.int 64
    %int1280_2980 = torch.constant.int 1280
    %2755 = torch.prim.ListConstruct %int1_2978, %int64_2979, %int1280_2980 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2756 = torch.aten._unsafe_view %2754, %2755 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_2981 = torch.constant.int 64
    %int1280_2982 = torch.constant.int 1280
    %2757 = torch.prim.ListConstruct %int64_2981, %int1280_2982 : (!torch.int, !torch.int) -> !torch.list<int>
    %2758 = torch.aten.view %2756, %2757 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %2759 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2983 = torch.constant.int 0
    %int1_2984 = torch.constant.int 1
    %2760 = torch.aten.transpose.int %2759, %int0_2983, %int1_2984 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.bias : tensor<1280xf16>
    %2761 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2985 = torch.constant.int 6
    %2762 = torch.prims.convert_element_type %2761, %int6_2985 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2986 = torch.constant.int 6
    %2763 = torch.prims.convert_element_type %2758, %int6_2986 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_2987 = torch.constant.int 6
    %2764 = torch.prims.convert_element_type %2760, %int6_2987 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2765 = torch.aten.mm %2763, %2764 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_2988 = torch.constant.int 1
    %2766 = torch.aten.mul.Scalar %2765, %int1_2988 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_2989 = torch.constant.int 1
    %2767 = torch.aten.mul.Scalar %2762, %int1_2989 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2990 = torch.constant.int 1
    %2768 = torch.aten.add.Tensor %2766, %2767, %int1_2990 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_2991 = torch.constant.int 5
    %2769 = torch.prims.convert_element_type %2768, %int5_2991 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_2992 = torch.constant.int 1
    %int64_2993 = torch.constant.int 64
    %int1280_2994 = torch.constant.int 1280
    %2770 = torch.prim.ListConstruct %int1_2992, %int64_2993, %int1280_2994 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2771 = torch.aten.view %2769, %2770 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_2995 = torch.constant.int 1
    %2772 = torch.aten.add.Tensor %2662, %2771, %int1_2995 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_2996 = torch.constant.int 6
    %2773 = torch.prims.convert_element_type %2772, %int6_2996 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_2997 = torch.constant.int 2
    %2774 = torch.prim.ListConstruct %int2_2997 : (!torch.int) -> !torch.list<int>
    %int0_2998 = torch.constant.int 0
    %true_2999 = torch.constant.bool true
    %result0_3000, %result1_3001 = torch.aten.var_mean.correction %2773, %2774, %int0_2998, %true_2999 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3002 = torch.constant.float 1.000000e-05
    %int1_3003 = torch.constant.int 1
    %2775 = torch.aten.add.Scalar %result0_3000, %float1.000000e-05_3002, %int1_3003 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2776 = torch.aten.rsqrt %2775 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3004 = torch.constant.int 1
    %2777 = torch.aten.sub.Tensor %2772, %result1_3001, %int1_3004 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2778 = torch.aten.mul.Tensor %2777, %2776 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.weight : tensor<1280xf16>
    %2779 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2780 = torch.aten.mul.Tensor %2778, %2779 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.bias : tensor<1280xf16>
    %2781 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3005 = torch.constant.int 1
    %2782 = torch.aten.add.Tensor %2780, %2781, %int1_3005 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3006 = torch.constant.int 5
    %2783 = torch.prims.convert_element_type %2782, %int5_3006 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3007 = torch.constant.int 5
    %2784 = torch.prims.convert_element_type %result1_3001, %int5_3007 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3008 = torch.constant.int 5
    %2785 = torch.prims.convert_element_type %2776, %int5_3008 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3009 = torch.constant.int 64
    %int1280_3010 = torch.constant.int 1280
    %2786 = torch.prim.ListConstruct %int64_3009, %int1280_3010 : (!torch.int, !torch.int) -> !torch.list<int>
    %2787 = torch.aten.view %2783, %2786 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.weight : tensor<5120x1280xf16>
    %2788 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_3011 = torch.constant.int 0
    %int1_3012 = torch.constant.int 1
    %2789 = torch.aten.transpose.int %2788, %int0_3011, %int1_3012 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.bias : tensor<5120xf16>
    %2790 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_3013 = torch.constant.int 6
    %2791 = torch.prims.convert_element_type %2790, %int6_3013 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_3014 = torch.constant.int 6
    %2792 = torch.prims.convert_element_type %2787, %int6_3014 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3015 = torch.constant.int 6
    %2793 = torch.prims.convert_element_type %2789, %int6_3015 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2794 = torch.aten.mm %2792, %2793 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_3016 = torch.constant.int 1
    %2795 = torch.aten.mul.Scalar %2794, %int1_3016 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_3017 = torch.constant.int 1
    %2796 = torch.aten.mul.Scalar %2791, %int1_3017 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_3018 = torch.constant.int 1
    %2797 = torch.aten.add.Tensor %2795, %2796, %int1_3018 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_3019 = torch.constant.int 5
    %2798 = torch.prims.convert_element_type %2797, %int5_3019 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_3020 = torch.constant.int 1
    %int64_3021 = torch.constant.int 64
    %int5120_3022 = torch.constant.int 5120
    %2799 = torch.prim.ListConstruct %int1_3020, %int64_3021, %int5120_3022 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2800 = torch.aten.view %2798, %2799 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_3023 = torch.constant.str "none"
    %2801 = torch.aten.gelu %2800, %str_3023 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_3024 = torch.constant.int 64
    %int5120_3025 = torch.constant.int 5120
    %2802 = torch.prim.ListConstruct %int64_3024, %int5120_3025 : (!torch.int, !torch.int) -> !torch.list<int>
    %2803 = torch.aten.view %2801, %2802 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.weight : tensor<1280x5120xf16>
    %2804 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3026 = torch.constant.int 0
    %int1_3027 = torch.constant.int 1
    %2805 = torch.aten.transpose.int %2804, %int0_3026, %int1_3027 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.bias : tensor<1280xf16>
    %2806 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.17.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3028 = torch.constant.int 6
    %2807 = torch.prims.convert_element_type %2806, %int6_3028 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3029 = torch.constant.int 6
    %2808 = torch.prims.convert_element_type %2803, %int6_3029 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_3030 = torch.constant.int 6
    %2809 = torch.prims.convert_element_type %2805, %int6_3030 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2810 = torch.aten.mm %2808, %2809 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3031 = torch.constant.int 1
    %2811 = torch.aten.mul.Scalar %2810, %int1_3031 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3032 = torch.constant.int 1
    %2812 = torch.aten.mul.Scalar %2807, %int1_3032 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3033 = torch.constant.int 1
    %2813 = torch.aten.add.Tensor %2811, %2812, %int1_3033 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3034 = torch.constant.int 5
    %2814 = torch.prims.convert_element_type %2813, %int5_3034 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3035 = torch.constant.int 1
    %int64_3036 = torch.constant.int 64
    %int1280_3037 = torch.constant.int 1280
    %2815 = torch.prim.ListConstruct %int1_3035, %int64_3036, %int1280_3037 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2816 = torch.aten.view %2814, %2815 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3038 = torch.constant.int 1
    %2817 = torch.aten.add.Tensor %2772, %2816, %int1_3038 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3039 = torch.constant.int 6
    %2818 = torch.prims.convert_element_type %2817, %int6_3039 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3040 = torch.constant.int 2
    %2819 = torch.prim.ListConstruct %int2_3040 : (!torch.int) -> !torch.list<int>
    %int0_3041 = torch.constant.int 0
    %true_3042 = torch.constant.bool true
    %result0_3043, %result1_3044 = torch.aten.var_mean.correction %2818, %2819, %int0_3041, %true_3042 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3045 = torch.constant.float 1.000000e-05
    %int1_3046 = torch.constant.int 1
    %2820 = torch.aten.add.Scalar %result0_3043, %float1.000000e-05_3045, %int1_3046 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2821 = torch.aten.rsqrt %2820 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3047 = torch.constant.int 1
    %2822 = torch.aten.sub.Tensor %2817, %result1_3044, %int1_3047 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2823 = torch.aten.mul.Tensor %2822, %2821 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.weight : tensor<1280xf16>
    %2824 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2825 = torch.aten.mul.Tensor %2823, %2824 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.bias : tensor<1280xf16>
    %2826 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3048 = torch.constant.int 1
    %2827 = torch.aten.add.Tensor %2825, %2826, %int1_3048 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3049 = torch.constant.int 5
    %2828 = torch.prims.convert_element_type %2827, %int5_3049 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3050 = torch.constant.int 5
    %2829 = torch.prims.convert_element_type %result1_3044, %int5_3050 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3051 = torch.constant.int 5
    %2830 = torch.prims.convert_element_type %2821, %int5_3051 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3052 = torch.constant.int 64
    %int1280_3053 = torch.constant.int 1280
    %2831 = torch.prim.ListConstruct %int64_3052, %int1280_3053 : (!torch.int, !torch.int) -> !torch.list<int>
    %2832 = torch.aten.view %2828, %2831 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2833 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3054 = torch.constant.int 0
    %int1_3055 = torch.constant.int 1
    %2834 = torch.aten.transpose.int %2833, %int0_3054, %int1_3055 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.bias : tensor<1280xf16>
    %2835 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3056 = torch.constant.int 6
    %2836 = torch.prims.convert_element_type %2835, %int6_3056 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3057 = torch.constant.int 6
    %2837 = torch.prims.convert_element_type %2832, %int6_3057 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3058 = torch.constant.int 6
    %2838 = torch.prims.convert_element_type %2834, %int6_3058 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2839 = torch.aten.mm %2837, %2838 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3059 = torch.constant.int 1
    %2840 = torch.aten.mul.Scalar %2839, %int1_3059 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3060 = torch.constant.int 1
    %2841 = torch.aten.mul.Scalar %2836, %int1_3060 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3061 = torch.constant.int 1
    %2842 = torch.aten.add.Tensor %2840, %2841, %int1_3061 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3062 = torch.constant.int 5
    %2843 = torch.prims.convert_element_type %2842, %int5_3062 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3063 = torch.constant.int 1
    %int64_3064 = torch.constant.int 64
    %int1280_3065 = torch.constant.int 1280
    %2844 = torch.prim.ListConstruct %int1_3063, %int64_3064, %int1280_3065 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2845 = torch.aten.view %2843, %2844 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_3066 = torch.constant.float 1.250000e-01
    %2846 = torch.aten.mul.Scalar %2845, %float1.250000e-01_3066 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_3067 = torch.constant.int 64
    %int1280_3068 = torch.constant.int 1280
    %2847 = torch.prim.ListConstruct %int64_3067, %int1280_3068 : (!torch.int, !torch.int) -> !torch.list<int>
    %2848 = torch.aten.view %2828, %2847 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %2849 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3069 = torch.constant.int 0
    %int1_3070 = torch.constant.int 1
    %2850 = torch.aten.transpose.int %2849, %int0_3069, %int1_3070 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.bias : tensor<1280xf16>
    %2851 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3071 = torch.constant.int 6
    %2852 = torch.prims.convert_element_type %2851, %int6_3071 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3072 = torch.constant.int 6
    %2853 = torch.prims.convert_element_type %2848, %int6_3072 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3073 = torch.constant.int 6
    %2854 = torch.prims.convert_element_type %2850, %int6_3073 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2855 = torch.aten.mm %2853, %2854 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3074 = torch.constant.int 1
    %2856 = torch.aten.mul.Scalar %2855, %int1_3074 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3075 = torch.constant.int 1
    %2857 = torch.aten.mul.Scalar %2852, %int1_3075 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3076 = torch.constant.int 1
    %2858 = torch.aten.add.Tensor %2856, %2857, %int1_3076 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3077 = torch.constant.int 5
    %2859 = torch.prims.convert_element_type %2858, %int5_3077 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3078 = torch.constant.int 1
    %int64_3079 = torch.constant.int 64
    %int1280_3080 = torch.constant.int 1280
    %2860 = torch.prim.ListConstruct %int1_3078, %int64_3079, %int1280_3080 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2861 = torch.aten.view %2859, %2860 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3081 = torch.constant.int 1
    %int-1_3082 = torch.constant.int -1
    %int20_3083 = torch.constant.int 20
    %int64_3084 = torch.constant.int 64
    %2862 = torch.prim.ListConstruct %int1_3081, %int-1_3082, %int20_3083, %int64_3084 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2863 = torch.aten.view %2861, %2862 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3085 = torch.constant.int 1
    %int2_3086 = torch.constant.int 2
    %2864 = torch.aten.transpose.int %2863, %int1_3085, %int2_3086 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3087 = torch.constant.int 0
    %2865 = torch.aten.clone %2864, %int0_3087 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_3088 = torch.constant.int 64
    %int1280_3089 = torch.constant.int 1280
    %2866 = torch.prim.ListConstruct %int64_3088, %int1280_3089 : (!torch.int, !torch.int) -> !torch.list<int>
    %2867 = torch.aten.view %2828, %2866 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %2868 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3090 = torch.constant.int 0
    %int1_3091 = torch.constant.int 1
    %2869 = torch.aten.transpose.int %2868, %int0_3090, %int1_3091 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.bias : tensor<1280xf16>
    %2870 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3092 = torch.constant.int 6
    %2871 = torch.prims.convert_element_type %2870, %int6_3092 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3093 = torch.constant.int 6
    %2872 = torch.prims.convert_element_type %2867, %int6_3093 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3094 = torch.constant.int 6
    %2873 = torch.prims.convert_element_type %2869, %int6_3094 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2874 = torch.aten.mm %2872, %2873 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3095 = torch.constant.int 1
    %2875 = torch.aten.mul.Scalar %2874, %int1_3095 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3096 = torch.constant.int 1
    %2876 = torch.aten.mul.Scalar %2871, %int1_3096 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3097 = torch.constant.int 1
    %2877 = torch.aten.add.Tensor %2875, %2876, %int1_3097 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3098 = torch.constant.int 5
    %2878 = torch.prims.convert_element_type %2877, %int5_3098 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3099 = torch.constant.int 1
    %int64_3100 = torch.constant.int 64
    %int1280_3101 = torch.constant.int 1280
    %2879 = torch.prim.ListConstruct %int1_3099, %int64_3100, %int1280_3101 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2880 = torch.aten.view %2878, %2879 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3102 = torch.constant.int 1
    %int-1_3103 = torch.constant.int -1
    %int20_3104 = torch.constant.int 20
    %int64_3105 = torch.constant.int 64
    %2881 = torch.prim.ListConstruct %int1_3102, %int-1_3103, %int20_3104, %int64_3105 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2882 = torch.aten.view %2880, %2881 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3106 = torch.constant.int 1
    %int2_3107 = torch.constant.int 2
    %2883 = torch.aten.transpose.int %2882, %int1_3106, %int2_3107 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3108 = torch.constant.int 0
    %2884 = torch.aten.clone %2883, %int0_3108 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3109 = torch.constant.int 1
    %int64_3110 = torch.constant.int 64
    %int20_3111 = torch.constant.int 20
    %int64_3112 = torch.constant.int 64
    %2885 = torch.prim.ListConstruct %int1_3109, %int64_3110, %int20_3111, %int64_3112 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2886 = torch.aten.view %2846, %2885 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3113 = torch.constant.int 1
    %int2_3114 = torch.constant.int 2
    %2887 = torch.aten.transpose.int %2886, %int1_3113, %int2_3114 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3115 = torch.constant.int 0
    %2888 = torch.aten.clone %2887, %int0_3115 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3116 = torch.constant.int 20
    %int-1_3117 = torch.constant.int -1
    %int64_3118 = torch.constant.int 64
    %2889 = torch.prim.ListConstruct %int20_3116, %int-1_3117, %int64_3118 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2890 = torch.aten.view %2888, %2889 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3119 = torch.constant.int 20
    %int-1_3120 = torch.constant.int -1
    %int64_3121 = torch.constant.int 64
    %2891 = torch.prim.ListConstruct %int20_3119, %int-1_3120, %int64_3121 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2892 = torch.aten.view %2865, %2891 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3122 = torch.constant.int 20
    %int-1_3123 = torch.constant.int -1
    %int64_3124 = torch.constant.int 64
    %2893 = torch.prim.ListConstruct %int20_3122, %int-1_3123, %int64_3124 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2894 = torch.aten.view %2884, %2893 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_3125 = torch.constant.int 1
    %int2_3126 = torch.constant.int 2
    %2895 = torch.aten.transpose.int %2892, %int1_3125, %int2_3126 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %2896 = torch.aten.bmm %2890, %2895 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3127 = torch.constant.int 1
    %int20_3128 = torch.constant.int 20
    %int64_3129 = torch.constant.int 64
    %int64_3130 = torch.constant.int 64
    %2897 = torch.prim.ListConstruct %int1_3127, %int20_3128, %int64_3129, %int64_3130 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2898 = torch.aten.view %2896, %2897 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3131 = torch.constant.int 1
    %2899 = torch.aten.add.Tensor %2898, %27, %int1_3131 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3132 = torch.constant.int 20
    %int64_3133 = torch.constant.int 64
    %int64_3134 = torch.constant.int 64
    %2900 = torch.prim.ListConstruct %int20_3132, %int64_3133, %int64_3134 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2901 = torch.aten.view %2899, %2900 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_3135 = torch.constant.int -1
    %false_3136 = torch.constant.bool false
    %2902 = torch.aten._softmax %2901, %int-1_3135, %false_3136 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %2903 = torch.aten.detach %2902 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_3137 = torch.constant.none
    %2904 = torch.aten.clone %2902, %none_3137 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %2905 = torch.aten.bmm %2904, %2894 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3138 = torch.constant.int 1
    %int20_3139 = torch.constant.int 20
    %int64_3140 = torch.constant.int 64
    %int64_3141 = torch.constant.int 64
    %2906 = torch.prim.ListConstruct %int1_3138, %int20_3139, %int64_3140, %int64_3141 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2907 = torch.aten.view %2905, %2906 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3142 = torch.constant.int 1
    %int2_3143 = torch.constant.int 2
    %2908 = torch.aten.transpose.int %2907, %int1_3142, %int2_3143 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_3144 = torch.constant.int 0
    %2909 = torch.aten.clone %2908, %int0_3144 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3145 = torch.constant.int 1
    %int64_3146 = torch.constant.int 64
    %int1280_3147 = torch.constant.int 1280
    %2910 = torch.prim.ListConstruct %int1_3145, %int64_3146, %int1280_3147 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2911 = torch.aten._unsafe_view %2909, %2910 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_3148 = torch.constant.int 64
    %int1280_3149 = torch.constant.int 1280
    %2912 = torch.prim.ListConstruct %int64_3148, %int1280_3149 : (!torch.int, !torch.int) -> !torch.list<int>
    %2913 = torch.aten.view %2911, %2912 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %2914 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3150 = torch.constant.int 0
    %int1_3151 = torch.constant.int 1
    %2915 = torch.aten.transpose.int %2914, %int0_3150, %int1_3151 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.bias : tensor<1280xf16>
    %2916 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3152 = torch.constant.int 6
    %2917 = torch.prims.convert_element_type %2916, %int6_3152 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3153 = torch.constant.int 6
    %2918 = torch.prims.convert_element_type %2913, %int6_3153 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3154 = torch.constant.int 6
    %2919 = torch.prims.convert_element_type %2915, %int6_3154 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2920 = torch.aten.mm %2918, %2919 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3155 = torch.constant.int 1
    %2921 = torch.aten.mul.Scalar %2920, %int1_3155 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3156 = torch.constant.int 1
    %2922 = torch.aten.mul.Scalar %2917, %int1_3156 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3157 = torch.constant.int 1
    %2923 = torch.aten.add.Tensor %2921, %2922, %int1_3157 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3158 = torch.constant.int 5
    %2924 = torch.prims.convert_element_type %2923, %int5_3158 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3159 = torch.constant.int 1
    %int64_3160 = torch.constant.int 64
    %int1280_3161 = torch.constant.int 1280
    %2925 = torch.prim.ListConstruct %int1_3159, %int64_3160, %int1280_3161 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2926 = torch.aten.view %2924, %2925 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3162 = torch.constant.int 1
    %2927 = torch.aten.add.Tensor %2817, %2926, %int1_3162 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3163 = torch.constant.int 6
    %2928 = torch.prims.convert_element_type %2927, %int6_3163 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3164 = torch.constant.int 2
    %2929 = torch.prim.ListConstruct %int2_3164 : (!torch.int) -> !torch.list<int>
    %int0_3165 = torch.constant.int 0
    %true_3166 = torch.constant.bool true
    %result0_3167, %result1_3168 = torch.aten.var_mean.correction %2928, %2929, %int0_3165, %true_3166 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3169 = torch.constant.float 1.000000e-05
    %int1_3170 = torch.constant.int 1
    %2930 = torch.aten.add.Scalar %result0_3167, %float1.000000e-05_3169, %int1_3170 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2931 = torch.aten.rsqrt %2930 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3171 = torch.constant.int 1
    %2932 = torch.aten.sub.Tensor %2927, %result1_3168, %int1_3171 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2933 = torch.aten.mul.Tensor %2932, %2931 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.weight : tensor<1280xf16>
    %2934 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2935 = torch.aten.mul.Tensor %2933, %2934 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.bias : tensor<1280xf16>
    %2936 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3172 = torch.constant.int 1
    %2937 = torch.aten.add.Tensor %2935, %2936, %int1_3172 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3173 = torch.constant.int 5
    %2938 = torch.prims.convert_element_type %2937, %int5_3173 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3174 = torch.constant.int 5
    %2939 = torch.prims.convert_element_type %result1_3168, %int5_3174 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3175 = torch.constant.int 5
    %2940 = torch.prims.convert_element_type %2931, %int5_3175 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3176 = torch.constant.int 64
    %int1280_3177 = torch.constant.int 1280
    %2941 = torch.prim.ListConstruct %int64_3176, %int1280_3177 : (!torch.int, !torch.int) -> !torch.list<int>
    %2942 = torch.aten.view %2938, %2941 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.weight : tensor<5120x1280xf16>
    %2943 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_3178 = torch.constant.int 0
    %int1_3179 = torch.constant.int 1
    %2944 = torch.aten.transpose.int %2943, %int0_3178, %int1_3179 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.bias : tensor<5120xf16>
    %2945 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_3180 = torch.constant.int 6
    %2946 = torch.prims.convert_element_type %2945, %int6_3180 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_3181 = torch.constant.int 6
    %2947 = torch.prims.convert_element_type %2942, %int6_3181 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3182 = torch.constant.int 6
    %2948 = torch.prims.convert_element_type %2944, %int6_3182 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %2949 = torch.aten.mm %2947, %2948 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_3183 = torch.constant.int 1
    %2950 = torch.aten.mul.Scalar %2949, %int1_3183 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_3184 = torch.constant.int 1
    %2951 = torch.aten.mul.Scalar %2946, %int1_3184 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_3185 = torch.constant.int 1
    %2952 = torch.aten.add.Tensor %2950, %2951, %int1_3185 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_3186 = torch.constant.int 5
    %2953 = torch.prims.convert_element_type %2952, %int5_3186 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_3187 = torch.constant.int 1
    %int64_3188 = torch.constant.int 64
    %int5120_3189 = torch.constant.int 5120
    %2954 = torch.prim.ListConstruct %int1_3187, %int64_3188, %int5120_3189 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2955 = torch.aten.view %2953, %2954 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_3190 = torch.constant.str "none"
    %2956 = torch.aten.gelu %2955, %str_3190 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_3191 = torch.constant.int 64
    %int5120_3192 = torch.constant.int 5120
    %2957 = torch.prim.ListConstruct %int64_3191, %int5120_3192 : (!torch.int, !torch.int) -> !torch.list<int>
    %2958 = torch.aten.view %2956, %2957 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.weight : tensor<1280x5120xf16>
    %2959 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3193 = torch.constant.int 0
    %int1_3194 = torch.constant.int 1
    %2960 = torch.aten.transpose.int %2959, %int0_3193, %int1_3194 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.bias : tensor<1280xf16>
    %2961 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.18.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3195 = torch.constant.int 6
    %2962 = torch.prims.convert_element_type %2961, %int6_3195 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3196 = torch.constant.int 6
    %2963 = torch.prims.convert_element_type %2958, %int6_3196 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_3197 = torch.constant.int 6
    %2964 = torch.prims.convert_element_type %2960, %int6_3197 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2965 = torch.aten.mm %2963, %2964 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3198 = torch.constant.int 1
    %2966 = torch.aten.mul.Scalar %2965, %int1_3198 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3199 = torch.constant.int 1
    %2967 = torch.aten.mul.Scalar %2962, %int1_3199 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3200 = torch.constant.int 1
    %2968 = torch.aten.add.Tensor %2966, %2967, %int1_3200 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3201 = torch.constant.int 5
    %2969 = torch.prims.convert_element_type %2968, %int5_3201 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3202 = torch.constant.int 1
    %int64_3203 = torch.constant.int 64
    %int1280_3204 = torch.constant.int 1280
    %2970 = torch.prim.ListConstruct %int1_3202, %int64_3203, %int1280_3204 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2971 = torch.aten.view %2969, %2970 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3205 = torch.constant.int 1
    %2972 = torch.aten.add.Tensor %2927, %2971, %int1_3205 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3206 = torch.constant.int 6
    %2973 = torch.prims.convert_element_type %2972, %int6_3206 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3207 = torch.constant.int 2
    %2974 = torch.prim.ListConstruct %int2_3207 : (!torch.int) -> !torch.list<int>
    %int0_3208 = torch.constant.int 0
    %true_3209 = torch.constant.bool true
    %result0_3210, %result1_3211 = torch.aten.var_mean.correction %2973, %2974, %int0_3208, %true_3209 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3212 = torch.constant.float 1.000000e-05
    %int1_3213 = torch.constant.int 1
    %2975 = torch.aten.add.Scalar %result0_3210, %float1.000000e-05_3212, %int1_3213 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %2976 = torch.aten.rsqrt %2975 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3214 = torch.constant.int 1
    %2977 = torch.aten.sub.Tensor %2972, %result1_3211, %int1_3214 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %2978 = torch.aten.mul.Tensor %2977, %2976 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.weight : tensor<1280xf16>
    %2979 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2980 = torch.aten.mul.Tensor %2978, %2979 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.bias : tensor<1280xf16>
    %2981 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3215 = torch.constant.int 1
    %2982 = torch.aten.add.Tensor %2980, %2981, %int1_3215 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3216 = torch.constant.int 5
    %2983 = torch.prims.convert_element_type %2982, %int5_3216 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3217 = torch.constant.int 5
    %2984 = torch.prims.convert_element_type %result1_3211, %int5_3217 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3218 = torch.constant.int 5
    %2985 = torch.prims.convert_element_type %2976, %int5_3218 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3219 = torch.constant.int 64
    %int1280_3220 = torch.constant.int 1280
    %2986 = torch.prim.ListConstruct %int64_3219, %int1280_3220 : (!torch.int, !torch.int) -> !torch.list<int>
    %2987 = torch.aten.view %2983, %2986 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %2988 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3221 = torch.constant.int 0
    %int1_3222 = torch.constant.int 1
    %2989 = torch.aten.transpose.int %2988, %int0_3221, %int1_3222 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.bias : tensor<1280xf16>
    %2990 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3223 = torch.constant.int 6
    %2991 = torch.prims.convert_element_type %2990, %int6_3223 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3224 = torch.constant.int 6
    %2992 = torch.prims.convert_element_type %2987, %int6_3224 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3225 = torch.constant.int 6
    %2993 = torch.prims.convert_element_type %2989, %int6_3225 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2994 = torch.aten.mm %2992, %2993 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3226 = torch.constant.int 1
    %2995 = torch.aten.mul.Scalar %2994, %int1_3226 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3227 = torch.constant.int 1
    %2996 = torch.aten.mul.Scalar %2991, %int1_3227 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3228 = torch.constant.int 1
    %2997 = torch.aten.add.Tensor %2995, %2996, %int1_3228 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3229 = torch.constant.int 5
    %2998 = torch.prims.convert_element_type %2997, %int5_3229 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3230 = torch.constant.int 1
    %int64_3231 = torch.constant.int 64
    %int1280_3232 = torch.constant.int 1280
    %2999 = torch.prim.ListConstruct %int1_3230, %int64_3231, %int1280_3232 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3000 = torch.aten.view %2998, %2999 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_3233 = torch.constant.float 1.250000e-01
    %3001 = torch.aten.mul.Scalar %3000, %float1.250000e-01_3233 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_3234 = torch.constant.int 64
    %int1280_3235 = torch.constant.int 1280
    %3002 = torch.prim.ListConstruct %int64_3234, %int1280_3235 : (!torch.int, !torch.int) -> !torch.list<int>
    %3003 = torch.aten.view %2983, %3002 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3004 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3236 = torch.constant.int 0
    %int1_3237 = torch.constant.int 1
    %3005 = torch.aten.transpose.int %3004, %int0_3236, %int1_3237 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.bias : tensor<1280xf16>
    %3006 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3238 = torch.constant.int 6
    %3007 = torch.prims.convert_element_type %3006, %int6_3238 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3239 = torch.constant.int 6
    %3008 = torch.prims.convert_element_type %3003, %int6_3239 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3240 = torch.constant.int 6
    %3009 = torch.prims.convert_element_type %3005, %int6_3240 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3010 = torch.aten.mm %3008, %3009 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3241 = torch.constant.int 1
    %3011 = torch.aten.mul.Scalar %3010, %int1_3241 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3242 = torch.constant.int 1
    %3012 = torch.aten.mul.Scalar %3007, %int1_3242 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3243 = torch.constant.int 1
    %3013 = torch.aten.add.Tensor %3011, %3012, %int1_3243 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3244 = torch.constant.int 5
    %3014 = torch.prims.convert_element_type %3013, %int5_3244 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3245 = torch.constant.int 1
    %int64_3246 = torch.constant.int 64
    %int1280_3247 = torch.constant.int 1280
    %3015 = torch.prim.ListConstruct %int1_3245, %int64_3246, %int1280_3247 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3016 = torch.aten.view %3014, %3015 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3248 = torch.constant.int 1
    %int-1_3249 = torch.constant.int -1
    %int20_3250 = torch.constant.int 20
    %int64_3251 = torch.constant.int 64
    %3017 = torch.prim.ListConstruct %int1_3248, %int-1_3249, %int20_3250, %int64_3251 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3018 = torch.aten.view %3016, %3017 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3252 = torch.constant.int 1
    %int2_3253 = torch.constant.int 2
    %3019 = torch.aten.transpose.int %3018, %int1_3252, %int2_3253 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3254 = torch.constant.int 0
    %3020 = torch.aten.clone %3019, %int0_3254 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_3255 = torch.constant.int 64
    %int1280_3256 = torch.constant.int 1280
    %3021 = torch.prim.ListConstruct %int64_3255, %int1280_3256 : (!torch.int, !torch.int) -> !torch.list<int>
    %3022 = torch.aten.view %2983, %3021 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3023 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3257 = torch.constant.int 0
    %int1_3258 = torch.constant.int 1
    %3024 = torch.aten.transpose.int %3023, %int0_3257, %int1_3258 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.bias : tensor<1280xf16>
    %3025 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3259 = torch.constant.int 6
    %3026 = torch.prims.convert_element_type %3025, %int6_3259 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3260 = torch.constant.int 6
    %3027 = torch.prims.convert_element_type %3022, %int6_3260 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3261 = torch.constant.int 6
    %3028 = torch.prims.convert_element_type %3024, %int6_3261 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3029 = torch.aten.mm %3027, %3028 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3262 = torch.constant.int 1
    %3030 = torch.aten.mul.Scalar %3029, %int1_3262 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3263 = torch.constant.int 1
    %3031 = torch.aten.mul.Scalar %3026, %int1_3263 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3264 = torch.constant.int 1
    %3032 = torch.aten.add.Tensor %3030, %3031, %int1_3264 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3265 = torch.constant.int 5
    %3033 = torch.prims.convert_element_type %3032, %int5_3265 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3266 = torch.constant.int 1
    %int64_3267 = torch.constant.int 64
    %int1280_3268 = torch.constant.int 1280
    %3034 = torch.prim.ListConstruct %int1_3266, %int64_3267, %int1280_3268 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3035 = torch.aten.view %3033, %3034 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3269 = torch.constant.int 1
    %int-1_3270 = torch.constant.int -1
    %int20_3271 = torch.constant.int 20
    %int64_3272 = torch.constant.int 64
    %3036 = torch.prim.ListConstruct %int1_3269, %int-1_3270, %int20_3271, %int64_3272 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3037 = torch.aten.view %3035, %3036 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3273 = torch.constant.int 1
    %int2_3274 = torch.constant.int 2
    %3038 = torch.aten.transpose.int %3037, %int1_3273, %int2_3274 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3275 = torch.constant.int 0
    %3039 = torch.aten.clone %3038, %int0_3275 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3276 = torch.constant.int 1
    %int64_3277 = torch.constant.int 64
    %int20_3278 = torch.constant.int 20
    %int64_3279 = torch.constant.int 64
    %3040 = torch.prim.ListConstruct %int1_3276, %int64_3277, %int20_3278, %int64_3279 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3041 = torch.aten.view %3001, %3040 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3280 = torch.constant.int 1
    %int2_3281 = torch.constant.int 2
    %3042 = torch.aten.transpose.int %3041, %int1_3280, %int2_3281 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3282 = torch.constant.int 0
    %3043 = torch.aten.clone %3042, %int0_3282 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3283 = torch.constant.int 20
    %int-1_3284 = torch.constant.int -1
    %int64_3285 = torch.constant.int 64
    %3044 = torch.prim.ListConstruct %int20_3283, %int-1_3284, %int64_3285 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3045 = torch.aten.view %3043, %3044 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3286 = torch.constant.int 20
    %int-1_3287 = torch.constant.int -1
    %int64_3288 = torch.constant.int 64
    %3046 = torch.prim.ListConstruct %int20_3286, %int-1_3287, %int64_3288 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3047 = torch.aten.view %3020, %3046 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3289 = torch.constant.int 20
    %int-1_3290 = torch.constant.int -1
    %int64_3291 = torch.constant.int 64
    %3048 = torch.prim.ListConstruct %int20_3289, %int-1_3290, %int64_3291 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3049 = torch.aten.view %3039, %3048 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_3292 = torch.constant.int 1
    %int2_3293 = torch.constant.int 2
    %3050 = torch.aten.transpose.int %3047, %int1_3292, %int2_3293 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3051 = torch.aten.bmm %3045, %3050 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3294 = torch.constant.int 1
    %int20_3295 = torch.constant.int 20
    %int64_3296 = torch.constant.int 64
    %int64_3297 = torch.constant.int 64
    %3052 = torch.prim.ListConstruct %int1_3294, %int20_3295, %int64_3296, %int64_3297 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3053 = torch.aten.view %3051, %3052 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3298 = torch.constant.int 1
    %3054 = torch.aten.add.Tensor %3053, %27, %int1_3298 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3299 = torch.constant.int 20
    %int64_3300 = torch.constant.int 64
    %int64_3301 = torch.constant.int 64
    %3055 = torch.prim.ListConstruct %int20_3299, %int64_3300, %int64_3301 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3056 = torch.aten.view %3054, %3055 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_3302 = torch.constant.int -1
    %false_3303 = torch.constant.bool false
    %3057 = torch.aten._softmax %3056, %int-1_3302, %false_3303 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3058 = torch.aten.detach %3057 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_3304 = torch.constant.none
    %3059 = torch.aten.clone %3057, %none_3304 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3060 = torch.aten.bmm %3059, %3049 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3305 = torch.constant.int 1
    %int20_3306 = torch.constant.int 20
    %int64_3307 = torch.constant.int 64
    %int64_3308 = torch.constant.int 64
    %3061 = torch.prim.ListConstruct %int1_3305, %int20_3306, %int64_3307, %int64_3308 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3062 = torch.aten.view %3060, %3061 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3309 = torch.constant.int 1
    %int2_3310 = torch.constant.int 2
    %3063 = torch.aten.transpose.int %3062, %int1_3309, %int2_3310 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_3311 = torch.constant.int 0
    %3064 = torch.aten.clone %3063, %int0_3311 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3312 = torch.constant.int 1
    %int64_3313 = torch.constant.int 64
    %int1280_3314 = torch.constant.int 1280
    %3065 = torch.prim.ListConstruct %int1_3312, %int64_3313, %int1280_3314 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3066 = torch.aten._unsafe_view %3064, %3065 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_3315 = torch.constant.int 64
    %int1280_3316 = torch.constant.int 1280
    %3067 = torch.prim.ListConstruct %int64_3315, %int1280_3316 : (!torch.int, !torch.int) -> !torch.list<int>
    %3068 = torch.aten.view %3066, %3067 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3069 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3317 = torch.constant.int 0
    %int1_3318 = torch.constant.int 1
    %3070 = torch.aten.transpose.int %3069, %int0_3317, %int1_3318 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.bias : tensor<1280xf16>
    %3071 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3319 = torch.constant.int 6
    %3072 = torch.prims.convert_element_type %3071, %int6_3319 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3320 = torch.constant.int 6
    %3073 = torch.prims.convert_element_type %3068, %int6_3320 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3321 = torch.constant.int 6
    %3074 = torch.prims.convert_element_type %3070, %int6_3321 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3075 = torch.aten.mm %3073, %3074 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3322 = torch.constant.int 1
    %3076 = torch.aten.mul.Scalar %3075, %int1_3322 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3323 = torch.constant.int 1
    %3077 = torch.aten.mul.Scalar %3072, %int1_3323 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3324 = torch.constant.int 1
    %3078 = torch.aten.add.Tensor %3076, %3077, %int1_3324 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3325 = torch.constant.int 5
    %3079 = torch.prims.convert_element_type %3078, %int5_3325 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3326 = torch.constant.int 1
    %int64_3327 = torch.constant.int 64
    %int1280_3328 = torch.constant.int 1280
    %3080 = torch.prim.ListConstruct %int1_3326, %int64_3327, %int1280_3328 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3081 = torch.aten.view %3079, %3080 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3329 = torch.constant.int 1
    %3082 = torch.aten.add.Tensor %2972, %3081, %int1_3329 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3330 = torch.constant.int 6
    %3083 = torch.prims.convert_element_type %3082, %int6_3330 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3331 = torch.constant.int 2
    %3084 = torch.prim.ListConstruct %int2_3331 : (!torch.int) -> !torch.list<int>
    %int0_3332 = torch.constant.int 0
    %true_3333 = torch.constant.bool true
    %result0_3334, %result1_3335 = torch.aten.var_mean.correction %3083, %3084, %int0_3332, %true_3333 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3336 = torch.constant.float 1.000000e-05
    %int1_3337 = torch.constant.int 1
    %3085 = torch.aten.add.Scalar %result0_3334, %float1.000000e-05_3336, %int1_3337 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3086 = torch.aten.rsqrt %3085 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3338 = torch.constant.int 1
    %3087 = torch.aten.sub.Tensor %3082, %result1_3335, %int1_3338 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3088 = torch.aten.mul.Tensor %3087, %3086 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.weight : tensor<1280xf16>
    %3089 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3090 = torch.aten.mul.Tensor %3088, %3089 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.bias : tensor<1280xf16>
    %3091 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3339 = torch.constant.int 1
    %3092 = torch.aten.add.Tensor %3090, %3091, %int1_3339 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3340 = torch.constant.int 5
    %3093 = torch.prims.convert_element_type %3092, %int5_3340 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3341 = torch.constant.int 5
    %3094 = torch.prims.convert_element_type %result1_3335, %int5_3341 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3342 = torch.constant.int 5
    %3095 = torch.prims.convert_element_type %3086, %int5_3342 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3343 = torch.constant.int 64
    %int1280_3344 = torch.constant.int 1280
    %3096 = torch.prim.ListConstruct %int64_3343, %int1280_3344 : (!torch.int, !torch.int) -> !torch.list<int>
    %3097 = torch.aten.view %3093, %3096 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.weight : tensor<5120x1280xf16>
    %3098 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_3345 = torch.constant.int 0
    %int1_3346 = torch.constant.int 1
    %3099 = torch.aten.transpose.int %3098, %int0_3345, %int1_3346 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.bias : tensor<5120xf16>
    %3100 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_3347 = torch.constant.int 6
    %3101 = torch.prims.convert_element_type %3100, %int6_3347 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_3348 = torch.constant.int 6
    %3102 = torch.prims.convert_element_type %3097, %int6_3348 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3349 = torch.constant.int 6
    %3103 = torch.prims.convert_element_type %3099, %int6_3349 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %3104 = torch.aten.mm %3102, %3103 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_3350 = torch.constant.int 1
    %3105 = torch.aten.mul.Scalar %3104, %int1_3350 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_3351 = torch.constant.int 1
    %3106 = torch.aten.mul.Scalar %3101, %int1_3351 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_3352 = torch.constant.int 1
    %3107 = torch.aten.add.Tensor %3105, %3106, %int1_3352 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_3353 = torch.constant.int 5
    %3108 = torch.prims.convert_element_type %3107, %int5_3353 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_3354 = torch.constant.int 1
    %int64_3355 = torch.constant.int 64
    %int5120_3356 = torch.constant.int 5120
    %3109 = torch.prim.ListConstruct %int1_3354, %int64_3355, %int5120_3356 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3110 = torch.aten.view %3108, %3109 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_3357 = torch.constant.str "none"
    %3111 = torch.aten.gelu %3110, %str_3357 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_3358 = torch.constant.int 64
    %int5120_3359 = torch.constant.int 5120
    %3112 = torch.prim.ListConstruct %int64_3358, %int5120_3359 : (!torch.int, !torch.int) -> !torch.list<int>
    %3113 = torch.aten.view %3111, %3112 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.weight : tensor<1280x5120xf16>
    %3114 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3360 = torch.constant.int 0
    %int1_3361 = torch.constant.int 1
    %3115 = torch.aten.transpose.int %3114, %int0_3360, %int1_3361 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.bias : tensor<1280xf16>
    %3116 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.19.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3362 = torch.constant.int 6
    %3117 = torch.prims.convert_element_type %3116, %int6_3362 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3363 = torch.constant.int 6
    %3118 = torch.prims.convert_element_type %3113, %int6_3363 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_3364 = torch.constant.int 6
    %3119 = torch.prims.convert_element_type %3115, %int6_3364 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3120 = torch.aten.mm %3118, %3119 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3365 = torch.constant.int 1
    %3121 = torch.aten.mul.Scalar %3120, %int1_3365 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3366 = torch.constant.int 1
    %3122 = torch.aten.mul.Scalar %3117, %int1_3366 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3367 = torch.constant.int 1
    %3123 = torch.aten.add.Tensor %3121, %3122, %int1_3367 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3368 = torch.constant.int 5
    %3124 = torch.prims.convert_element_type %3123, %int5_3368 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3369 = torch.constant.int 1
    %int64_3370 = torch.constant.int 64
    %int1280_3371 = torch.constant.int 1280
    %3125 = torch.prim.ListConstruct %int1_3369, %int64_3370, %int1280_3371 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3126 = torch.aten.view %3124, %3125 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3372 = torch.constant.int 1
    %3127 = torch.aten.add.Tensor %3082, %3126, %int1_3372 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3373 = torch.constant.int 6
    %3128 = torch.prims.convert_element_type %3127, %int6_3373 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3374 = torch.constant.int 2
    %3129 = torch.prim.ListConstruct %int2_3374 : (!torch.int) -> !torch.list<int>
    %int0_3375 = torch.constant.int 0
    %true_3376 = torch.constant.bool true
    %result0_3377, %result1_3378 = torch.aten.var_mean.correction %3128, %3129, %int0_3375, %true_3376 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3379 = torch.constant.float 1.000000e-05
    %int1_3380 = torch.constant.int 1
    %3130 = torch.aten.add.Scalar %result0_3377, %float1.000000e-05_3379, %int1_3380 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3131 = torch.aten.rsqrt %3130 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3381 = torch.constant.int 1
    %3132 = torch.aten.sub.Tensor %3127, %result1_3378, %int1_3381 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3133 = torch.aten.mul.Tensor %3132, %3131 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.weight : tensor<1280xf16>
    %3134 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3135 = torch.aten.mul.Tensor %3133, %3134 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.bias : tensor<1280xf16>
    %3136 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3382 = torch.constant.int 1
    %3137 = torch.aten.add.Tensor %3135, %3136, %int1_3382 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3383 = torch.constant.int 5
    %3138 = torch.prims.convert_element_type %3137, %int5_3383 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3384 = torch.constant.int 5
    %3139 = torch.prims.convert_element_type %result1_3378, %int5_3384 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3385 = torch.constant.int 5
    %3140 = torch.prims.convert_element_type %3131, %int5_3385 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3386 = torch.constant.int 64
    %int1280_3387 = torch.constant.int 1280
    %3141 = torch.prim.ListConstruct %int64_3386, %int1280_3387 : (!torch.int, !torch.int) -> !torch.list<int>
    %3142 = torch.aten.view %3138, %3141 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %3143 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3388 = torch.constant.int 0
    %int1_3389 = torch.constant.int 1
    %3144 = torch.aten.transpose.int %3143, %int0_3388, %int1_3389 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.bias : tensor<1280xf16>
    %3145 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3390 = torch.constant.int 6
    %3146 = torch.prims.convert_element_type %3145, %int6_3390 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3391 = torch.constant.int 6
    %3147 = torch.prims.convert_element_type %3142, %int6_3391 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3392 = torch.constant.int 6
    %3148 = torch.prims.convert_element_type %3144, %int6_3392 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3149 = torch.aten.mm %3147, %3148 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3393 = torch.constant.int 1
    %3150 = torch.aten.mul.Scalar %3149, %int1_3393 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3394 = torch.constant.int 1
    %3151 = torch.aten.mul.Scalar %3146, %int1_3394 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3395 = torch.constant.int 1
    %3152 = torch.aten.add.Tensor %3150, %3151, %int1_3395 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3396 = torch.constant.int 5
    %3153 = torch.prims.convert_element_type %3152, %int5_3396 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3397 = torch.constant.int 1
    %int64_3398 = torch.constant.int 64
    %int1280_3399 = torch.constant.int 1280
    %3154 = torch.prim.ListConstruct %int1_3397, %int64_3398, %int1280_3399 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3155 = torch.aten.view %3153, %3154 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_3400 = torch.constant.float 1.250000e-01
    %3156 = torch.aten.mul.Scalar %3155, %float1.250000e-01_3400 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_3401 = torch.constant.int 64
    %int1280_3402 = torch.constant.int 1280
    %3157 = torch.prim.ListConstruct %int64_3401, %int1280_3402 : (!torch.int, !torch.int) -> !torch.list<int>
    %3158 = torch.aten.view %3138, %3157 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3159 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3403 = torch.constant.int 0
    %int1_3404 = torch.constant.int 1
    %3160 = torch.aten.transpose.int %3159, %int0_3403, %int1_3404 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.bias : tensor<1280xf16>
    %3161 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3405 = torch.constant.int 6
    %3162 = torch.prims.convert_element_type %3161, %int6_3405 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3406 = torch.constant.int 6
    %3163 = torch.prims.convert_element_type %3158, %int6_3406 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3407 = torch.constant.int 6
    %3164 = torch.prims.convert_element_type %3160, %int6_3407 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3165 = torch.aten.mm %3163, %3164 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3408 = torch.constant.int 1
    %3166 = torch.aten.mul.Scalar %3165, %int1_3408 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3409 = torch.constant.int 1
    %3167 = torch.aten.mul.Scalar %3162, %int1_3409 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3410 = torch.constant.int 1
    %3168 = torch.aten.add.Tensor %3166, %3167, %int1_3410 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3411 = torch.constant.int 5
    %3169 = torch.prims.convert_element_type %3168, %int5_3411 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3412 = torch.constant.int 1
    %int64_3413 = torch.constant.int 64
    %int1280_3414 = torch.constant.int 1280
    %3170 = torch.prim.ListConstruct %int1_3412, %int64_3413, %int1280_3414 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3171 = torch.aten.view %3169, %3170 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3415 = torch.constant.int 1
    %int-1_3416 = torch.constant.int -1
    %int20_3417 = torch.constant.int 20
    %int64_3418 = torch.constant.int 64
    %3172 = torch.prim.ListConstruct %int1_3415, %int-1_3416, %int20_3417, %int64_3418 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3173 = torch.aten.view %3171, %3172 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3419 = torch.constant.int 1
    %int2_3420 = torch.constant.int 2
    %3174 = torch.aten.transpose.int %3173, %int1_3419, %int2_3420 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3421 = torch.constant.int 0
    %3175 = torch.aten.clone %3174, %int0_3421 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_3422 = torch.constant.int 64
    %int1280_3423 = torch.constant.int 1280
    %3176 = torch.prim.ListConstruct %int64_3422, %int1280_3423 : (!torch.int, !torch.int) -> !torch.list<int>
    %3177 = torch.aten.view %3138, %3176 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3178 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3424 = torch.constant.int 0
    %int1_3425 = torch.constant.int 1
    %3179 = torch.aten.transpose.int %3178, %int0_3424, %int1_3425 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.bias : tensor<1280xf16>
    %3180 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3426 = torch.constant.int 6
    %3181 = torch.prims.convert_element_type %3180, %int6_3426 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3427 = torch.constant.int 6
    %3182 = torch.prims.convert_element_type %3177, %int6_3427 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3428 = torch.constant.int 6
    %3183 = torch.prims.convert_element_type %3179, %int6_3428 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3184 = torch.aten.mm %3182, %3183 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3429 = torch.constant.int 1
    %3185 = torch.aten.mul.Scalar %3184, %int1_3429 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3430 = torch.constant.int 1
    %3186 = torch.aten.mul.Scalar %3181, %int1_3430 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3431 = torch.constant.int 1
    %3187 = torch.aten.add.Tensor %3185, %3186, %int1_3431 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3432 = torch.constant.int 5
    %3188 = torch.prims.convert_element_type %3187, %int5_3432 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3433 = torch.constant.int 1
    %int64_3434 = torch.constant.int 64
    %int1280_3435 = torch.constant.int 1280
    %3189 = torch.prim.ListConstruct %int1_3433, %int64_3434, %int1280_3435 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3190 = torch.aten.view %3188, %3189 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3436 = torch.constant.int 1
    %int-1_3437 = torch.constant.int -1
    %int20_3438 = torch.constant.int 20
    %int64_3439 = torch.constant.int 64
    %3191 = torch.prim.ListConstruct %int1_3436, %int-1_3437, %int20_3438, %int64_3439 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3192 = torch.aten.view %3190, %3191 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3440 = torch.constant.int 1
    %int2_3441 = torch.constant.int 2
    %3193 = torch.aten.transpose.int %3192, %int1_3440, %int2_3441 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3442 = torch.constant.int 0
    %3194 = torch.aten.clone %3193, %int0_3442 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3443 = torch.constant.int 1
    %int64_3444 = torch.constant.int 64
    %int20_3445 = torch.constant.int 20
    %int64_3446 = torch.constant.int 64
    %3195 = torch.prim.ListConstruct %int1_3443, %int64_3444, %int20_3445, %int64_3446 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3196 = torch.aten.view %3156, %3195 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3447 = torch.constant.int 1
    %int2_3448 = torch.constant.int 2
    %3197 = torch.aten.transpose.int %3196, %int1_3447, %int2_3448 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3449 = torch.constant.int 0
    %3198 = torch.aten.clone %3197, %int0_3449 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3450 = torch.constant.int 20
    %int-1_3451 = torch.constant.int -1
    %int64_3452 = torch.constant.int 64
    %3199 = torch.prim.ListConstruct %int20_3450, %int-1_3451, %int64_3452 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3200 = torch.aten.view %3198, %3199 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3453 = torch.constant.int 20
    %int-1_3454 = torch.constant.int -1
    %int64_3455 = torch.constant.int 64
    %3201 = torch.prim.ListConstruct %int20_3453, %int-1_3454, %int64_3455 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3202 = torch.aten.view %3175, %3201 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3456 = torch.constant.int 20
    %int-1_3457 = torch.constant.int -1
    %int64_3458 = torch.constant.int 64
    %3203 = torch.prim.ListConstruct %int20_3456, %int-1_3457, %int64_3458 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3204 = torch.aten.view %3194, %3203 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_3459 = torch.constant.int 1
    %int2_3460 = torch.constant.int 2
    %3205 = torch.aten.transpose.int %3202, %int1_3459, %int2_3460 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3206 = torch.aten.bmm %3200, %3205 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3461 = torch.constant.int 1
    %int20_3462 = torch.constant.int 20
    %int64_3463 = torch.constant.int 64
    %int64_3464 = torch.constant.int 64
    %3207 = torch.prim.ListConstruct %int1_3461, %int20_3462, %int64_3463, %int64_3464 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3208 = torch.aten.view %3206, %3207 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3465 = torch.constant.int 1
    %3209 = torch.aten.add.Tensor %3208, %27, %int1_3465 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3466 = torch.constant.int 20
    %int64_3467 = torch.constant.int 64
    %int64_3468 = torch.constant.int 64
    %3210 = torch.prim.ListConstruct %int20_3466, %int64_3467, %int64_3468 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3211 = torch.aten.view %3209, %3210 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_3469 = torch.constant.int -1
    %false_3470 = torch.constant.bool false
    %3212 = torch.aten._softmax %3211, %int-1_3469, %false_3470 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3213 = torch.aten.detach %3212 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_3471 = torch.constant.none
    %3214 = torch.aten.clone %3212, %none_3471 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3215 = torch.aten.bmm %3214, %3204 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3472 = torch.constant.int 1
    %int20_3473 = torch.constant.int 20
    %int64_3474 = torch.constant.int 64
    %int64_3475 = torch.constant.int 64
    %3216 = torch.prim.ListConstruct %int1_3472, %int20_3473, %int64_3474, %int64_3475 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3217 = torch.aten.view %3215, %3216 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3476 = torch.constant.int 1
    %int2_3477 = torch.constant.int 2
    %3218 = torch.aten.transpose.int %3217, %int1_3476, %int2_3477 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_3478 = torch.constant.int 0
    %3219 = torch.aten.clone %3218, %int0_3478 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3479 = torch.constant.int 1
    %int64_3480 = torch.constant.int 64
    %int1280_3481 = torch.constant.int 1280
    %3220 = torch.prim.ListConstruct %int1_3479, %int64_3480, %int1280_3481 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3221 = torch.aten._unsafe_view %3219, %3220 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_3482 = torch.constant.int 64
    %int1280_3483 = torch.constant.int 1280
    %3222 = torch.prim.ListConstruct %int64_3482, %int1280_3483 : (!torch.int, !torch.int) -> !torch.list<int>
    %3223 = torch.aten.view %3221, %3222 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3224 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3484 = torch.constant.int 0
    %int1_3485 = torch.constant.int 1
    %3225 = torch.aten.transpose.int %3224, %int0_3484, %int1_3485 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.bias : tensor<1280xf16>
    %3226 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3486 = torch.constant.int 6
    %3227 = torch.prims.convert_element_type %3226, %int6_3486 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3487 = torch.constant.int 6
    %3228 = torch.prims.convert_element_type %3223, %int6_3487 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3488 = torch.constant.int 6
    %3229 = torch.prims.convert_element_type %3225, %int6_3488 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3230 = torch.aten.mm %3228, %3229 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3489 = torch.constant.int 1
    %3231 = torch.aten.mul.Scalar %3230, %int1_3489 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3490 = torch.constant.int 1
    %3232 = torch.aten.mul.Scalar %3227, %int1_3490 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3491 = torch.constant.int 1
    %3233 = torch.aten.add.Tensor %3231, %3232, %int1_3491 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3492 = torch.constant.int 5
    %3234 = torch.prims.convert_element_type %3233, %int5_3492 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3493 = torch.constant.int 1
    %int64_3494 = torch.constant.int 64
    %int1280_3495 = torch.constant.int 1280
    %3235 = torch.prim.ListConstruct %int1_3493, %int64_3494, %int1280_3495 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3236 = torch.aten.view %3234, %3235 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3496 = torch.constant.int 1
    %3237 = torch.aten.add.Tensor %3127, %3236, %int1_3496 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3497 = torch.constant.int 6
    %3238 = torch.prims.convert_element_type %3237, %int6_3497 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3498 = torch.constant.int 2
    %3239 = torch.prim.ListConstruct %int2_3498 : (!torch.int) -> !torch.list<int>
    %int0_3499 = torch.constant.int 0
    %true_3500 = torch.constant.bool true
    %result0_3501, %result1_3502 = torch.aten.var_mean.correction %3238, %3239, %int0_3499, %true_3500 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3503 = torch.constant.float 1.000000e-05
    %int1_3504 = torch.constant.int 1
    %3240 = torch.aten.add.Scalar %result0_3501, %float1.000000e-05_3503, %int1_3504 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3241 = torch.aten.rsqrt %3240 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3505 = torch.constant.int 1
    %3242 = torch.aten.sub.Tensor %3237, %result1_3502, %int1_3505 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3243 = torch.aten.mul.Tensor %3242, %3241 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.weight : tensor<1280xf16>
    %3244 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3245 = torch.aten.mul.Tensor %3243, %3244 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.bias : tensor<1280xf16>
    %3246 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3506 = torch.constant.int 1
    %3247 = torch.aten.add.Tensor %3245, %3246, %int1_3506 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3507 = torch.constant.int 5
    %3248 = torch.prims.convert_element_type %3247, %int5_3507 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3508 = torch.constant.int 5
    %3249 = torch.prims.convert_element_type %result1_3502, %int5_3508 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3509 = torch.constant.int 5
    %3250 = torch.prims.convert_element_type %3241, %int5_3509 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3510 = torch.constant.int 64
    %int1280_3511 = torch.constant.int 1280
    %3251 = torch.prim.ListConstruct %int64_3510, %int1280_3511 : (!torch.int, !torch.int) -> !torch.list<int>
    %3252 = torch.aten.view %3248, %3251 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.weight : tensor<5120x1280xf16>
    %3253 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_3512 = torch.constant.int 0
    %int1_3513 = torch.constant.int 1
    %3254 = torch.aten.transpose.int %3253, %int0_3512, %int1_3513 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.bias : tensor<5120xf16>
    %3255 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_3514 = torch.constant.int 6
    %3256 = torch.prims.convert_element_type %3255, %int6_3514 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_3515 = torch.constant.int 6
    %3257 = torch.prims.convert_element_type %3252, %int6_3515 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3516 = torch.constant.int 6
    %3258 = torch.prims.convert_element_type %3254, %int6_3516 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %3259 = torch.aten.mm %3257, %3258 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_3517 = torch.constant.int 1
    %3260 = torch.aten.mul.Scalar %3259, %int1_3517 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_3518 = torch.constant.int 1
    %3261 = torch.aten.mul.Scalar %3256, %int1_3518 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_3519 = torch.constant.int 1
    %3262 = torch.aten.add.Tensor %3260, %3261, %int1_3519 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_3520 = torch.constant.int 5
    %3263 = torch.prims.convert_element_type %3262, %int5_3520 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_3521 = torch.constant.int 1
    %int64_3522 = torch.constant.int 64
    %int5120_3523 = torch.constant.int 5120
    %3264 = torch.prim.ListConstruct %int1_3521, %int64_3522, %int5120_3523 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3265 = torch.aten.view %3263, %3264 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_3524 = torch.constant.str "none"
    %3266 = torch.aten.gelu %3265, %str_3524 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_3525 = torch.constant.int 64
    %int5120_3526 = torch.constant.int 5120
    %3267 = torch.prim.ListConstruct %int64_3525, %int5120_3526 : (!torch.int, !torch.int) -> !torch.list<int>
    %3268 = torch.aten.view %3266, %3267 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.weight : tensor<1280x5120xf16>
    %3269 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3527 = torch.constant.int 0
    %int1_3528 = torch.constant.int 1
    %3270 = torch.aten.transpose.int %3269, %int0_3527, %int1_3528 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.bias : tensor<1280xf16>
    %3271 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.20.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3529 = torch.constant.int 6
    %3272 = torch.prims.convert_element_type %3271, %int6_3529 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3530 = torch.constant.int 6
    %3273 = torch.prims.convert_element_type %3268, %int6_3530 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_3531 = torch.constant.int 6
    %3274 = torch.prims.convert_element_type %3270, %int6_3531 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3275 = torch.aten.mm %3273, %3274 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3532 = torch.constant.int 1
    %3276 = torch.aten.mul.Scalar %3275, %int1_3532 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3533 = torch.constant.int 1
    %3277 = torch.aten.mul.Scalar %3272, %int1_3533 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3534 = torch.constant.int 1
    %3278 = torch.aten.add.Tensor %3276, %3277, %int1_3534 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3535 = torch.constant.int 5
    %3279 = torch.prims.convert_element_type %3278, %int5_3535 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3536 = torch.constant.int 1
    %int64_3537 = torch.constant.int 64
    %int1280_3538 = torch.constant.int 1280
    %3280 = torch.prim.ListConstruct %int1_3536, %int64_3537, %int1280_3538 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3281 = torch.aten.view %3279, %3280 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3539 = torch.constant.int 1
    %3282 = torch.aten.add.Tensor %3237, %3281, %int1_3539 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3540 = torch.constant.int 6
    %3283 = torch.prims.convert_element_type %3282, %int6_3540 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3541 = torch.constant.int 2
    %3284 = torch.prim.ListConstruct %int2_3541 : (!torch.int) -> !torch.list<int>
    %int0_3542 = torch.constant.int 0
    %true_3543 = torch.constant.bool true
    %result0_3544, %result1_3545 = torch.aten.var_mean.correction %3283, %3284, %int0_3542, %true_3543 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3546 = torch.constant.float 1.000000e-05
    %int1_3547 = torch.constant.int 1
    %3285 = torch.aten.add.Scalar %result0_3544, %float1.000000e-05_3546, %int1_3547 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3286 = torch.aten.rsqrt %3285 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3548 = torch.constant.int 1
    %3287 = torch.aten.sub.Tensor %3282, %result1_3545, %int1_3548 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3288 = torch.aten.mul.Tensor %3287, %3286 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.weight : tensor<1280xf16>
    %3289 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3290 = torch.aten.mul.Tensor %3288, %3289 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.bias : tensor<1280xf16>
    %3291 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3549 = torch.constant.int 1
    %3292 = torch.aten.add.Tensor %3290, %3291, %int1_3549 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3550 = torch.constant.int 5
    %3293 = torch.prims.convert_element_type %3292, %int5_3550 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3551 = torch.constant.int 5
    %3294 = torch.prims.convert_element_type %result1_3545, %int5_3551 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3552 = torch.constant.int 5
    %3295 = torch.prims.convert_element_type %3286, %int5_3552 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3553 = torch.constant.int 64
    %int1280_3554 = torch.constant.int 1280
    %3296 = torch.prim.ListConstruct %int64_3553, %int1280_3554 : (!torch.int, !torch.int) -> !torch.list<int>
    %3297 = torch.aten.view %3293, %3296 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %3298 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3555 = torch.constant.int 0
    %int1_3556 = torch.constant.int 1
    %3299 = torch.aten.transpose.int %3298, %int0_3555, %int1_3556 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.bias : tensor<1280xf16>
    %3300 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3557 = torch.constant.int 6
    %3301 = torch.prims.convert_element_type %3300, %int6_3557 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3558 = torch.constant.int 6
    %3302 = torch.prims.convert_element_type %3297, %int6_3558 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3559 = torch.constant.int 6
    %3303 = torch.prims.convert_element_type %3299, %int6_3559 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3304 = torch.aten.mm %3302, %3303 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3560 = torch.constant.int 1
    %3305 = torch.aten.mul.Scalar %3304, %int1_3560 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3561 = torch.constant.int 1
    %3306 = torch.aten.mul.Scalar %3301, %int1_3561 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3562 = torch.constant.int 1
    %3307 = torch.aten.add.Tensor %3305, %3306, %int1_3562 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3563 = torch.constant.int 5
    %3308 = torch.prims.convert_element_type %3307, %int5_3563 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3564 = torch.constant.int 1
    %int64_3565 = torch.constant.int 64
    %int1280_3566 = torch.constant.int 1280
    %3309 = torch.prim.ListConstruct %int1_3564, %int64_3565, %int1280_3566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3310 = torch.aten.view %3308, %3309 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_3567 = torch.constant.float 1.250000e-01
    %3311 = torch.aten.mul.Scalar %3310, %float1.250000e-01_3567 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_3568 = torch.constant.int 64
    %int1280_3569 = torch.constant.int 1280
    %3312 = torch.prim.ListConstruct %int64_3568, %int1280_3569 : (!torch.int, !torch.int) -> !torch.list<int>
    %3313 = torch.aten.view %3293, %3312 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3314 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3570 = torch.constant.int 0
    %int1_3571 = torch.constant.int 1
    %3315 = torch.aten.transpose.int %3314, %int0_3570, %int1_3571 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.bias : tensor<1280xf16>
    %3316 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3572 = torch.constant.int 6
    %3317 = torch.prims.convert_element_type %3316, %int6_3572 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3573 = torch.constant.int 6
    %3318 = torch.prims.convert_element_type %3313, %int6_3573 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3574 = torch.constant.int 6
    %3319 = torch.prims.convert_element_type %3315, %int6_3574 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3320 = torch.aten.mm %3318, %3319 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3575 = torch.constant.int 1
    %3321 = torch.aten.mul.Scalar %3320, %int1_3575 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3576 = torch.constant.int 1
    %3322 = torch.aten.mul.Scalar %3317, %int1_3576 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3577 = torch.constant.int 1
    %3323 = torch.aten.add.Tensor %3321, %3322, %int1_3577 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3578 = torch.constant.int 5
    %3324 = torch.prims.convert_element_type %3323, %int5_3578 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3579 = torch.constant.int 1
    %int64_3580 = torch.constant.int 64
    %int1280_3581 = torch.constant.int 1280
    %3325 = torch.prim.ListConstruct %int1_3579, %int64_3580, %int1280_3581 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3326 = torch.aten.view %3324, %3325 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3582 = torch.constant.int 1
    %int-1_3583 = torch.constant.int -1
    %int20_3584 = torch.constant.int 20
    %int64_3585 = torch.constant.int 64
    %3327 = torch.prim.ListConstruct %int1_3582, %int-1_3583, %int20_3584, %int64_3585 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3328 = torch.aten.view %3326, %3327 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3586 = torch.constant.int 1
    %int2_3587 = torch.constant.int 2
    %3329 = torch.aten.transpose.int %3328, %int1_3586, %int2_3587 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3588 = torch.constant.int 0
    %3330 = torch.aten.clone %3329, %int0_3588 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_3589 = torch.constant.int 64
    %int1280_3590 = torch.constant.int 1280
    %3331 = torch.prim.ListConstruct %int64_3589, %int1280_3590 : (!torch.int, !torch.int) -> !torch.list<int>
    %3332 = torch.aten.view %3293, %3331 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3333 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3591 = torch.constant.int 0
    %int1_3592 = torch.constant.int 1
    %3334 = torch.aten.transpose.int %3333, %int0_3591, %int1_3592 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.bias : tensor<1280xf16>
    %3335 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3593 = torch.constant.int 6
    %3336 = torch.prims.convert_element_type %3335, %int6_3593 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3594 = torch.constant.int 6
    %3337 = torch.prims.convert_element_type %3332, %int6_3594 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3595 = torch.constant.int 6
    %3338 = torch.prims.convert_element_type %3334, %int6_3595 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3339 = torch.aten.mm %3337, %3338 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3596 = torch.constant.int 1
    %3340 = torch.aten.mul.Scalar %3339, %int1_3596 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3597 = torch.constant.int 1
    %3341 = torch.aten.mul.Scalar %3336, %int1_3597 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3598 = torch.constant.int 1
    %3342 = torch.aten.add.Tensor %3340, %3341, %int1_3598 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3599 = torch.constant.int 5
    %3343 = torch.prims.convert_element_type %3342, %int5_3599 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3600 = torch.constant.int 1
    %int64_3601 = torch.constant.int 64
    %int1280_3602 = torch.constant.int 1280
    %3344 = torch.prim.ListConstruct %int1_3600, %int64_3601, %int1280_3602 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3345 = torch.aten.view %3343, %3344 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3603 = torch.constant.int 1
    %int-1_3604 = torch.constant.int -1
    %int20_3605 = torch.constant.int 20
    %int64_3606 = torch.constant.int 64
    %3346 = torch.prim.ListConstruct %int1_3603, %int-1_3604, %int20_3605, %int64_3606 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3347 = torch.aten.view %3345, %3346 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3607 = torch.constant.int 1
    %int2_3608 = torch.constant.int 2
    %3348 = torch.aten.transpose.int %3347, %int1_3607, %int2_3608 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3609 = torch.constant.int 0
    %3349 = torch.aten.clone %3348, %int0_3609 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3610 = torch.constant.int 1
    %int64_3611 = torch.constant.int 64
    %int20_3612 = torch.constant.int 20
    %int64_3613 = torch.constant.int 64
    %3350 = torch.prim.ListConstruct %int1_3610, %int64_3611, %int20_3612, %int64_3613 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3351 = torch.aten.view %3311, %3350 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3614 = torch.constant.int 1
    %int2_3615 = torch.constant.int 2
    %3352 = torch.aten.transpose.int %3351, %int1_3614, %int2_3615 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3616 = torch.constant.int 0
    %3353 = torch.aten.clone %3352, %int0_3616 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3617 = torch.constant.int 20
    %int-1_3618 = torch.constant.int -1
    %int64_3619 = torch.constant.int 64
    %3354 = torch.prim.ListConstruct %int20_3617, %int-1_3618, %int64_3619 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3355 = torch.aten.view %3353, %3354 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3620 = torch.constant.int 20
    %int-1_3621 = torch.constant.int -1
    %int64_3622 = torch.constant.int 64
    %3356 = torch.prim.ListConstruct %int20_3620, %int-1_3621, %int64_3622 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3357 = torch.aten.view %3330, %3356 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3623 = torch.constant.int 20
    %int-1_3624 = torch.constant.int -1
    %int64_3625 = torch.constant.int 64
    %3358 = torch.prim.ListConstruct %int20_3623, %int-1_3624, %int64_3625 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3359 = torch.aten.view %3349, %3358 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_3626 = torch.constant.int 1
    %int2_3627 = torch.constant.int 2
    %3360 = torch.aten.transpose.int %3357, %int1_3626, %int2_3627 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3361 = torch.aten.bmm %3355, %3360 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3628 = torch.constant.int 1
    %int20_3629 = torch.constant.int 20
    %int64_3630 = torch.constant.int 64
    %int64_3631 = torch.constant.int 64
    %3362 = torch.prim.ListConstruct %int1_3628, %int20_3629, %int64_3630, %int64_3631 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3363 = torch.aten.view %3361, %3362 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3632 = torch.constant.int 1
    %3364 = torch.aten.add.Tensor %3363, %27, %int1_3632 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3633 = torch.constant.int 20
    %int64_3634 = torch.constant.int 64
    %int64_3635 = torch.constant.int 64
    %3365 = torch.prim.ListConstruct %int20_3633, %int64_3634, %int64_3635 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3366 = torch.aten.view %3364, %3365 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_3636 = torch.constant.int -1
    %false_3637 = torch.constant.bool false
    %3367 = torch.aten._softmax %3366, %int-1_3636, %false_3637 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3368 = torch.aten.detach %3367 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_3638 = torch.constant.none
    %3369 = torch.aten.clone %3367, %none_3638 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3370 = torch.aten.bmm %3369, %3359 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3639 = torch.constant.int 1
    %int20_3640 = torch.constant.int 20
    %int64_3641 = torch.constant.int 64
    %int64_3642 = torch.constant.int 64
    %3371 = torch.prim.ListConstruct %int1_3639, %int20_3640, %int64_3641, %int64_3642 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3372 = torch.aten.view %3370, %3371 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3643 = torch.constant.int 1
    %int2_3644 = torch.constant.int 2
    %3373 = torch.aten.transpose.int %3372, %int1_3643, %int2_3644 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_3645 = torch.constant.int 0
    %3374 = torch.aten.clone %3373, %int0_3645 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3646 = torch.constant.int 1
    %int64_3647 = torch.constant.int 64
    %int1280_3648 = torch.constant.int 1280
    %3375 = torch.prim.ListConstruct %int1_3646, %int64_3647, %int1280_3648 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3376 = torch.aten._unsafe_view %3374, %3375 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_3649 = torch.constant.int 64
    %int1280_3650 = torch.constant.int 1280
    %3377 = torch.prim.ListConstruct %int64_3649, %int1280_3650 : (!torch.int, !torch.int) -> !torch.list<int>
    %3378 = torch.aten.view %3376, %3377 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3379 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3651 = torch.constant.int 0
    %int1_3652 = torch.constant.int 1
    %3380 = torch.aten.transpose.int %3379, %int0_3651, %int1_3652 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.bias : tensor<1280xf16>
    %3381 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3653 = torch.constant.int 6
    %3382 = torch.prims.convert_element_type %3381, %int6_3653 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3654 = torch.constant.int 6
    %3383 = torch.prims.convert_element_type %3378, %int6_3654 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3655 = torch.constant.int 6
    %3384 = torch.prims.convert_element_type %3380, %int6_3655 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3385 = torch.aten.mm %3383, %3384 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3656 = torch.constant.int 1
    %3386 = torch.aten.mul.Scalar %3385, %int1_3656 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3657 = torch.constant.int 1
    %3387 = torch.aten.mul.Scalar %3382, %int1_3657 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3658 = torch.constant.int 1
    %3388 = torch.aten.add.Tensor %3386, %3387, %int1_3658 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3659 = torch.constant.int 5
    %3389 = torch.prims.convert_element_type %3388, %int5_3659 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3660 = torch.constant.int 1
    %int64_3661 = torch.constant.int 64
    %int1280_3662 = torch.constant.int 1280
    %3390 = torch.prim.ListConstruct %int1_3660, %int64_3661, %int1280_3662 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3391 = torch.aten.view %3389, %3390 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3663 = torch.constant.int 1
    %3392 = torch.aten.add.Tensor %3282, %3391, %int1_3663 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3664 = torch.constant.int 6
    %3393 = torch.prims.convert_element_type %3392, %int6_3664 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3665 = torch.constant.int 2
    %3394 = torch.prim.ListConstruct %int2_3665 : (!torch.int) -> !torch.list<int>
    %int0_3666 = torch.constant.int 0
    %true_3667 = torch.constant.bool true
    %result0_3668, %result1_3669 = torch.aten.var_mean.correction %3393, %3394, %int0_3666, %true_3667 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3670 = torch.constant.float 1.000000e-05
    %int1_3671 = torch.constant.int 1
    %3395 = torch.aten.add.Scalar %result0_3668, %float1.000000e-05_3670, %int1_3671 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3396 = torch.aten.rsqrt %3395 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3672 = torch.constant.int 1
    %3397 = torch.aten.sub.Tensor %3392, %result1_3669, %int1_3672 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3398 = torch.aten.mul.Tensor %3397, %3396 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.weight : tensor<1280xf16>
    %3399 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3400 = torch.aten.mul.Tensor %3398, %3399 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.bias : tensor<1280xf16>
    %3401 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3673 = torch.constant.int 1
    %3402 = torch.aten.add.Tensor %3400, %3401, %int1_3673 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3674 = torch.constant.int 5
    %3403 = torch.prims.convert_element_type %3402, %int5_3674 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3675 = torch.constant.int 5
    %3404 = torch.prims.convert_element_type %result1_3669, %int5_3675 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3676 = torch.constant.int 5
    %3405 = torch.prims.convert_element_type %3396, %int5_3676 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3677 = torch.constant.int 64
    %int1280_3678 = torch.constant.int 1280
    %3406 = torch.prim.ListConstruct %int64_3677, %int1280_3678 : (!torch.int, !torch.int) -> !torch.list<int>
    %3407 = torch.aten.view %3403, %3406 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.weight : tensor<5120x1280xf16>
    %3408 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_3679 = torch.constant.int 0
    %int1_3680 = torch.constant.int 1
    %3409 = torch.aten.transpose.int %3408, %int0_3679, %int1_3680 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.bias : tensor<5120xf16>
    %3410 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_3681 = torch.constant.int 6
    %3411 = torch.prims.convert_element_type %3410, %int6_3681 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_3682 = torch.constant.int 6
    %3412 = torch.prims.convert_element_type %3407, %int6_3682 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3683 = torch.constant.int 6
    %3413 = torch.prims.convert_element_type %3409, %int6_3683 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %3414 = torch.aten.mm %3412, %3413 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_3684 = torch.constant.int 1
    %3415 = torch.aten.mul.Scalar %3414, %int1_3684 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_3685 = torch.constant.int 1
    %3416 = torch.aten.mul.Scalar %3411, %int1_3685 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_3686 = torch.constant.int 1
    %3417 = torch.aten.add.Tensor %3415, %3416, %int1_3686 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_3687 = torch.constant.int 5
    %3418 = torch.prims.convert_element_type %3417, %int5_3687 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_3688 = torch.constant.int 1
    %int64_3689 = torch.constant.int 64
    %int5120_3690 = torch.constant.int 5120
    %3419 = torch.prim.ListConstruct %int1_3688, %int64_3689, %int5120_3690 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3420 = torch.aten.view %3418, %3419 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_3691 = torch.constant.str "none"
    %3421 = torch.aten.gelu %3420, %str_3691 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_3692 = torch.constant.int 64
    %int5120_3693 = torch.constant.int 5120
    %3422 = torch.prim.ListConstruct %int64_3692, %int5120_3693 : (!torch.int, !torch.int) -> !torch.list<int>
    %3423 = torch.aten.view %3421, %3422 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.weight : tensor<1280x5120xf16>
    %3424 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3694 = torch.constant.int 0
    %int1_3695 = torch.constant.int 1
    %3425 = torch.aten.transpose.int %3424, %int0_3694, %int1_3695 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.bias : tensor<1280xf16>
    %3426 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.21.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3696 = torch.constant.int 6
    %3427 = torch.prims.convert_element_type %3426, %int6_3696 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3697 = torch.constant.int 6
    %3428 = torch.prims.convert_element_type %3423, %int6_3697 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_3698 = torch.constant.int 6
    %3429 = torch.prims.convert_element_type %3425, %int6_3698 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3430 = torch.aten.mm %3428, %3429 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3699 = torch.constant.int 1
    %3431 = torch.aten.mul.Scalar %3430, %int1_3699 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3700 = torch.constant.int 1
    %3432 = torch.aten.mul.Scalar %3427, %int1_3700 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3701 = torch.constant.int 1
    %3433 = torch.aten.add.Tensor %3431, %3432, %int1_3701 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3702 = torch.constant.int 5
    %3434 = torch.prims.convert_element_type %3433, %int5_3702 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3703 = torch.constant.int 1
    %int64_3704 = torch.constant.int 64
    %int1280_3705 = torch.constant.int 1280
    %3435 = torch.prim.ListConstruct %int1_3703, %int64_3704, %int1280_3705 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3436 = torch.aten.view %3434, %3435 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3706 = torch.constant.int 1
    %3437 = torch.aten.add.Tensor %3392, %3436, %int1_3706 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3707 = torch.constant.int 6
    %3438 = torch.prims.convert_element_type %3437, %int6_3707 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3708 = torch.constant.int 2
    %3439 = torch.prim.ListConstruct %int2_3708 : (!torch.int) -> !torch.list<int>
    %int0_3709 = torch.constant.int 0
    %true_3710 = torch.constant.bool true
    %result0_3711, %result1_3712 = torch.aten.var_mean.correction %3438, %3439, %int0_3709, %true_3710 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3713 = torch.constant.float 1.000000e-05
    %int1_3714 = torch.constant.int 1
    %3440 = torch.aten.add.Scalar %result0_3711, %float1.000000e-05_3713, %int1_3714 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3441 = torch.aten.rsqrt %3440 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3715 = torch.constant.int 1
    %3442 = torch.aten.sub.Tensor %3437, %result1_3712, %int1_3715 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3443 = torch.aten.mul.Tensor %3442, %3441 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.weight : tensor<1280xf16>
    %3444 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3445 = torch.aten.mul.Tensor %3443, %3444 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.bias : tensor<1280xf16>
    %3446 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3716 = torch.constant.int 1
    %3447 = torch.aten.add.Tensor %3445, %3446, %int1_3716 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3717 = torch.constant.int 5
    %3448 = torch.prims.convert_element_type %3447, %int5_3717 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3718 = torch.constant.int 5
    %3449 = torch.prims.convert_element_type %result1_3712, %int5_3718 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3719 = torch.constant.int 5
    %3450 = torch.prims.convert_element_type %3441, %int5_3719 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3720 = torch.constant.int 64
    %int1280_3721 = torch.constant.int 1280
    %3451 = torch.prim.ListConstruct %int64_3720, %int1280_3721 : (!torch.int, !torch.int) -> !torch.list<int>
    %3452 = torch.aten.view %3448, %3451 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %3453 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3722 = torch.constant.int 0
    %int1_3723 = torch.constant.int 1
    %3454 = torch.aten.transpose.int %3453, %int0_3722, %int1_3723 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.bias : tensor<1280xf16>
    %3455 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3724 = torch.constant.int 6
    %3456 = torch.prims.convert_element_type %3455, %int6_3724 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3725 = torch.constant.int 6
    %3457 = torch.prims.convert_element_type %3452, %int6_3725 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3726 = torch.constant.int 6
    %3458 = torch.prims.convert_element_type %3454, %int6_3726 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3459 = torch.aten.mm %3457, %3458 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3727 = torch.constant.int 1
    %3460 = torch.aten.mul.Scalar %3459, %int1_3727 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3728 = torch.constant.int 1
    %3461 = torch.aten.mul.Scalar %3456, %int1_3728 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3729 = torch.constant.int 1
    %3462 = torch.aten.add.Tensor %3460, %3461, %int1_3729 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3730 = torch.constant.int 5
    %3463 = torch.prims.convert_element_type %3462, %int5_3730 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3731 = torch.constant.int 1
    %int64_3732 = torch.constant.int 64
    %int1280_3733 = torch.constant.int 1280
    %3464 = torch.prim.ListConstruct %int1_3731, %int64_3732, %int1280_3733 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3465 = torch.aten.view %3463, %3464 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_3734 = torch.constant.float 1.250000e-01
    %3466 = torch.aten.mul.Scalar %3465, %float1.250000e-01_3734 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_3735 = torch.constant.int 64
    %int1280_3736 = torch.constant.int 1280
    %3467 = torch.prim.ListConstruct %int64_3735, %int1280_3736 : (!torch.int, !torch.int) -> !torch.list<int>
    %3468 = torch.aten.view %3448, %3467 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3469 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3737 = torch.constant.int 0
    %int1_3738 = torch.constant.int 1
    %3470 = torch.aten.transpose.int %3469, %int0_3737, %int1_3738 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.bias : tensor<1280xf16>
    %3471 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3739 = torch.constant.int 6
    %3472 = torch.prims.convert_element_type %3471, %int6_3739 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3740 = torch.constant.int 6
    %3473 = torch.prims.convert_element_type %3468, %int6_3740 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3741 = torch.constant.int 6
    %3474 = torch.prims.convert_element_type %3470, %int6_3741 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3475 = torch.aten.mm %3473, %3474 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3742 = torch.constant.int 1
    %3476 = torch.aten.mul.Scalar %3475, %int1_3742 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3743 = torch.constant.int 1
    %3477 = torch.aten.mul.Scalar %3472, %int1_3743 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3744 = torch.constant.int 1
    %3478 = torch.aten.add.Tensor %3476, %3477, %int1_3744 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3745 = torch.constant.int 5
    %3479 = torch.prims.convert_element_type %3478, %int5_3745 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3746 = torch.constant.int 1
    %int64_3747 = torch.constant.int 64
    %int1280_3748 = torch.constant.int 1280
    %3480 = torch.prim.ListConstruct %int1_3746, %int64_3747, %int1280_3748 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3481 = torch.aten.view %3479, %3480 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3749 = torch.constant.int 1
    %int-1_3750 = torch.constant.int -1
    %int20_3751 = torch.constant.int 20
    %int64_3752 = torch.constant.int 64
    %3482 = torch.prim.ListConstruct %int1_3749, %int-1_3750, %int20_3751, %int64_3752 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3483 = torch.aten.view %3481, %3482 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3753 = torch.constant.int 1
    %int2_3754 = torch.constant.int 2
    %3484 = torch.aten.transpose.int %3483, %int1_3753, %int2_3754 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3755 = torch.constant.int 0
    %3485 = torch.aten.clone %3484, %int0_3755 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_3756 = torch.constant.int 64
    %int1280_3757 = torch.constant.int 1280
    %3486 = torch.prim.ListConstruct %int64_3756, %int1280_3757 : (!torch.int, !torch.int) -> !torch.list<int>
    %3487 = torch.aten.view %3448, %3486 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3488 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3758 = torch.constant.int 0
    %int1_3759 = torch.constant.int 1
    %3489 = torch.aten.transpose.int %3488, %int0_3758, %int1_3759 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.bias : tensor<1280xf16>
    %3490 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3760 = torch.constant.int 6
    %3491 = torch.prims.convert_element_type %3490, %int6_3760 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3761 = torch.constant.int 6
    %3492 = torch.prims.convert_element_type %3487, %int6_3761 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3762 = torch.constant.int 6
    %3493 = torch.prims.convert_element_type %3489, %int6_3762 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3494 = torch.aten.mm %3492, %3493 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3763 = torch.constant.int 1
    %3495 = torch.aten.mul.Scalar %3494, %int1_3763 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3764 = torch.constant.int 1
    %3496 = torch.aten.mul.Scalar %3491, %int1_3764 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3765 = torch.constant.int 1
    %3497 = torch.aten.add.Tensor %3495, %3496, %int1_3765 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3766 = torch.constant.int 5
    %3498 = torch.prims.convert_element_type %3497, %int5_3766 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3767 = torch.constant.int 1
    %int64_3768 = torch.constant.int 64
    %int1280_3769 = torch.constant.int 1280
    %3499 = torch.prim.ListConstruct %int1_3767, %int64_3768, %int1280_3769 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3500 = torch.aten.view %3498, %3499 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3770 = torch.constant.int 1
    %int-1_3771 = torch.constant.int -1
    %int20_3772 = torch.constant.int 20
    %int64_3773 = torch.constant.int 64
    %3501 = torch.prim.ListConstruct %int1_3770, %int-1_3771, %int20_3772, %int64_3773 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3502 = torch.aten.view %3500, %3501 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3774 = torch.constant.int 1
    %int2_3775 = torch.constant.int 2
    %3503 = torch.aten.transpose.int %3502, %int1_3774, %int2_3775 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3776 = torch.constant.int 0
    %3504 = torch.aten.clone %3503, %int0_3776 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3777 = torch.constant.int 1
    %int64_3778 = torch.constant.int 64
    %int20_3779 = torch.constant.int 20
    %int64_3780 = torch.constant.int 64
    %3505 = torch.prim.ListConstruct %int1_3777, %int64_3778, %int20_3779, %int64_3780 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3506 = torch.aten.view %3466, %3505 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3781 = torch.constant.int 1
    %int2_3782 = torch.constant.int 2
    %3507 = torch.aten.transpose.int %3506, %int1_3781, %int2_3782 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3783 = torch.constant.int 0
    %3508 = torch.aten.clone %3507, %int0_3783 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3784 = torch.constant.int 20
    %int-1_3785 = torch.constant.int -1
    %int64_3786 = torch.constant.int 64
    %3509 = torch.prim.ListConstruct %int20_3784, %int-1_3785, %int64_3786 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3510 = torch.aten.view %3508, %3509 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3787 = torch.constant.int 20
    %int-1_3788 = torch.constant.int -1
    %int64_3789 = torch.constant.int 64
    %3511 = torch.prim.ListConstruct %int20_3787, %int-1_3788, %int64_3789 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3512 = torch.aten.view %3485, %3511 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3790 = torch.constant.int 20
    %int-1_3791 = torch.constant.int -1
    %int64_3792 = torch.constant.int 64
    %3513 = torch.prim.ListConstruct %int20_3790, %int-1_3791, %int64_3792 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3514 = torch.aten.view %3504, %3513 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_3793 = torch.constant.int 1
    %int2_3794 = torch.constant.int 2
    %3515 = torch.aten.transpose.int %3512, %int1_3793, %int2_3794 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3516 = torch.aten.bmm %3510, %3515 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3795 = torch.constant.int 1
    %int20_3796 = torch.constant.int 20
    %int64_3797 = torch.constant.int 64
    %int64_3798 = torch.constant.int 64
    %3517 = torch.prim.ListConstruct %int1_3795, %int20_3796, %int64_3797, %int64_3798 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3518 = torch.aten.view %3516, %3517 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3799 = torch.constant.int 1
    %3519 = torch.aten.add.Tensor %3518, %27, %int1_3799 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3800 = torch.constant.int 20
    %int64_3801 = torch.constant.int 64
    %int64_3802 = torch.constant.int 64
    %3520 = torch.prim.ListConstruct %int20_3800, %int64_3801, %int64_3802 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3521 = torch.aten.view %3519, %3520 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_3803 = torch.constant.int -1
    %false_3804 = torch.constant.bool false
    %3522 = torch.aten._softmax %3521, %int-1_3803, %false_3804 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3523 = torch.aten.detach %3522 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_3805 = torch.constant.none
    %3524 = torch.aten.clone %3522, %none_3805 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3525 = torch.aten.bmm %3524, %3514 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3806 = torch.constant.int 1
    %int20_3807 = torch.constant.int 20
    %int64_3808 = torch.constant.int 64
    %int64_3809 = torch.constant.int 64
    %3526 = torch.prim.ListConstruct %int1_3806, %int20_3807, %int64_3808, %int64_3809 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3527 = torch.aten.view %3525, %3526 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3810 = torch.constant.int 1
    %int2_3811 = torch.constant.int 2
    %3528 = torch.aten.transpose.int %3527, %int1_3810, %int2_3811 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_3812 = torch.constant.int 0
    %3529 = torch.aten.clone %3528, %int0_3812 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3813 = torch.constant.int 1
    %int64_3814 = torch.constant.int 64
    %int1280_3815 = torch.constant.int 1280
    %3530 = torch.prim.ListConstruct %int1_3813, %int64_3814, %int1280_3815 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3531 = torch.aten._unsafe_view %3529, %3530 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_3816 = torch.constant.int 64
    %int1280_3817 = torch.constant.int 1280
    %3532 = torch.prim.ListConstruct %int64_3816, %int1280_3817 : (!torch.int, !torch.int) -> !torch.list<int>
    %3533 = torch.aten.view %3531, %3532 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3534 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3818 = torch.constant.int 0
    %int1_3819 = torch.constant.int 1
    %3535 = torch.aten.transpose.int %3534, %int0_3818, %int1_3819 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.bias : tensor<1280xf16>
    %3536 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3820 = torch.constant.int 6
    %3537 = torch.prims.convert_element_type %3536, %int6_3820 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3821 = torch.constant.int 6
    %3538 = torch.prims.convert_element_type %3533, %int6_3821 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3822 = torch.constant.int 6
    %3539 = torch.prims.convert_element_type %3535, %int6_3822 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3540 = torch.aten.mm %3538, %3539 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3823 = torch.constant.int 1
    %3541 = torch.aten.mul.Scalar %3540, %int1_3823 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3824 = torch.constant.int 1
    %3542 = torch.aten.mul.Scalar %3537, %int1_3824 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3825 = torch.constant.int 1
    %3543 = torch.aten.add.Tensor %3541, %3542, %int1_3825 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3826 = torch.constant.int 5
    %3544 = torch.prims.convert_element_type %3543, %int5_3826 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3827 = torch.constant.int 1
    %int64_3828 = torch.constant.int 64
    %int1280_3829 = torch.constant.int 1280
    %3545 = torch.prim.ListConstruct %int1_3827, %int64_3828, %int1280_3829 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3546 = torch.aten.view %3544, %3545 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3830 = torch.constant.int 1
    %3547 = torch.aten.add.Tensor %3437, %3546, %int1_3830 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3831 = torch.constant.int 6
    %3548 = torch.prims.convert_element_type %3547, %int6_3831 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3832 = torch.constant.int 2
    %3549 = torch.prim.ListConstruct %int2_3832 : (!torch.int) -> !torch.list<int>
    %int0_3833 = torch.constant.int 0
    %true_3834 = torch.constant.bool true
    %result0_3835, %result1_3836 = torch.aten.var_mean.correction %3548, %3549, %int0_3833, %true_3834 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3837 = torch.constant.float 1.000000e-05
    %int1_3838 = torch.constant.int 1
    %3550 = torch.aten.add.Scalar %result0_3835, %float1.000000e-05_3837, %int1_3838 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3551 = torch.aten.rsqrt %3550 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3839 = torch.constant.int 1
    %3552 = torch.aten.sub.Tensor %3547, %result1_3836, %int1_3839 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3553 = torch.aten.mul.Tensor %3552, %3551 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.weight : tensor<1280xf16>
    %3554 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3555 = torch.aten.mul.Tensor %3553, %3554 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.bias : tensor<1280xf16>
    %3556 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3840 = torch.constant.int 1
    %3557 = torch.aten.add.Tensor %3555, %3556, %int1_3840 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3841 = torch.constant.int 5
    %3558 = torch.prims.convert_element_type %3557, %int5_3841 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3842 = torch.constant.int 5
    %3559 = torch.prims.convert_element_type %result1_3836, %int5_3842 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3843 = torch.constant.int 5
    %3560 = torch.prims.convert_element_type %3551, %int5_3843 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3844 = torch.constant.int 64
    %int1280_3845 = torch.constant.int 1280
    %3561 = torch.prim.ListConstruct %int64_3844, %int1280_3845 : (!torch.int, !torch.int) -> !torch.list<int>
    %3562 = torch.aten.view %3558, %3561 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.weight : tensor<5120x1280xf16>
    %3563 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_3846 = torch.constant.int 0
    %int1_3847 = torch.constant.int 1
    %3564 = torch.aten.transpose.int %3563, %int0_3846, %int1_3847 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.bias : tensor<5120xf16>
    %3565 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_3848 = torch.constant.int 6
    %3566 = torch.prims.convert_element_type %3565, %int6_3848 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_3849 = torch.constant.int 6
    %3567 = torch.prims.convert_element_type %3562, %int6_3849 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3850 = torch.constant.int 6
    %3568 = torch.prims.convert_element_type %3564, %int6_3850 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %3569 = torch.aten.mm %3567, %3568 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_3851 = torch.constant.int 1
    %3570 = torch.aten.mul.Scalar %3569, %int1_3851 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_3852 = torch.constant.int 1
    %3571 = torch.aten.mul.Scalar %3566, %int1_3852 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_3853 = torch.constant.int 1
    %3572 = torch.aten.add.Tensor %3570, %3571, %int1_3853 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_3854 = torch.constant.int 5
    %3573 = torch.prims.convert_element_type %3572, %int5_3854 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_3855 = torch.constant.int 1
    %int64_3856 = torch.constant.int 64
    %int5120_3857 = torch.constant.int 5120
    %3574 = torch.prim.ListConstruct %int1_3855, %int64_3856, %int5120_3857 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3575 = torch.aten.view %3573, %3574 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_3858 = torch.constant.str "none"
    %3576 = torch.aten.gelu %3575, %str_3858 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_3859 = torch.constant.int 64
    %int5120_3860 = torch.constant.int 5120
    %3577 = torch.prim.ListConstruct %int64_3859, %int5120_3860 : (!torch.int, !torch.int) -> !torch.list<int>
    %3578 = torch.aten.view %3576, %3577 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.weight : tensor<1280x5120xf16>
    %3579 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3861 = torch.constant.int 0
    %int1_3862 = torch.constant.int 1
    %3580 = torch.aten.transpose.int %3579, %int0_3861, %int1_3862 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.bias : tensor<1280xf16>
    %3581 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.22.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3863 = torch.constant.int 6
    %3582 = torch.prims.convert_element_type %3581, %int6_3863 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3864 = torch.constant.int 6
    %3583 = torch.prims.convert_element_type %3578, %int6_3864 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_3865 = torch.constant.int 6
    %3584 = torch.prims.convert_element_type %3580, %int6_3865 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3585 = torch.aten.mm %3583, %3584 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3866 = torch.constant.int 1
    %3586 = torch.aten.mul.Scalar %3585, %int1_3866 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3867 = torch.constant.int 1
    %3587 = torch.aten.mul.Scalar %3582, %int1_3867 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3868 = torch.constant.int 1
    %3588 = torch.aten.add.Tensor %3586, %3587, %int1_3868 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3869 = torch.constant.int 5
    %3589 = torch.prims.convert_element_type %3588, %int5_3869 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3870 = torch.constant.int 1
    %int64_3871 = torch.constant.int 64
    %int1280_3872 = torch.constant.int 1280
    %3590 = torch.prim.ListConstruct %int1_3870, %int64_3871, %int1280_3872 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3591 = torch.aten.view %3589, %3590 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3873 = torch.constant.int 1
    %3592 = torch.aten.add.Tensor %3547, %3591, %int1_3873 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3874 = torch.constant.int 6
    %3593 = torch.prims.convert_element_type %3592, %int6_3874 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3875 = torch.constant.int 2
    %3594 = torch.prim.ListConstruct %int2_3875 : (!torch.int) -> !torch.list<int>
    %int0_3876 = torch.constant.int 0
    %true_3877 = torch.constant.bool true
    %result0_3878, %result1_3879 = torch.aten.var_mean.correction %3593, %3594, %int0_3876, %true_3877 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_3880 = torch.constant.float 1.000000e-05
    %int1_3881 = torch.constant.int 1
    %3595 = torch.aten.add.Scalar %result0_3878, %float1.000000e-05_3880, %int1_3881 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3596 = torch.aten.rsqrt %3595 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_3882 = torch.constant.int 1
    %3597 = torch.aten.sub.Tensor %3592, %result1_3879, %int1_3882 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3598 = torch.aten.mul.Tensor %3597, %3596 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.weight : tensor<1280xf16>
    %3599 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3600 = torch.aten.mul.Tensor %3598, %3599 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.bias : tensor<1280xf16>
    %3601 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3883 = torch.constant.int 1
    %3602 = torch.aten.add.Tensor %3600, %3601, %int1_3883 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_3884 = torch.constant.int 5
    %3603 = torch.prims.convert_element_type %3602, %int5_3884 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_3885 = torch.constant.int 5
    %3604 = torch.prims.convert_element_type %result1_3879, %int5_3885 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_3886 = torch.constant.int 5
    %3605 = torch.prims.convert_element_type %3596, %int5_3886 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_3887 = torch.constant.int 64
    %int1280_3888 = torch.constant.int 1280
    %3606 = torch.prim.ListConstruct %int64_3887, %int1280_3888 : (!torch.int, !torch.int) -> !torch.list<int>
    %3607 = torch.aten.view %3603, %3606 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %3608 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3889 = torch.constant.int 0
    %int1_3890 = torch.constant.int 1
    %3609 = torch.aten.transpose.int %3608, %int0_3889, %int1_3890 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.bias : tensor<1280xf16>
    %3610 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3891 = torch.constant.int 6
    %3611 = torch.prims.convert_element_type %3610, %int6_3891 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3892 = torch.constant.int 6
    %3612 = torch.prims.convert_element_type %3607, %int6_3892 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3893 = torch.constant.int 6
    %3613 = torch.prims.convert_element_type %3609, %int6_3893 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3614 = torch.aten.mm %3612, %3613 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3894 = torch.constant.int 1
    %3615 = torch.aten.mul.Scalar %3614, %int1_3894 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3895 = torch.constant.int 1
    %3616 = torch.aten.mul.Scalar %3611, %int1_3895 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3896 = torch.constant.int 1
    %3617 = torch.aten.add.Tensor %3615, %3616, %int1_3896 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3897 = torch.constant.int 5
    %3618 = torch.prims.convert_element_type %3617, %int5_3897 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3898 = torch.constant.int 1
    %int64_3899 = torch.constant.int 64
    %int1280_3900 = torch.constant.int 1280
    %3619 = torch.prim.ListConstruct %int1_3898, %int64_3899, %int1280_3900 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3620 = torch.aten.view %3618, %3619 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_3901 = torch.constant.float 1.250000e-01
    %3621 = torch.aten.mul.Scalar %3620, %float1.250000e-01_3901 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_3902 = torch.constant.int 64
    %int1280_3903 = torch.constant.int 1280
    %3622 = torch.prim.ListConstruct %int64_3902, %int1280_3903 : (!torch.int, !torch.int) -> !torch.list<int>
    %3623 = torch.aten.view %3603, %3622 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3624 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3904 = torch.constant.int 0
    %int1_3905 = torch.constant.int 1
    %3625 = torch.aten.transpose.int %3624, %int0_3904, %int1_3905 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.bias : tensor<1280xf16>
    %3626 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3906 = torch.constant.int 6
    %3627 = torch.prims.convert_element_type %3626, %int6_3906 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3907 = torch.constant.int 6
    %3628 = torch.prims.convert_element_type %3623, %int6_3907 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3908 = torch.constant.int 6
    %3629 = torch.prims.convert_element_type %3625, %int6_3908 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3630 = torch.aten.mm %3628, %3629 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3909 = torch.constant.int 1
    %3631 = torch.aten.mul.Scalar %3630, %int1_3909 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3910 = torch.constant.int 1
    %3632 = torch.aten.mul.Scalar %3627, %int1_3910 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3911 = torch.constant.int 1
    %3633 = torch.aten.add.Tensor %3631, %3632, %int1_3911 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3912 = torch.constant.int 5
    %3634 = torch.prims.convert_element_type %3633, %int5_3912 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3913 = torch.constant.int 1
    %int64_3914 = torch.constant.int 64
    %int1280_3915 = torch.constant.int 1280
    %3635 = torch.prim.ListConstruct %int1_3913, %int64_3914, %int1280_3915 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3636 = torch.aten.view %3634, %3635 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3916 = torch.constant.int 1
    %int-1_3917 = torch.constant.int -1
    %int20_3918 = torch.constant.int 20
    %int64_3919 = torch.constant.int 64
    %3637 = torch.prim.ListConstruct %int1_3916, %int-1_3917, %int20_3918, %int64_3919 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3638 = torch.aten.view %3636, %3637 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3920 = torch.constant.int 1
    %int2_3921 = torch.constant.int 2
    %3639 = torch.aten.transpose.int %3638, %int1_3920, %int2_3921 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3922 = torch.constant.int 0
    %3640 = torch.aten.clone %3639, %int0_3922 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_3923 = torch.constant.int 64
    %int1280_3924 = torch.constant.int 1280
    %3641 = torch.prim.ListConstruct %int64_3923, %int1280_3924 : (!torch.int, !torch.int) -> !torch.list<int>
    %3642 = torch.aten.view %3603, %3641 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3643 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3925 = torch.constant.int 0
    %int1_3926 = torch.constant.int 1
    %3644 = torch.aten.transpose.int %3643, %int0_3925, %int1_3926 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.bias : tensor<1280xf16>
    %3645 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3927 = torch.constant.int 6
    %3646 = torch.prims.convert_element_type %3645, %int6_3927 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3928 = torch.constant.int 6
    %3647 = torch.prims.convert_element_type %3642, %int6_3928 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3929 = torch.constant.int 6
    %3648 = torch.prims.convert_element_type %3644, %int6_3929 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3649 = torch.aten.mm %3647, %3648 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3930 = torch.constant.int 1
    %3650 = torch.aten.mul.Scalar %3649, %int1_3930 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3931 = torch.constant.int 1
    %3651 = torch.aten.mul.Scalar %3646, %int1_3931 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3932 = torch.constant.int 1
    %3652 = torch.aten.add.Tensor %3650, %3651, %int1_3932 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3933 = torch.constant.int 5
    %3653 = torch.prims.convert_element_type %3652, %int5_3933 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3934 = torch.constant.int 1
    %int64_3935 = torch.constant.int 64
    %int1280_3936 = torch.constant.int 1280
    %3654 = torch.prim.ListConstruct %int1_3934, %int64_3935, %int1280_3936 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3655 = torch.aten.view %3653, %3654 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3937 = torch.constant.int 1
    %int-1_3938 = torch.constant.int -1
    %int20_3939 = torch.constant.int 20
    %int64_3940 = torch.constant.int 64
    %3656 = torch.prim.ListConstruct %int1_3937, %int-1_3938, %int20_3939, %int64_3940 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3657 = torch.aten.view %3655, %3656 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3941 = torch.constant.int 1
    %int2_3942 = torch.constant.int 2
    %3658 = torch.aten.transpose.int %3657, %int1_3941, %int2_3942 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3943 = torch.constant.int 0
    %3659 = torch.aten.clone %3658, %int0_3943 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3944 = torch.constant.int 1
    %int64_3945 = torch.constant.int 64
    %int20_3946 = torch.constant.int 20
    %int64_3947 = torch.constant.int 64
    %3660 = torch.prim.ListConstruct %int1_3944, %int64_3945, %int20_3946, %int64_3947 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3661 = torch.aten.view %3621, %3660 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3948 = torch.constant.int 1
    %int2_3949 = torch.constant.int 2
    %3662 = torch.aten.transpose.int %3661, %int1_3948, %int2_3949 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_3950 = torch.constant.int 0
    %3663 = torch.aten.clone %3662, %int0_3950 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3951 = torch.constant.int 20
    %int-1_3952 = torch.constant.int -1
    %int64_3953 = torch.constant.int 64
    %3664 = torch.prim.ListConstruct %int20_3951, %int-1_3952, %int64_3953 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3665 = torch.aten.view %3663, %3664 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3954 = torch.constant.int 20
    %int-1_3955 = torch.constant.int -1
    %int64_3956 = torch.constant.int 64
    %3666 = torch.prim.ListConstruct %int20_3954, %int-1_3955, %int64_3956 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3667 = torch.aten.view %3640, %3666 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_3957 = torch.constant.int 20
    %int-1_3958 = torch.constant.int -1
    %int64_3959 = torch.constant.int 64
    %3668 = torch.prim.ListConstruct %int20_3957, %int-1_3958, %int64_3959 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3669 = torch.aten.view %3659, %3668 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_3960 = torch.constant.int 1
    %int2_3961 = torch.constant.int 2
    %3670 = torch.aten.transpose.int %3667, %int1_3960, %int2_3961 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3671 = torch.aten.bmm %3665, %3670 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3962 = torch.constant.int 1
    %int20_3963 = torch.constant.int 20
    %int64_3964 = torch.constant.int 64
    %int64_3965 = torch.constant.int 64
    %3672 = torch.prim.ListConstruct %int1_3962, %int20_3963, %int64_3964, %int64_3965 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3673 = torch.aten.view %3671, %3672 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3966 = torch.constant.int 1
    %3674 = torch.aten.add.Tensor %3673, %27, %int1_3966 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_3967 = torch.constant.int 20
    %int64_3968 = torch.constant.int 64
    %int64_3969 = torch.constant.int 64
    %3675 = torch.prim.ListConstruct %int20_3967, %int64_3968, %int64_3969 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3676 = torch.aten.view %3674, %3675 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_3970 = torch.constant.int -1
    %false_3971 = torch.constant.bool false
    %3677 = torch.aten._softmax %3676, %int-1_3970, %false_3971 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3678 = torch.aten.detach %3677 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_3972 = torch.constant.none
    %3679 = torch.aten.clone %3677, %none_3972 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3680 = torch.aten.bmm %3679, %3669 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_3973 = torch.constant.int 1
    %int20_3974 = torch.constant.int 20
    %int64_3975 = torch.constant.int 64
    %int64_3976 = torch.constant.int 64
    %3681 = torch.prim.ListConstruct %int1_3973, %int20_3974, %int64_3975, %int64_3976 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3682 = torch.aten.view %3680, %3681 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_3977 = torch.constant.int 1
    %int2_3978 = torch.constant.int 2
    %3683 = torch.aten.transpose.int %3682, %int1_3977, %int2_3978 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_3979 = torch.constant.int 0
    %3684 = torch.aten.clone %3683, %int0_3979 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_3980 = torch.constant.int 1
    %int64_3981 = torch.constant.int 64
    %int1280_3982 = torch.constant.int 1280
    %3685 = torch.prim.ListConstruct %int1_3980, %int64_3981, %int1280_3982 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3686 = torch.aten._unsafe_view %3684, %3685 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_3983 = torch.constant.int 64
    %int1280_3984 = torch.constant.int 1280
    %3687 = torch.prim.ListConstruct %int64_3983, %int1280_3984 : (!torch.int, !torch.int) -> !torch.list<int>
    %3688 = torch.aten.view %3686, %3687 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3689 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3985 = torch.constant.int 0
    %int1_3986 = torch.constant.int 1
    %3690 = torch.aten.transpose.int %3689, %int0_3985, %int1_3986 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.bias : tensor<1280xf16>
    %3691 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3987 = torch.constant.int 6
    %3692 = torch.prims.convert_element_type %3691, %int6_3987 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3988 = torch.constant.int 6
    %3693 = torch.prims.convert_element_type %3688, %int6_3988 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_3989 = torch.constant.int 6
    %3694 = torch.prims.convert_element_type %3690, %int6_3989 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3695 = torch.aten.mm %3693, %3694 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_3990 = torch.constant.int 1
    %3696 = torch.aten.mul.Scalar %3695, %int1_3990 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_3991 = torch.constant.int 1
    %3697 = torch.aten.mul.Scalar %3692, %int1_3991 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3992 = torch.constant.int 1
    %3698 = torch.aten.add.Tensor %3696, %3697, %int1_3992 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_3993 = torch.constant.int 5
    %3699 = torch.prims.convert_element_type %3698, %int5_3993 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_3994 = torch.constant.int 1
    %int64_3995 = torch.constant.int 64
    %int1280_3996 = torch.constant.int 1280
    %3700 = torch.prim.ListConstruct %int1_3994, %int64_3995, %int1280_3996 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3701 = torch.aten.view %3699, %3700 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_3997 = torch.constant.int 1
    %3702 = torch.aten.add.Tensor %3592, %3701, %int1_3997 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_3998 = torch.constant.int 6
    %3703 = torch.prims.convert_element_type %3702, %int6_3998 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_3999 = torch.constant.int 2
    %3704 = torch.prim.ListConstruct %int2_3999 : (!torch.int) -> !torch.list<int>
    %int0_4000 = torch.constant.int 0
    %true_4001 = torch.constant.bool true
    %result0_4002, %result1_4003 = torch.aten.var_mean.correction %3703, %3704, %int0_4000, %true_4001 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4004 = torch.constant.float 1.000000e-05
    %int1_4005 = torch.constant.int 1
    %3705 = torch.aten.add.Scalar %result0_4002, %float1.000000e-05_4004, %int1_4005 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3706 = torch.aten.rsqrt %3705 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4006 = torch.constant.int 1
    %3707 = torch.aten.sub.Tensor %3702, %result1_4003, %int1_4006 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3708 = torch.aten.mul.Tensor %3707, %3706 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.weight : tensor<1280xf16>
    %3709 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3710 = torch.aten.mul.Tensor %3708, %3709 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.bias : tensor<1280xf16>
    %3711 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4007 = torch.constant.int 1
    %3712 = torch.aten.add.Tensor %3710, %3711, %int1_4007 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4008 = torch.constant.int 5
    %3713 = torch.prims.convert_element_type %3712, %int5_4008 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4009 = torch.constant.int 5
    %3714 = torch.prims.convert_element_type %result1_4003, %int5_4009 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4010 = torch.constant.int 5
    %3715 = torch.prims.convert_element_type %3706, %int5_4010 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4011 = torch.constant.int 64
    %int1280_4012 = torch.constant.int 1280
    %3716 = torch.prim.ListConstruct %int64_4011, %int1280_4012 : (!torch.int, !torch.int) -> !torch.list<int>
    %3717 = torch.aten.view %3713, %3716 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.weight : tensor<5120x1280xf16>
    %3718 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_4013 = torch.constant.int 0
    %int1_4014 = torch.constant.int 1
    %3719 = torch.aten.transpose.int %3718, %int0_4013, %int1_4014 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.bias : tensor<5120xf16>
    %3720 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_4015 = torch.constant.int 6
    %3721 = torch.prims.convert_element_type %3720, %int6_4015 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_4016 = torch.constant.int 6
    %3722 = torch.prims.convert_element_type %3717, %int6_4016 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4017 = torch.constant.int 6
    %3723 = torch.prims.convert_element_type %3719, %int6_4017 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %3724 = torch.aten.mm %3722, %3723 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_4018 = torch.constant.int 1
    %3725 = torch.aten.mul.Scalar %3724, %int1_4018 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_4019 = torch.constant.int 1
    %3726 = torch.aten.mul.Scalar %3721, %int1_4019 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_4020 = torch.constant.int 1
    %3727 = torch.aten.add.Tensor %3725, %3726, %int1_4020 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_4021 = torch.constant.int 5
    %3728 = torch.prims.convert_element_type %3727, %int5_4021 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_4022 = torch.constant.int 1
    %int64_4023 = torch.constant.int 64
    %int5120_4024 = torch.constant.int 5120
    %3729 = torch.prim.ListConstruct %int1_4022, %int64_4023, %int5120_4024 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3730 = torch.aten.view %3728, %3729 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_4025 = torch.constant.str "none"
    %3731 = torch.aten.gelu %3730, %str_4025 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_4026 = torch.constant.int 64
    %int5120_4027 = torch.constant.int 5120
    %3732 = torch.prim.ListConstruct %int64_4026, %int5120_4027 : (!torch.int, !torch.int) -> !torch.list<int>
    %3733 = torch.aten.view %3731, %3732 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.weight : tensor<1280x5120xf16>
    %3734 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4028 = torch.constant.int 0
    %int1_4029 = torch.constant.int 1
    %3735 = torch.aten.transpose.int %3734, %int0_4028, %int1_4029 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.bias : tensor<1280xf16>
    %3736 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.23.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4030 = torch.constant.int 6
    %3737 = torch.prims.convert_element_type %3736, %int6_4030 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4031 = torch.constant.int 6
    %3738 = torch.prims.convert_element_type %3733, %int6_4031 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_4032 = torch.constant.int 6
    %3739 = torch.prims.convert_element_type %3735, %int6_4032 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3740 = torch.aten.mm %3738, %3739 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4033 = torch.constant.int 1
    %3741 = torch.aten.mul.Scalar %3740, %int1_4033 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4034 = torch.constant.int 1
    %3742 = torch.aten.mul.Scalar %3737, %int1_4034 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4035 = torch.constant.int 1
    %3743 = torch.aten.add.Tensor %3741, %3742, %int1_4035 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4036 = torch.constant.int 5
    %3744 = torch.prims.convert_element_type %3743, %int5_4036 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4037 = torch.constant.int 1
    %int64_4038 = torch.constant.int 64
    %int1280_4039 = torch.constant.int 1280
    %3745 = torch.prim.ListConstruct %int1_4037, %int64_4038, %int1280_4039 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3746 = torch.aten.view %3744, %3745 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4040 = torch.constant.int 1
    %3747 = torch.aten.add.Tensor %3702, %3746, %int1_4040 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4041 = torch.constant.int 6
    %3748 = torch.prims.convert_element_type %3747, %int6_4041 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4042 = torch.constant.int 2
    %3749 = torch.prim.ListConstruct %int2_4042 : (!torch.int) -> !torch.list<int>
    %int0_4043 = torch.constant.int 0
    %true_4044 = torch.constant.bool true
    %result0_4045, %result1_4046 = torch.aten.var_mean.correction %3748, %3749, %int0_4043, %true_4044 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4047 = torch.constant.float 1.000000e-05
    %int1_4048 = torch.constant.int 1
    %3750 = torch.aten.add.Scalar %result0_4045, %float1.000000e-05_4047, %int1_4048 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3751 = torch.aten.rsqrt %3750 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4049 = torch.constant.int 1
    %3752 = torch.aten.sub.Tensor %3747, %result1_4046, %int1_4049 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3753 = torch.aten.mul.Tensor %3752, %3751 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.weight : tensor<1280xf16>
    %3754 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3755 = torch.aten.mul.Tensor %3753, %3754 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.bias : tensor<1280xf16>
    %3756 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4050 = torch.constant.int 1
    %3757 = torch.aten.add.Tensor %3755, %3756, %int1_4050 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4051 = torch.constant.int 5
    %3758 = torch.prims.convert_element_type %3757, %int5_4051 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4052 = torch.constant.int 5
    %3759 = torch.prims.convert_element_type %result1_4046, %int5_4052 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4053 = torch.constant.int 5
    %3760 = torch.prims.convert_element_type %3751, %int5_4053 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4054 = torch.constant.int 64
    %int1280_4055 = torch.constant.int 1280
    %3761 = torch.prim.ListConstruct %int64_4054, %int1280_4055 : (!torch.int, !torch.int) -> !torch.list<int>
    %3762 = torch.aten.view %3758, %3761 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %3763 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4056 = torch.constant.int 0
    %int1_4057 = torch.constant.int 1
    %3764 = torch.aten.transpose.int %3763, %int0_4056, %int1_4057 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.bias : tensor<1280xf16>
    %3765 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4058 = torch.constant.int 6
    %3766 = torch.prims.convert_element_type %3765, %int6_4058 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4059 = torch.constant.int 6
    %3767 = torch.prims.convert_element_type %3762, %int6_4059 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4060 = torch.constant.int 6
    %3768 = torch.prims.convert_element_type %3764, %int6_4060 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3769 = torch.aten.mm %3767, %3768 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4061 = torch.constant.int 1
    %3770 = torch.aten.mul.Scalar %3769, %int1_4061 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4062 = torch.constant.int 1
    %3771 = torch.aten.mul.Scalar %3766, %int1_4062 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4063 = torch.constant.int 1
    %3772 = torch.aten.add.Tensor %3770, %3771, %int1_4063 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4064 = torch.constant.int 5
    %3773 = torch.prims.convert_element_type %3772, %int5_4064 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4065 = torch.constant.int 1
    %int64_4066 = torch.constant.int 64
    %int1280_4067 = torch.constant.int 1280
    %3774 = torch.prim.ListConstruct %int1_4065, %int64_4066, %int1280_4067 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3775 = torch.aten.view %3773, %3774 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_4068 = torch.constant.float 1.250000e-01
    %3776 = torch.aten.mul.Scalar %3775, %float1.250000e-01_4068 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_4069 = torch.constant.int 64
    %int1280_4070 = torch.constant.int 1280
    %3777 = torch.prim.ListConstruct %int64_4069, %int1280_4070 : (!torch.int, !torch.int) -> !torch.list<int>
    %3778 = torch.aten.view %3758, %3777 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3779 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4071 = torch.constant.int 0
    %int1_4072 = torch.constant.int 1
    %3780 = torch.aten.transpose.int %3779, %int0_4071, %int1_4072 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.bias : tensor<1280xf16>
    %3781 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4073 = torch.constant.int 6
    %3782 = torch.prims.convert_element_type %3781, %int6_4073 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4074 = torch.constant.int 6
    %3783 = torch.prims.convert_element_type %3778, %int6_4074 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4075 = torch.constant.int 6
    %3784 = torch.prims.convert_element_type %3780, %int6_4075 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3785 = torch.aten.mm %3783, %3784 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4076 = torch.constant.int 1
    %3786 = torch.aten.mul.Scalar %3785, %int1_4076 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4077 = torch.constant.int 1
    %3787 = torch.aten.mul.Scalar %3782, %int1_4077 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4078 = torch.constant.int 1
    %3788 = torch.aten.add.Tensor %3786, %3787, %int1_4078 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4079 = torch.constant.int 5
    %3789 = torch.prims.convert_element_type %3788, %int5_4079 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4080 = torch.constant.int 1
    %int64_4081 = torch.constant.int 64
    %int1280_4082 = torch.constant.int 1280
    %3790 = torch.prim.ListConstruct %int1_4080, %int64_4081, %int1280_4082 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3791 = torch.aten.view %3789, %3790 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4083 = torch.constant.int 1
    %int-1_4084 = torch.constant.int -1
    %int20_4085 = torch.constant.int 20
    %int64_4086 = torch.constant.int 64
    %3792 = torch.prim.ListConstruct %int1_4083, %int-1_4084, %int20_4085, %int64_4086 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3793 = torch.aten.view %3791, %3792 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4087 = torch.constant.int 1
    %int2_4088 = torch.constant.int 2
    %3794 = torch.aten.transpose.int %3793, %int1_4087, %int2_4088 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4089 = torch.constant.int 0
    %3795 = torch.aten.clone %3794, %int0_4089 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_4090 = torch.constant.int 64
    %int1280_4091 = torch.constant.int 1280
    %3796 = torch.prim.ListConstruct %int64_4090, %int1280_4091 : (!torch.int, !torch.int) -> !torch.list<int>
    %3797 = torch.aten.view %3758, %3796 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3798 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4092 = torch.constant.int 0
    %int1_4093 = torch.constant.int 1
    %3799 = torch.aten.transpose.int %3798, %int0_4092, %int1_4093 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.bias : tensor<1280xf16>
    %3800 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4094 = torch.constant.int 6
    %3801 = torch.prims.convert_element_type %3800, %int6_4094 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4095 = torch.constant.int 6
    %3802 = torch.prims.convert_element_type %3797, %int6_4095 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4096 = torch.constant.int 6
    %3803 = torch.prims.convert_element_type %3799, %int6_4096 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3804 = torch.aten.mm %3802, %3803 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4097 = torch.constant.int 1
    %3805 = torch.aten.mul.Scalar %3804, %int1_4097 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4098 = torch.constant.int 1
    %3806 = torch.aten.mul.Scalar %3801, %int1_4098 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4099 = torch.constant.int 1
    %3807 = torch.aten.add.Tensor %3805, %3806, %int1_4099 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4100 = torch.constant.int 5
    %3808 = torch.prims.convert_element_type %3807, %int5_4100 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4101 = torch.constant.int 1
    %int64_4102 = torch.constant.int 64
    %int1280_4103 = torch.constant.int 1280
    %3809 = torch.prim.ListConstruct %int1_4101, %int64_4102, %int1280_4103 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3810 = torch.aten.view %3808, %3809 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4104 = torch.constant.int 1
    %int-1_4105 = torch.constant.int -1
    %int20_4106 = torch.constant.int 20
    %int64_4107 = torch.constant.int 64
    %3811 = torch.prim.ListConstruct %int1_4104, %int-1_4105, %int20_4106, %int64_4107 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3812 = torch.aten.view %3810, %3811 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4108 = torch.constant.int 1
    %int2_4109 = torch.constant.int 2
    %3813 = torch.aten.transpose.int %3812, %int1_4108, %int2_4109 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4110 = torch.constant.int 0
    %3814 = torch.aten.clone %3813, %int0_4110 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4111 = torch.constant.int 1
    %int64_4112 = torch.constant.int 64
    %int20_4113 = torch.constant.int 20
    %int64_4114 = torch.constant.int 64
    %3815 = torch.prim.ListConstruct %int1_4111, %int64_4112, %int20_4113, %int64_4114 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3816 = torch.aten.view %3776, %3815 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4115 = torch.constant.int 1
    %int2_4116 = torch.constant.int 2
    %3817 = torch.aten.transpose.int %3816, %int1_4115, %int2_4116 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4117 = torch.constant.int 0
    %3818 = torch.aten.clone %3817, %int0_4117 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4118 = torch.constant.int 20
    %int-1_4119 = torch.constant.int -1
    %int64_4120 = torch.constant.int 64
    %3819 = torch.prim.ListConstruct %int20_4118, %int-1_4119, %int64_4120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3820 = torch.aten.view %3818, %3819 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4121 = torch.constant.int 20
    %int-1_4122 = torch.constant.int -1
    %int64_4123 = torch.constant.int 64
    %3821 = torch.prim.ListConstruct %int20_4121, %int-1_4122, %int64_4123 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3822 = torch.aten.view %3795, %3821 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4124 = torch.constant.int 20
    %int-1_4125 = torch.constant.int -1
    %int64_4126 = torch.constant.int 64
    %3823 = torch.prim.ListConstruct %int20_4124, %int-1_4125, %int64_4126 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3824 = torch.aten.view %3814, %3823 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_4127 = torch.constant.int 1
    %int2_4128 = torch.constant.int 2
    %3825 = torch.aten.transpose.int %3822, %int1_4127, %int2_4128 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3826 = torch.aten.bmm %3820, %3825 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4129 = torch.constant.int 1
    %int20_4130 = torch.constant.int 20
    %int64_4131 = torch.constant.int 64
    %int64_4132 = torch.constant.int 64
    %3827 = torch.prim.ListConstruct %int1_4129, %int20_4130, %int64_4131, %int64_4132 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3828 = torch.aten.view %3826, %3827 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4133 = torch.constant.int 1
    %3829 = torch.aten.add.Tensor %3828, %27, %int1_4133 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4134 = torch.constant.int 20
    %int64_4135 = torch.constant.int 64
    %int64_4136 = torch.constant.int 64
    %3830 = torch.prim.ListConstruct %int20_4134, %int64_4135, %int64_4136 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3831 = torch.aten.view %3829, %3830 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_4137 = torch.constant.int -1
    %false_4138 = torch.constant.bool false
    %3832 = torch.aten._softmax %3831, %int-1_4137, %false_4138 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3833 = torch.aten.detach %3832 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_4139 = torch.constant.none
    %3834 = torch.aten.clone %3832, %none_4139 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3835 = torch.aten.bmm %3834, %3824 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4140 = torch.constant.int 1
    %int20_4141 = torch.constant.int 20
    %int64_4142 = torch.constant.int 64
    %int64_4143 = torch.constant.int 64
    %3836 = torch.prim.ListConstruct %int1_4140, %int20_4141, %int64_4142, %int64_4143 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3837 = torch.aten.view %3835, %3836 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4144 = torch.constant.int 1
    %int2_4145 = torch.constant.int 2
    %3838 = torch.aten.transpose.int %3837, %int1_4144, %int2_4145 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_4146 = torch.constant.int 0
    %3839 = torch.aten.clone %3838, %int0_4146 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4147 = torch.constant.int 1
    %int64_4148 = torch.constant.int 64
    %int1280_4149 = torch.constant.int 1280
    %3840 = torch.prim.ListConstruct %int1_4147, %int64_4148, %int1280_4149 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3841 = torch.aten._unsafe_view %3839, %3840 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_4150 = torch.constant.int 64
    %int1280_4151 = torch.constant.int 1280
    %3842 = torch.prim.ListConstruct %int64_4150, %int1280_4151 : (!torch.int, !torch.int) -> !torch.list<int>
    %3843 = torch.aten.view %3841, %3842 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3844 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4152 = torch.constant.int 0
    %int1_4153 = torch.constant.int 1
    %3845 = torch.aten.transpose.int %3844, %int0_4152, %int1_4153 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.bias : tensor<1280xf16>
    %3846 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4154 = torch.constant.int 6
    %3847 = torch.prims.convert_element_type %3846, %int6_4154 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4155 = torch.constant.int 6
    %3848 = torch.prims.convert_element_type %3843, %int6_4155 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4156 = torch.constant.int 6
    %3849 = torch.prims.convert_element_type %3845, %int6_4156 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3850 = torch.aten.mm %3848, %3849 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4157 = torch.constant.int 1
    %3851 = torch.aten.mul.Scalar %3850, %int1_4157 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4158 = torch.constant.int 1
    %3852 = torch.aten.mul.Scalar %3847, %int1_4158 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4159 = torch.constant.int 1
    %3853 = torch.aten.add.Tensor %3851, %3852, %int1_4159 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4160 = torch.constant.int 5
    %3854 = torch.prims.convert_element_type %3853, %int5_4160 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4161 = torch.constant.int 1
    %int64_4162 = torch.constant.int 64
    %int1280_4163 = torch.constant.int 1280
    %3855 = torch.prim.ListConstruct %int1_4161, %int64_4162, %int1280_4163 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3856 = torch.aten.view %3854, %3855 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4164 = torch.constant.int 1
    %3857 = torch.aten.add.Tensor %3747, %3856, %int1_4164 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4165 = torch.constant.int 6
    %3858 = torch.prims.convert_element_type %3857, %int6_4165 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4166 = torch.constant.int 2
    %3859 = torch.prim.ListConstruct %int2_4166 : (!torch.int) -> !torch.list<int>
    %int0_4167 = torch.constant.int 0
    %true_4168 = torch.constant.bool true
    %result0_4169, %result1_4170 = torch.aten.var_mean.correction %3858, %3859, %int0_4167, %true_4168 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4171 = torch.constant.float 1.000000e-05
    %int1_4172 = torch.constant.int 1
    %3860 = torch.aten.add.Scalar %result0_4169, %float1.000000e-05_4171, %int1_4172 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3861 = torch.aten.rsqrt %3860 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4173 = torch.constant.int 1
    %3862 = torch.aten.sub.Tensor %3857, %result1_4170, %int1_4173 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3863 = torch.aten.mul.Tensor %3862, %3861 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.weight : tensor<1280xf16>
    %3864 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3865 = torch.aten.mul.Tensor %3863, %3864 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.bias : tensor<1280xf16>
    %3866 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4174 = torch.constant.int 1
    %3867 = torch.aten.add.Tensor %3865, %3866, %int1_4174 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4175 = torch.constant.int 5
    %3868 = torch.prims.convert_element_type %3867, %int5_4175 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4176 = torch.constant.int 5
    %3869 = torch.prims.convert_element_type %result1_4170, %int5_4176 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4177 = torch.constant.int 5
    %3870 = torch.prims.convert_element_type %3861, %int5_4177 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4178 = torch.constant.int 64
    %int1280_4179 = torch.constant.int 1280
    %3871 = torch.prim.ListConstruct %int64_4178, %int1280_4179 : (!torch.int, !torch.int) -> !torch.list<int>
    %3872 = torch.aten.view %3868, %3871 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.weight : tensor<5120x1280xf16>
    %3873 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_4180 = torch.constant.int 0
    %int1_4181 = torch.constant.int 1
    %3874 = torch.aten.transpose.int %3873, %int0_4180, %int1_4181 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.bias : tensor<5120xf16>
    %3875 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_4182 = torch.constant.int 6
    %3876 = torch.prims.convert_element_type %3875, %int6_4182 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_4183 = torch.constant.int 6
    %3877 = torch.prims.convert_element_type %3872, %int6_4183 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4184 = torch.constant.int 6
    %3878 = torch.prims.convert_element_type %3874, %int6_4184 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %3879 = torch.aten.mm %3877, %3878 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_4185 = torch.constant.int 1
    %3880 = torch.aten.mul.Scalar %3879, %int1_4185 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_4186 = torch.constant.int 1
    %3881 = torch.aten.mul.Scalar %3876, %int1_4186 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_4187 = torch.constant.int 1
    %3882 = torch.aten.add.Tensor %3880, %3881, %int1_4187 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_4188 = torch.constant.int 5
    %3883 = torch.prims.convert_element_type %3882, %int5_4188 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_4189 = torch.constant.int 1
    %int64_4190 = torch.constant.int 64
    %int5120_4191 = torch.constant.int 5120
    %3884 = torch.prim.ListConstruct %int1_4189, %int64_4190, %int5120_4191 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3885 = torch.aten.view %3883, %3884 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_4192 = torch.constant.str "none"
    %3886 = torch.aten.gelu %3885, %str_4192 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_4193 = torch.constant.int 64
    %int5120_4194 = torch.constant.int 5120
    %3887 = torch.prim.ListConstruct %int64_4193, %int5120_4194 : (!torch.int, !torch.int) -> !torch.list<int>
    %3888 = torch.aten.view %3886, %3887 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.weight : tensor<1280x5120xf16>
    %3889 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4195 = torch.constant.int 0
    %int1_4196 = torch.constant.int 1
    %3890 = torch.aten.transpose.int %3889, %int0_4195, %int1_4196 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.bias : tensor<1280xf16>
    %3891 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.24.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4197 = torch.constant.int 6
    %3892 = torch.prims.convert_element_type %3891, %int6_4197 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4198 = torch.constant.int 6
    %3893 = torch.prims.convert_element_type %3888, %int6_4198 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_4199 = torch.constant.int 6
    %3894 = torch.prims.convert_element_type %3890, %int6_4199 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3895 = torch.aten.mm %3893, %3894 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4200 = torch.constant.int 1
    %3896 = torch.aten.mul.Scalar %3895, %int1_4200 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4201 = torch.constant.int 1
    %3897 = torch.aten.mul.Scalar %3892, %int1_4201 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4202 = torch.constant.int 1
    %3898 = torch.aten.add.Tensor %3896, %3897, %int1_4202 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4203 = torch.constant.int 5
    %3899 = torch.prims.convert_element_type %3898, %int5_4203 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4204 = torch.constant.int 1
    %int64_4205 = torch.constant.int 64
    %int1280_4206 = torch.constant.int 1280
    %3900 = torch.prim.ListConstruct %int1_4204, %int64_4205, %int1280_4206 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3901 = torch.aten.view %3899, %3900 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4207 = torch.constant.int 1
    %3902 = torch.aten.add.Tensor %3857, %3901, %int1_4207 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4208 = torch.constant.int 6
    %3903 = torch.prims.convert_element_type %3902, %int6_4208 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4209 = torch.constant.int 2
    %3904 = torch.prim.ListConstruct %int2_4209 : (!torch.int) -> !torch.list<int>
    %int0_4210 = torch.constant.int 0
    %true_4211 = torch.constant.bool true
    %result0_4212, %result1_4213 = torch.aten.var_mean.correction %3903, %3904, %int0_4210, %true_4211 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4214 = torch.constant.float 1.000000e-05
    %int1_4215 = torch.constant.int 1
    %3905 = torch.aten.add.Scalar %result0_4212, %float1.000000e-05_4214, %int1_4215 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %3906 = torch.aten.rsqrt %3905 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4216 = torch.constant.int 1
    %3907 = torch.aten.sub.Tensor %3902, %result1_4213, %int1_4216 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %3908 = torch.aten.mul.Tensor %3907, %3906 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.weight : tensor<1280xf16>
    %3909 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3910 = torch.aten.mul.Tensor %3908, %3909 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.bias : tensor<1280xf16>
    %3911 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4217 = torch.constant.int 1
    %3912 = torch.aten.add.Tensor %3910, %3911, %int1_4217 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4218 = torch.constant.int 5
    %3913 = torch.prims.convert_element_type %3912, %int5_4218 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4219 = torch.constant.int 5
    %3914 = torch.prims.convert_element_type %result1_4213, %int5_4219 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4220 = torch.constant.int 5
    %3915 = torch.prims.convert_element_type %3906, %int5_4220 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4221 = torch.constant.int 64
    %int1280_4222 = torch.constant.int 1280
    %3916 = torch.prim.ListConstruct %int64_4221, %int1280_4222 : (!torch.int, !torch.int) -> !torch.list<int>
    %3917 = torch.aten.view %3913, %3916 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %3918 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4223 = torch.constant.int 0
    %int1_4224 = torch.constant.int 1
    %3919 = torch.aten.transpose.int %3918, %int0_4223, %int1_4224 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.bias : tensor<1280xf16>
    %3920 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4225 = torch.constant.int 6
    %3921 = torch.prims.convert_element_type %3920, %int6_4225 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4226 = torch.constant.int 6
    %3922 = torch.prims.convert_element_type %3917, %int6_4226 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4227 = torch.constant.int 6
    %3923 = torch.prims.convert_element_type %3919, %int6_4227 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3924 = torch.aten.mm %3922, %3923 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4228 = torch.constant.int 1
    %3925 = torch.aten.mul.Scalar %3924, %int1_4228 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4229 = torch.constant.int 1
    %3926 = torch.aten.mul.Scalar %3921, %int1_4229 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4230 = torch.constant.int 1
    %3927 = torch.aten.add.Tensor %3925, %3926, %int1_4230 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4231 = torch.constant.int 5
    %3928 = torch.prims.convert_element_type %3927, %int5_4231 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4232 = torch.constant.int 1
    %int64_4233 = torch.constant.int 64
    %int1280_4234 = torch.constant.int 1280
    %3929 = torch.prim.ListConstruct %int1_4232, %int64_4233, %int1280_4234 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3930 = torch.aten.view %3928, %3929 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_4235 = torch.constant.float 1.250000e-01
    %3931 = torch.aten.mul.Scalar %3930, %float1.250000e-01_4235 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_4236 = torch.constant.int 64
    %int1280_4237 = torch.constant.int 1280
    %3932 = torch.prim.ListConstruct %int64_4236, %int1280_4237 : (!torch.int, !torch.int) -> !torch.list<int>
    %3933 = torch.aten.view %3913, %3932 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %3934 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4238 = torch.constant.int 0
    %int1_4239 = torch.constant.int 1
    %3935 = torch.aten.transpose.int %3934, %int0_4238, %int1_4239 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.bias : tensor<1280xf16>
    %3936 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4240 = torch.constant.int 6
    %3937 = torch.prims.convert_element_type %3936, %int6_4240 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4241 = torch.constant.int 6
    %3938 = torch.prims.convert_element_type %3933, %int6_4241 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4242 = torch.constant.int 6
    %3939 = torch.prims.convert_element_type %3935, %int6_4242 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3940 = torch.aten.mm %3938, %3939 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4243 = torch.constant.int 1
    %3941 = torch.aten.mul.Scalar %3940, %int1_4243 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4244 = torch.constant.int 1
    %3942 = torch.aten.mul.Scalar %3937, %int1_4244 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4245 = torch.constant.int 1
    %3943 = torch.aten.add.Tensor %3941, %3942, %int1_4245 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4246 = torch.constant.int 5
    %3944 = torch.prims.convert_element_type %3943, %int5_4246 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4247 = torch.constant.int 1
    %int64_4248 = torch.constant.int 64
    %int1280_4249 = torch.constant.int 1280
    %3945 = torch.prim.ListConstruct %int1_4247, %int64_4248, %int1280_4249 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3946 = torch.aten.view %3944, %3945 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4250 = torch.constant.int 1
    %int-1_4251 = torch.constant.int -1
    %int20_4252 = torch.constant.int 20
    %int64_4253 = torch.constant.int 64
    %3947 = torch.prim.ListConstruct %int1_4250, %int-1_4251, %int20_4252, %int64_4253 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3948 = torch.aten.view %3946, %3947 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4254 = torch.constant.int 1
    %int2_4255 = torch.constant.int 2
    %3949 = torch.aten.transpose.int %3948, %int1_4254, %int2_4255 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4256 = torch.constant.int 0
    %3950 = torch.aten.clone %3949, %int0_4256 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_4257 = torch.constant.int 64
    %int1280_4258 = torch.constant.int 1280
    %3951 = torch.prim.ListConstruct %int64_4257, %int1280_4258 : (!torch.int, !torch.int) -> !torch.list<int>
    %3952 = torch.aten.view %3913, %3951 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %3953 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4259 = torch.constant.int 0
    %int1_4260 = torch.constant.int 1
    %3954 = torch.aten.transpose.int %3953, %int0_4259, %int1_4260 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.bias : tensor<1280xf16>
    %3955 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4261 = torch.constant.int 6
    %3956 = torch.prims.convert_element_type %3955, %int6_4261 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4262 = torch.constant.int 6
    %3957 = torch.prims.convert_element_type %3952, %int6_4262 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4263 = torch.constant.int 6
    %3958 = torch.prims.convert_element_type %3954, %int6_4263 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3959 = torch.aten.mm %3957, %3958 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4264 = torch.constant.int 1
    %3960 = torch.aten.mul.Scalar %3959, %int1_4264 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4265 = torch.constant.int 1
    %3961 = torch.aten.mul.Scalar %3956, %int1_4265 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4266 = torch.constant.int 1
    %3962 = torch.aten.add.Tensor %3960, %3961, %int1_4266 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4267 = torch.constant.int 5
    %3963 = torch.prims.convert_element_type %3962, %int5_4267 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4268 = torch.constant.int 1
    %int64_4269 = torch.constant.int 64
    %int1280_4270 = torch.constant.int 1280
    %3964 = torch.prim.ListConstruct %int1_4268, %int64_4269, %int1280_4270 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3965 = torch.aten.view %3963, %3964 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4271 = torch.constant.int 1
    %int-1_4272 = torch.constant.int -1
    %int20_4273 = torch.constant.int 20
    %int64_4274 = torch.constant.int 64
    %3966 = torch.prim.ListConstruct %int1_4271, %int-1_4272, %int20_4273, %int64_4274 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3967 = torch.aten.view %3965, %3966 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4275 = torch.constant.int 1
    %int2_4276 = torch.constant.int 2
    %3968 = torch.aten.transpose.int %3967, %int1_4275, %int2_4276 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4277 = torch.constant.int 0
    %3969 = torch.aten.clone %3968, %int0_4277 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4278 = torch.constant.int 1
    %int64_4279 = torch.constant.int 64
    %int20_4280 = torch.constant.int 20
    %int64_4281 = torch.constant.int 64
    %3970 = torch.prim.ListConstruct %int1_4278, %int64_4279, %int20_4280, %int64_4281 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3971 = torch.aten.view %3931, %3970 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4282 = torch.constant.int 1
    %int2_4283 = torch.constant.int 2
    %3972 = torch.aten.transpose.int %3971, %int1_4282, %int2_4283 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4284 = torch.constant.int 0
    %3973 = torch.aten.clone %3972, %int0_4284 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4285 = torch.constant.int 20
    %int-1_4286 = torch.constant.int -1
    %int64_4287 = torch.constant.int 64
    %3974 = torch.prim.ListConstruct %int20_4285, %int-1_4286, %int64_4287 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3975 = torch.aten.view %3973, %3974 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4288 = torch.constant.int 20
    %int-1_4289 = torch.constant.int -1
    %int64_4290 = torch.constant.int 64
    %3976 = torch.prim.ListConstruct %int20_4288, %int-1_4289, %int64_4290 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3977 = torch.aten.view %3950, %3976 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4291 = torch.constant.int 20
    %int-1_4292 = torch.constant.int -1
    %int64_4293 = torch.constant.int 64
    %3978 = torch.prim.ListConstruct %int20_4291, %int-1_4292, %int64_4293 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3979 = torch.aten.view %3969, %3978 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_4294 = torch.constant.int 1
    %int2_4295 = torch.constant.int 2
    %3980 = torch.aten.transpose.int %3977, %int1_4294, %int2_4295 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %3981 = torch.aten.bmm %3975, %3980 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4296 = torch.constant.int 1
    %int20_4297 = torch.constant.int 20
    %int64_4298 = torch.constant.int 64
    %int64_4299 = torch.constant.int 64
    %3982 = torch.prim.ListConstruct %int1_4296, %int20_4297, %int64_4298, %int64_4299 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3983 = torch.aten.view %3981, %3982 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4300 = torch.constant.int 1
    %3984 = torch.aten.add.Tensor %3983, %27, %int1_4300 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4301 = torch.constant.int 20
    %int64_4302 = torch.constant.int 64
    %int64_4303 = torch.constant.int 64
    %3985 = torch.prim.ListConstruct %int20_4301, %int64_4302, %int64_4303 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3986 = torch.aten.view %3984, %3985 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_4304 = torch.constant.int -1
    %false_4305 = torch.constant.bool false
    %3987 = torch.aten._softmax %3986, %int-1_4304, %false_4305 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %3988 = torch.aten.detach %3987 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_4306 = torch.constant.none
    %3989 = torch.aten.clone %3987, %none_4306 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %3990 = torch.aten.bmm %3989, %3979 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4307 = torch.constant.int 1
    %int20_4308 = torch.constant.int 20
    %int64_4309 = torch.constant.int 64
    %int64_4310 = torch.constant.int 64
    %3991 = torch.prim.ListConstruct %int1_4307, %int20_4308, %int64_4309, %int64_4310 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3992 = torch.aten.view %3990, %3991 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4311 = torch.constant.int 1
    %int2_4312 = torch.constant.int 2
    %3993 = torch.aten.transpose.int %3992, %int1_4311, %int2_4312 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_4313 = torch.constant.int 0
    %3994 = torch.aten.clone %3993, %int0_4313 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4314 = torch.constant.int 1
    %int64_4315 = torch.constant.int 64
    %int1280_4316 = torch.constant.int 1280
    %3995 = torch.prim.ListConstruct %int1_4314, %int64_4315, %int1280_4316 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3996 = torch.aten._unsafe_view %3994, %3995 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_4317 = torch.constant.int 64
    %int1280_4318 = torch.constant.int 1280
    %3997 = torch.prim.ListConstruct %int64_4317, %int1280_4318 : (!torch.int, !torch.int) -> !torch.list<int>
    %3998 = torch.aten.view %3996, %3997 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %3999 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4319 = torch.constant.int 0
    %int1_4320 = torch.constant.int 1
    %4000 = torch.aten.transpose.int %3999, %int0_4319, %int1_4320 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.bias : tensor<1280xf16>
    %4001 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4321 = torch.constant.int 6
    %4002 = torch.prims.convert_element_type %4001, %int6_4321 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4322 = torch.constant.int 6
    %4003 = torch.prims.convert_element_type %3998, %int6_4322 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4323 = torch.constant.int 6
    %4004 = torch.prims.convert_element_type %4000, %int6_4323 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4005 = torch.aten.mm %4003, %4004 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4324 = torch.constant.int 1
    %4006 = torch.aten.mul.Scalar %4005, %int1_4324 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4325 = torch.constant.int 1
    %4007 = torch.aten.mul.Scalar %4002, %int1_4325 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4326 = torch.constant.int 1
    %4008 = torch.aten.add.Tensor %4006, %4007, %int1_4326 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4327 = torch.constant.int 5
    %4009 = torch.prims.convert_element_type %4008, %int5_4327 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4328 = torch.constant.int 1
    %int64_4329 = torch.constant.int 64
    %int1280_4330 = torch.constant.int 1280
    %4010 = torch.prim.ListConstruct %int1_4328, %int64_4329, %int1280_4330 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4011 = torch.aten.view %4009, %4010 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4331 = torch.constant.int 1
    %4012 = torch.aten.add.Tensor %3902, %4011, %int1_4331 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4332 = torch.constant.int 6
    %4013 = torch.prims.convert_element_type %4012, %int6_4332 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4333 = torch.constant.int 2
    %4014 = torch.prim.ListConstruct %int2_4333 : (!torch.int) -> !torch.list<int>
    %int0_4334 = torch.constant.int 0
    %true_4335 = torch.constant.bool true
    %result0_4336, %result1_4337 = torch.aten.var_mean.correction %4013, %4014, %int0_4334, %true_4335 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4338 = torch.constant.float 1.000000e-05
    %int1_4339 = torch.constant.int 1
    %4015 = torch.aten.add.Scalar %result0_4336, %float1.000000e-05_4338, %int1_4339 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4016 = torch.aten.rsqrt %4015 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4340 = torch.constant.int 1
    %4017 = torch.aten.sub.Tensor %4012, %result1_4337, %int1_4340 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4018 = torch.aten.mul.Tensor %4017, %4016 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.weight : tensor<1280xf16>
    %4019 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4020 = torch.aten.mul.Tensor %4018, %4019 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.bias : tensor<1280xf16>
    %4021 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4341 = torch.constant.int 1
    %4022 = torch.aten.add.Tensor %4020, %4021, %int1_4341 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4342 = torch.constant.int 5
    %4023 = torch.prims.convert_element_type %4022, %int5_4342 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4343 = torch.constant.int 5
    %4024 = torch.prims.convert_element_type %result1_4337, %int5_4343 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4344 = torch.constant.int 5
    %4025 = torch.prims.convert_element_type %4016, %int5_4344 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4345 = torch.constant.int 64
    %int1280_4346 = torch.constant.int 1280
    %4026 = torch.prim.ListConstruct %int64_4345, %int1280_4346 : (!torch.int, !torch.int) -> !torch.list<int>
    %4027 = torch.aten.view %4023, %4026 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.weight : tensor<5120x1280xf16>
    %4028 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_4347 = torch.constant.int 0
    %int1_4348 = torch.constant.int 1
    %4029 = torch.aten.transpose.int %4028, %int0_4347, %int1_4348 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.bias : tensor<5120xf16>
    %4030 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_4349 = torch.constant.int 6
    %4031 = torch.prims.convert_element_type %4030, %int6_4349 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_4350 = torch.constant.int 6
    %4032 = torch.prims.convert_element_type %4027, %int6_4350 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4351 = torch.constant.int 6
    %4033 = torch.prims.convert_element_type %4029, %int6_4351 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4034 = torch.aten.mm %4032, %4033 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_4352 = torch.constant.int 1
    %4035 = torch.aten.mul.Scalar %4034, %int1_4352 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_4353 = torch.constant.int 1
    %4036 = torch.aten.mul.Scalar %4031, %int1_4353 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_4354 = torch.constant.int 1
    %4037 = torch.aten.add.Tensor %4035, %4036, %int1_4354 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_4355 = torch.constant.int 5
    %4038 = torch.prims.convert_element_type %4037, %int5_4355 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_4356 = torch.constant.int 1
    %int64_4357 = torch.constant.int 64
    %int5120_4358 = torch.constant.int 5120
    %4039 = torch.prim.ListConstruct %int1_4356, %int64_4357, %int5120_4358 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4040 = torch.aten.view %4038, %4039 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_4359 = torch.constant.str "none"
    %4041 = torch.aten.gelu %4040, %str_4359 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_4360 = torch.constant.int 64
    %int5120_4361 = torch.constant.int 5120
    %4042 = torch.prim.ListConstruct %int64_4360, %int5120_4361 : (!torch.int, !torch.int) -> !torch.list<int>
    %4043 = torch.aten.view %4041, %4042 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.weight : tensor<1280x5120xf16>
    %4044 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4362 = torch.constant.int 0
    %int1_4363 = torch.constant.int 1
    %4045 = torch.aten.transpose.int %4044, %int0_4362, %int1_4363 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.bias : tensor<1280xf16>
    %4046 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.25.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4364 = torch.constant.int 6
    %4047 = torch.prims.convert_element_type %4046, %int6_4364 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4365 = torch.constant.int 6
    %4048 = torch.prims.convert_element_type %4043, %int6_4365 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_4366 = torch.constant.int 6
    %4049 = torch.prims.convert_element_type %4045, %int6_4366 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4050 = torch.aten.mm %4048, %4049 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4367 = torch.constant.int 1
    %4051 = torch.aten.mul.Scalar %4050, %int1_4367 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4368 = torch.constant.int 1
    %4052 = torch.aten.mul.Scalar %4047, %int1_4368 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4369 = torch.constant.int 1
    %4053 = torch.aten.add.Tensor %4051, %4052, %int1_4369 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4370 = torch.constant.int 5
    %4054 = torch.prims.convert_element_type %4053, %int5_4370 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4371 = torch.constant.int 1
    %int64_4372 = torch.constant.int 64
    %int1280_4373 = torch.constant.int 1280
    %4055 = torch.prim.ListConstruct %int1_4371, %int64_4372, %int1280_4373 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4056 = torch.aten.view %4054, %4055 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4374 = torch.constant.int 1
    %4057 = torch.aten.add.Tensor %4012, %4056, %int1_4374 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4375 = torch.constant.int 6
    %4058 = torch.prims.convert_element_type %4057, %int6_4375 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4376 = torch.constant.int 2
    %4059 = torch.prim.ListConstruct %int2_4376 : (!torch.int) -> !torch.list<int>
    %int0_4377 = torch.constant.int 0
    %true_4378 = torch.constant.bool true
    %result0_4379, %result1_4380 = torch.aten.var_mean.correction %4058, %4059, %int0_4377, %true_4378 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4381 = torch.constant.float 1.000000e-05
    %int1_4382 = torch.constant.int 1
    %4060 = torch.aten.add.Scalar %result0_4379, %float1.000000e-05_4381, %int1_4382 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4061 = torch.aten.rsqrt %4060 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4383 = torch.constant.int 1
    %4062 = torch.aten.sub.Tensor %4057, %result1_4380, %int1_4383 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4063 = torch.aten.mul.Tensor %4062, %4061 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.weight : tensor<1280xf16>
    %4064 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4065 = torch.aten.mul.Tensor %4063, %4064 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.bias : tensor<1280xf16>
    %4066 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4384 = torch.constant.int 1
    %4067 = torch.aten.add.Tensor %4065, %4066, %int1_4384 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4385 = torch.constant.int 5
    %4068 = torch.prims.convert_element_type %4067, %int5_4385 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4386 = torch.constant.int 5
    %4069 = torch.prims.convert_element_type %result1_4380, %int5_4386 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4387 = torch.constant.int 5
    %4070 = torch.prims.convert_element_type %4061, %int5_4387 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4388 = torch.constant.int 64
    %int1280_4389 = torch.constant.int 1280
    %4071 = torch.prim.ListConstruct %int64_4388, %int1280_4389 : (!torch.int, !torch.int) -> !torch.list<int>
    %4072 = torch.aten.view %4068, %4071 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %4073 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4390 = torch.constant.int 0
    %int1_4391 = torch.constant.int 1
    %4074 = torch.aten.transpose.int %4073, %int0_4390, %int1_4391 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.bias : tensor<1280xf16>
    %4075 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4392 = torch.constant.int 6
    %4076 = torch.prims.convert_element_type %4075, %int6_4392 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4393 = torch.constant.int 6
    %4077 = torch.prims.convert_element_type %4072, %int6_4393 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4394 = torch.constant.int 6
    %4078 = torch.prims.convert_element_type %4074, %int6_4394 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4079 = torch.aten.mm %4077, %4078 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4395 = torch.constant.int 1
    %4080 = torch.aten.mul.Scalar %4079, %int1_4395 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4396 = torch.constant.int 1
    %4081 = torch.aten.mul.Scalar %4076, %int1_4396 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4397 = torch.constant.int 1
    %4082 = torch.aten.add.Tensor %4080, %4081, %int1_4397 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4398 = torch.constant.int 5
    %4083 = torch.prims.convert_element_type %4082, %int5_4398 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4399 = torch.constant.int 1
    %int64_4400 = torch.constant.int 64
    %int1280_4401 = torch.constant.int 1280
    %4084 = torch.prim.ListConstruct %int1_4399, %int64_4400, %int1280_4401 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4085 = torch.aten.view %4083, %4084 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_4402 = torch.constant.float 1.250000e-01
    %4086 = torch.aten.mul.Scalar %4085, %float1.250000e-01_4402 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_4403 = torch.constant.int 64
    %int1280_4404 = torch.constant.int 1280
    %4087 = torch.prim.ListConstruct %int64_4403, %int1280_4404 : (!torch.int, !torch.int) -> !torch.list<int>
    %4088 = torch.aten.view %4068, %4087 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %4089 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4405 = torch.constant.int 0
    %int1_4406 = torch.constant.int 1
    %4090 = torch.aten.transpose.int %4089, %int0_4405, %int1_4406 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.bias : tensor<1280xf16>
    %4091 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4407 = torch.constant.int 6
    %4092 = torch.prims.convert_element_type %4091, %int6_4407 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4408 = torch.constant.int 6
    %4093 = torch.prims.convert_element_type %4088, %int6_4408 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4409 = torch.constant.int 6
    %4094 = torch.prims.convert_element_type %4090, %int6_4409 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4095 = torch.aten.mm %4093, %4094 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4410 = torch.constant.int 1
    %4096 = torch.aten.mul.Scalar %4095, %int1_4410 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4411 = torch.constant.int 1
    %4097 = torch.aten.mul.Scalar %4092, %int1_4411 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4412 = torch.constant.int 1
    %4098 = torch.aten.add.Tensor %4096, %4097, %int1_4412 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4413 = torch.constant.int 5
    %4099 = torch.prims.convert_element_type %4098, %int5_4413 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4414 = torch.constant.int 1
    %int64_4415 = torch.constant.int 64
    %int1280_4416 = torch.constant.int 1280
    %4100 = torch.prim.ListConstruct %int1_4414, %int64_4415, %int1280_4416 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4101 = torch.aten.view %4099, %4100 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4417 = torch.constant.int 1
    %int-1_4418 = torch.constant.int -1
    %int20_4419 = torch.constant.int 20
    %int64_4420 = torch.constant.int 64
    %4102 = torch.prim.ListConstruct %int1_4417, %int-1_4418, %int20_4419, %int64_4420 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4103 = torch.aten.view %4101, %4102 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4421 = torch.constant.int 1
    %int2_4422 = torch.constant.int 2
    %4104 = torch.aten.transpose.int %4103, %int1_4421, %int2_4422 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4423 = torch.constant.int 0
    %4105 = torch.aten.clone %4104, %int0_4423 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_4424 = torch.constant.int 64
    %int1280_4425 = torch.constant.int 1280
    %4106 = torch.prim.ListConstruct %int64_4424, %int1280_4425 : (!torch.int, !torch.int) -> !torch.list<int>
    %4107 = torch.aten.view %4068, %4106 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %4108 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4426 = torch.constant.int 0
    %int1_4427 = torch.constant.int 1
    %4109 = torch.aten.transpose.int %4108, %int0_4426, %int1_4427 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.bias : tensor<1280xf16>
    %4110 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4428 = torch.constant.int 6
    %4111 = torch.prims.convert_element_type %4110, %int6_4428 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4429 = torch.constant.int 6
    %4112 = torch.prims.convert_element_type %4107, %int6_4429 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4430 = torch.constant.int 6
    %4113 = torch.prims.convert_element_type %4109, %int6_4430 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4114 = torch.aten.mm %4112, %4113 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4431 = torch.constant.int 1
    %4115 = torch.aten.mul.Scalar %4114, %int1_4431 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4432 = torch.constant.int 1
    %4116 = torch.aten.mul.Scalar %4111, %int1_4432 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4433 = torch.constant.int 1
    %4117 = torch.aten.add.Tensor %4115, %4116, %int1_4433 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4434 = torch.constant.int 5
    %4118 = torch.prims.convert_element_type %4117, %int5_4434 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4435 = torch.constant.int 1
    %int64_4436 = torch.constant.int 64
    %int1280_4437 = torch.constant.int 1280
    %4119 = torch.prim.ListConstruct %int1_4435, %int64_4436, %int1280_4437 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4120 = torch.aten.view %4118, %4119 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4438 = torch.constant.int 1
    %int-1_4439 = torch.constant.int -1
    %int20_4440 = torch.constant.int 20
    %int64_4441 = torch.constant.int 64
    %4121 = torch.prim.ListConstruct %int1_4438, %int-1_4439, %int20_4440, %int64_4441 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4122 = torch.aten.view %4120, %4121 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4442 = torch.constant.int 1
    %int2_4443 = torch.constant.int 2
    %4123 = torch.aten.transpose.int %4122, %int1_4442, %int2_4443 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4444 = torch.constant.int 0
    %4124 = torch.aten.clone %4123, %int0_4444 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4445 = torch.constant.int 1
    %int64_4446 = torch.constant.int 64
    %int20_4447 = torch.constant.int 20
    %int64_4448 = torch.constant.int 64
    %4125 = torch.prim.ListConstruct %int1_4445, %int64_4446, %int20_4447, %int64_4448 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4126 = torch.aten.view %4086, %4125 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4449 = torch.constant.int 1
    %int2_4450 = torch.constant.int 2
    %4127 = torch.aten.transpose.int %4126, %int1_4449, %int2_4450 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4451 = torch.constant.int 0
    %4128 = torch.aten.clone %4127, %int0_4451 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4452 = torch.constant.int 20
    %int-1_4453 = torch.constant.int -1
    %int64_4454 = torch.constant.int 64
    %4129 = torch.prim.ListConstruct %int20_4452, %int-1_4453, %int64_4454 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4130 = torch.aten.view %4128, %4129 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4455 = torch.constant.int 20
    %int-1_4456 = torch.constant.int -1
    %int64_4457 = torch.constant.int 64
    %4131 = torch.prim.ListConstruct %int20_4455, %int-1_4456, %int64_4457 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4132 = torch.aten.view %4105, %4131 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4458 = torch.constant.int 20
    %int-1_4459 = torch.constant.int -1
    %int64_4460 = torch.constant.int 64
    %4133 = torch.prim.ListConstruct %int20_4458, %int-1_4459, %int64_4460 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4134 = torch.aten.view %4124, %4133 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_4461 = torch.constant.int 1
    %int2_4462 = torch.constant.int 2
    %4135 = torch.aten.transpose.int %4132, %int1_4461, %int2_4462 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %4136 = torch.aten.bmm %4130, %4135 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4463 = torch.constant.int 1
    %int20_4464 = torch.constant.int 20
    %int64_4465 = torch.constant.int 64
    %int64_4466 = torch.constant.int 64
    %4137 = torch.prim.ListConstruct %int1_4463, %int20_4464, %int64_4465, %int64_4466 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4138 = torch.aten.view %4136, %4137 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4467 = torch.constant.int 1
    %4139 = torch.aten.add.Tensor %4138, %27, %int1_4467 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4468 = torch.constant.int 20
    %int64_4469 = torch.constant.int 64
    %int64_4470 = torch.constant.int 64
    %4140 = torch.prim.ListConstruct %int20_4468, %int64_4469, %int64_4470 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4141 = torch.aten.view %4139, %4140 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_4471 = torch.constant.int -1
    %false_4472 = torch.constant.bool false
    %4142 = torch.aten._softmax %4141, %int-1_4471, %false_4472 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %4143 = torch.aten.detach %4142 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_4473 = torch.constant.none
    %4144 = torch.aten.clone %4142, %none_4473 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %4145 = torch.aten.bmm %4144, %4134 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4474 = torch.constant.int 1
    %int20_4475 = torch.constant.int 20
    %int64_4476 = torch.constant.int 64
    %int64_4477 = torch.constant.int 64
    %4146 = torch.prim.ListConstruct %int1_4474, %int20_4475, %int64_4476, %int64_4477 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4147 = torch.aten.view %4145, %4146 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4478 = torch.constant.int 1
    %int2_4479 = torch.constant.int 2
    %4148 = torch.aten.transpose.int %4147, %int1_4478, %int2_4479 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_4480 = torch.constant.int 0
    %4149 = torch.aten.clone %4148, %int0_4480 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4481 = torch.constant.int 1
    %int64_4482 = torch.constant.int 64
    %int1280_4483 = torch.constant.int 1280
    %4150 = torch.prim.ListConstruct %int1_4481, %int64_4482, %int1280_4483 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4151 = torch.aten._unsafe_view %4149, %4150 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_4484 = torch.constant.int 64
    %int1280_4485 = torch.constant.int 1280
    %4152 = torch.prim.ListConstruct %int64_4484, %int1280_4485 : (!torch.int, !torch.int) -> !torch.list<int>
    %4153 = torch.aten.view %4151, %4152 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %4154 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4486 = torch.constant.int 0
    %int1_4487 = torch.constant.int 1
    %4155 = torch.aten.transpose.int %4154, %int0_4486, %int1_4487 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.bias : tensor<1280xf16>
    %4156 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4488 = torch.constant.int 6
    %4157 = torch.prims.convert_element_type %4156, %int6_4488 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4489 = torch.constant.int 6
    %4158 = torch.prims.convert_element_type %4153, %int6_4489 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4490 = torch.constant.int 6
    %4159 = torch.prims.convert_element_type %4155, %int6_4490 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4160 = torch.aten.mm %4158, %4159 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4491 = torch.constant.int 1
    %4161 = torch.aten.mul.Scalar %4160, %int1_4491 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4492 = torch.constant.int 1
    %4162 = torch.aten.mul.Scalar %4157, %int1_4492 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4493 = torch.constant.int 1
    %4163 = torch.aten.add.Tensor %4161, %4162, %int1_4493 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4494 = torch.constant.int 5
    %4164 = torch.prims.convert_element_type %4163, %int5_4494 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4495 = torch.constant.int 1
    %int64_4496 = torch.constant.int 64
    %int1280_4497 = torch.constant.int 1280
    %4165 = torch.prim.ListConstruct %int1_4495, %int64_4496, %int1280_4497 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4166 = torch.aten.view %4164, %4165 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4498 = torch.constant.int 1
    %4167 = torch.aten.add.Tensor %4057, %4166, %int1_4498 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4499 = torch.constant.int 6
    %4168 = torch.prims.convert_element_type %4167, %int6_4499 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4500 = torch.constant.int 2
    %4169 = torch.prim.ListConstruct %int2_4500 : (!torch.int) -> !torch.list<int>
    %int0_4501 = torch.constant.int 0
    %true_4502 = torch.constant.bool true
    %result0_4503, %result1_4504 = torch.aten.var_mean.correction %4168, %4169, %int0_4501, %true_4502 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4505 = torch.constant.float 1.000000e-05
    %int1_4506 = torch.constant.int 1
    %4170 = torch.aten.add.Scalar %result0_4503, %float1.000000e-05_4505, %int1_4506 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4171 = torch.aten.rsqrt %4170 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4507 = torch.constant.int 1
    %4172 = torch.aten.sub.Tensor %4167, %result1_4504, %int1_4507 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4173 = torch.aten.mul.Tensor %4172, %4171 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.weight : tensor<1280xf16>
    %4174 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4175 = torch.aten.mul.Tensor %4173, %4174 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.bias : tensor<1280xf16>
    %4176 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4508 = torch.constant.int 1
    %4177 = torch.aten.add.Tensor %4175, %4176, %int1_4508 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4509 = torch.constant.int 5
    %4178 = torch.prims.convert_element_type %4177, %int5_4509 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4510 = torch.constant.int 5
    %4179 = torch.prims.convert_element_type %result1_4504, %int5_4510 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4511 = torch.constant.int 5
    %4180 = torch.prims.convert_element_type %4171, %int5_4511 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4512 = torch.constant.int 64
    %int1280_4513 = torch.constant.int 1280
    %4181 = torch.prim.ListConstruct %int64_4512, %int1280_4513 : (!torch.int, !torch.int) -> !torch.list<int>
    %4182 = torch.aten.view %4178, %4181 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.weight : tensor<5120x1280xf16>
    %4183 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_4514 = torch.constant.int 0
    %int1_4515 = torch.constant.int 1
    %4184 = torch.aten.transpose.int %4183, %int0_4514, %int1_4515 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.bias : tensor<5120xf16>
    %4185 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_4516 = torch.constant.int 6
    %4186 = torch.prims.convert_element_type %4185, %int6_4516 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_4517 = torch.constant.int 6
    %4187 = torch.prims.convert_element_type %4182, %int6_4517 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4518 = torch.constant.int 6
    %4188 = torch.prims.convert_element_type %4184, %int6_4518 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4189 = torch.aten.mm %4187, %4188 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_4519 = torch.constant.int 1
    %4190 = torch.aten.mul.Scalar %4189, %int1_4519 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_4520 = torch.constant.int 1
    %4191 = torch.aten.mul.Scalar %4186, %int1_4520 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_4521 = torch.constant.int 1
    %4192 = torch.aten.add.Tensor %4190, %4191, %int1_4521 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_4522 = torch.constant.int 5
    %4193 = torch.prims.convert_element_type %4192, %int5_4522 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_4523 = torch.constant.int 1
    %int64_4524 = torch.constant.int 64
    %int5120_4525 = torch.constant.int 5120
    %4194 = torch.prim.ListConstruct %int1_4523, %int64_4524, %int5120_4525 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4195 = torch.aten.view %4193, %4194 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_4526 = torch.constant.str "none"
    %4196 = torch.aten.gelu %4195, %str_4526 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_4527 = torch.constant.int 64
    %int5120_4528 = torch.constant.int 5120
    %4197 = torch.prim.ListConstruct %int64_4527, %int5120_4528 : (!torch.int, !torch.int) -> !torch.list<int>
    %4198 = torch.aten.view %4196, %4197 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.weight : tensor<1280x5120xf16>
    %4199 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4529 = torch.constant.int 0
    %int1_4530 = torch.constant.int 1
    %4200 = torch.aten.transpose.int %4199, %int0_4529, %int1_4530 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.bias : tensor<1280xf16>
    %4201 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.26.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4531 = torch.constant.int 6
    %4202 = torch.prims.convert_element_type %4201, %int6_4531 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4532 = torch.constant.int 6
    %4203 = torch.prims.convert_element_type %4198, %int6_4532 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_4533 = torch.constant.int 6
    %4204 = torch.prims.convert_element_type %4200, %int6_4533 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4205 = torch.aten.mm %4203, %4204 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4534 = torch.constant.int 1
    %4206 = torch.aten.mul.Scalar %4205, %int1_4534 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4535 = torch.constant.int 1
    %4207 = torch.aten.mul.Scalar %4202, %int1_4535 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4536 = torch.constant.int 1
    %4208 = torch.aten.add.Tensor %4206, %4207, %int1_4536 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4537 = torch.constant.int 5
    %4209 = torch.prims.convert_element_type %4208, %int5_4537 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4538 = torch.constant.int 1
    %int64_4539 = torch.constant.int 64
    %int1280_4540 = torch.constant.int 1280
    %4210 = torch.prim.ListConstruct %int1_4538, %int64_4539, %int1280_4540 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4211 = torch.aten.view %4209, %4210 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4541 = torch.constant.int 1
    %4212 = torch.aten.add.Tensor %4167, %4211, %int1_4541 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4542 = torch.constant.int 6
    %4213 = torch.prims.convert_element_type %4212, %int6_4542 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4543 = torch.constant.int 2
    %4214 = torch.prim.ListConstruct %int2_4543 : (!torch.int) -> !torch.list<int>
    %int0_4544 = torch.constant.int 0
    %true_4545 = torch.constant.bool true
    %result0_4546, %result1_4547 = torch.aten.var_mean.correction %4213, %4214, %int0_4544, %true_4545 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4548 = torch.constant.float 1.000000e-05
    %int1_4549 = torch.constant.int 1
    %4215 = torch.aten.add.Scalar %result0_4546, %float1.000000e-05_4548, %int1_4549 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4216 = torch.aten.rsqrt %4215 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4550 = torch.constant.int 1
    %4217 = torch.aten.sub.Tensor %4212, %result1_4547, %int1_4550 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4218 = torch.aten.mul.Tensor %4217, %4216 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.weight : tensor<1280xf16>
    %4219 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4220 = torch.aten.mul.Tensor %4218, %4219 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.bias : tensor<1280xf16>
    %4221 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4551 = torch.constant.int 1
    %4222 = torch.aten.add.Tensor %4220, %4221, %int1_4551 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4552 = torch.constant.int 5
    %4223 = torch.prims.convert_element_type %4222, %int5_4552 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4553 = torch.constant.int 5
    %4224 = torch.prims.convert_element_type %result1_4547, %int5_4553 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4554 = torch.constant.int 5
    %4225 = torch.prims.convert_element_type %4216, %int5_4554 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4555 = torch.constant.int 64
    %int1280_4556 = torch.constant.int 1280
    %4226 = torch.prim.ListConstruct %int64_4555, %int1280_4556 : (!torch.int, !torch.int) -> !torch.list<int>
    %4227 = torch.aten.view %4223, %4226 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %4228 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4557 = torch.constant.int 0
    %int1_4558 = torch.constant.int 1
    %4229 = torch.aten.transpose.int %4228, %int0_4557, %int1_4558 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.bias : tensor<1280xf16>
    %4230 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4559 = torch.constant.int 6
    %4231 = torch.prims.convert_element_type %4230, %int6_4559 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4560 = torch.constant.int 6
    %4232 = torch.prims.convert_element_type %4227, %int6_4560 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4561 = torch.constant.int 6
    %4233 = torch.prims.convert_element_type %4229, %int6_4561 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4234 = torch.aten.mm %4232, %4233 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4562 = torch.constant.int 1
    %4235 = torch.aten.mul.Scalar %4234, %int1_4562 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4563 = torch.constant.int 1
    %4236 = torch.aten.mul.Scalar %4231, %int1_4563 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4564 = torch.constant.int 1
    %4237 = torch.aten.add.Tensor %4235, %4236, %int1_4564 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4565 = torch.constant.int 5
    %4238 = torch.prims.convert_element_type %4237, %int5_4565 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4566 = torch.constant.int 1
    %int64_4567 = torch.constant.int 64
    %int1280_4568 = torch.constant.int 1280
    %4239 = torch.prim.ListConstruct %int1_4566, %int64_4567, %int1280_4568 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4240 = torch.aten.view %4238, %4239 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_4569 = torch.constant.float 1.250000e-01
    %4241 = torch.aten.mul.Scalar %4240, %float1.250000e-01_4569 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_4570 = torch.constant.int 64
    %int1280_4571 = torch.constant.int 1280
    %4242 = torch.prim.ListConstruct %int64_4570, %int1280_4571 : (!torch.int, !torch.int) -> !torch.list<int>
    %4243 = torch.aten.view %4223, %4242 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %4244 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4572 = torch.constant.int 0
    %int1_4573 = torch.constant.int 1
    %4245 = torch.aten.transpose.int %4244, %int0_4572, %int1_4573 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.bias : tensor<1280xf16>
    %4246 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4574 = torch.constant.int 6
    %4247 = torch.prims.convert_element_type %4246, %int6_4574 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4575 = torch.constant.int 6
    %4248 = torch.prims.convert_element_type %4243, %int6_4575 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4576 = torch.constant.int 6
    %4249 = torch.prims.convert_element_type %4245, %int6_4576 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4250 = torch.aten.mm %4248, %4249 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4577 = torch.constant.int 1
    %4251 = torch.aten.mul.Scalar %4250, %int1_4577 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4578 = torch.constant.int 1
    %4252 = torch.aten.mul.Scalar %4247, %int1_4578 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4579 = torch.constant.int 1
    %4253 = torch.aten.add.Tensor %4251, %4252, %int1_4579 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4580 = torch.constant.int 5
    %4254 = torch.prims.convert_element_type %4253, %int5_4580 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4581 = torch.constant.int 1
    %int64_4582 = torch.constant.int 64
    %int1280_4583 = torch.constant.int 1280
    %4255 = torch.prim.ListConstruct %int1_4581, %int64_4582, %int1280_4583 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4256 = torch.aten.view %4254, %4255 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4584 = torch.constant.int 1
    %int-1_4585 = torch.constant.int -1
    %int20_4586 = torch.constant.int 20
    %int64_4587 = torch.constant.int 64
    %4257 = torch.prim.ListConstruct %int1_4584, %int-1_4585, %int20_4586, %int64_4587 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4258 = torch.aten.view %4256, %4257 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4588 = torch.constant.int 1
    %int2_4589 = torch.constant.int 2
    %4259 = torch.aten.transpose.int %4258, %int1_4588, %int2_4589 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4590 = torch.constant.int 0
    %4260 = torch.aten.clone %4259, %int0_4590 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_4591 = torch.constant.int 64
    %int1280_4592 = torch.constant.int 1280
    %4261 = torch.prim.ListConstruct %int64_4591, %int1280_4592 : (!torch.int, !torch.int) -> !torch.list<int>
    %4262 = torch.aten.view %4223, %4261 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %4263 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4593 = torch.constant.int 0
    %int1_4594 = torch.constant.int 1
    %4264 = torch.aten.transpose.int %4263, %int0_4593, %int1_4594 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.bias : tensor<1280xf16>
    %4265 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4595 = torch.constant.int 6
    %4266 = torch.prims.convert_element_type %4265, %int6_4595 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4596 = torch.constant.int 6
    %4267 = torch.prims.convert_element_type %4262, %int6_4596 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4597 = torch.constant.int 6
    %4268 = torch.prims.convert_element_type %4264, %int6_4597 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4269 = torch.aten.mm %4267, %4268 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4598 = torch.constant.int 1
    %4270 = torch.aten.mul.Scalar %4269, %int1_4598 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4599 = torch.constant.int 1
    %4271 = torch.aten.mul.Scalar %4266, %int1_4599 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4600 = torch.constant.int 1
    %4272 = torch.aten.add.Tensor %4270, %4271, %int1_4600 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4601 = torch.constant.int 5
    %4273 = torch.prims.convert_element_type %4272, %int5_4601 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4602 = torch.constant.int 1
    %int64_4603 = torch.constant.int 64
    %int1280_4604 = torch.constant.int 1280
    %4274 = torch.prim.ListConstruct %int1_4602, %int64_4603, %int1280_4604 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4275 = torch.aten.view %4273, %4274 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4605 = torch.constant.int 1
    %int-1_4606 = torch.constant.int -1
    %int20_4607 = torch.constant.int 20
    %int64_4608 = torch.constant.int 64
    %4276 = torch.prim.ListConstruct %int1_4605, %int-1_4606, %int20_4607, %int64_4608 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4277 = torch.aten.view %4275, %4276 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4609 = torch.constant.int 1
    %int2_4610 = torch.constant.int 2
    %4278 = torch.aten.transpose.int %4277, %int1_4609, %int2_4610 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4611 = torch.constant.int 0
    %4279 = torch.aten.clone %4278, %int0_4611 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4612 = torch.constant.int 1
    %int64_4613 = torch.constant.int 64
    %int20_4614 = torch.constant.int 20
    %int64_4615 = torch.constant.int 64
    %4280 = torch.prim.ListConstruct %int1_4612, %int64_4613, %int20_4614, %int64_4615 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4281 = torch.aten.view %4241, %4280 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4616 = torch.constant.int 1
    %int2_4617 = torch.constant.int 2
    %4282 = torch.aten.transpose.int %4281, %int1_4616, %int2_4617 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4618 = torch.constant.int 0
    %4283 = torch.aten.clone %4282, %int0_4618 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4619 = torch.constant.int 20
    %int-1_4620 = torch.constant.int -1
    %int64_4621 = torch.constant.int 64
    %4284 = torch.prim.ListConstruct %int20_4619, %int-1_4620, %int64_4621 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4285 = torch.aten.view %4283, %4284 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4622 = torch.constant.int 20
    %int-1_4623 = torch.constant.int -1
    %int64_4624 = torch.constant.int 64
    %4286 = torch.prim.ListConstruct %int20_4622, %int-1_4623, %int64_4624 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4287 = torch.aten.view %4260, %4286 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4625 = torch.constant.int 20
    %int-1_4626 = torch.constant.int -1
    %int64_4627 = torch.constant.int 64
    %4288 = torch.prim.ListConstruct %int20_4625, %int-1_4626, %int64_4627 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4289 = torch.aten.view %4279, %4288 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_4628 = torch.constant.int 1
    %int2_4629 = torch.constant.int 2
    %4290 = torch.aten.transpose.int %4287, %int1_4628, %int2_4629 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %4291 = torch.aten.bmm %4285, %4290 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4630 = torch.constant.int 1
    %int20_4631 = torch.constant.int 20
    %int64_4632 = torch.constant.int 64
    %int64_4633 = torch.constant.int 64
    %4292 = torch.prim.ListConstruct %int1_4630, %int20_4631, %int64_4632, %int64_4633 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4293 = torch.aten.view %4291, %4292 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4634 = torch.constant.int 1
    %4294 = torch.aten.add.Tensor %4293, %27, %int1_4634 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4635 = torch.constant.int 20
    %int64_4636 = torch.constant.int 64
    %int64_4637 = torch.constant.int 64
    %4295 = torch.prim.ListConstruct %int20_4635, %int64_4636, %int64_4637 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4296 = torch.aten.view %4294, %4295 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_4638 = torch.constant.int -1
    %false_4639 = torch.constant.bool false
    %4297 = torch.aten._softmax %4296, %int-1_4638, %false_4639 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %4298 = torch.aten.detach %4297 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_4640 = torch.constant.none
    %4299 = torch.aten.clone %4297, %none_4640 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %4300 = torch.aten.bmm %4299, %4289 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4641 = torch.constant.int 1
    %int20_4642 = torch.constant.int 20
    %int64_4643 = torch.constant.int 64
    %int64_4644 = torch.constant.int 64
    %4301 = torch.prim.ListConstruct %int1_4641, %int20_4642, %int64_4643, %int64_4644 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4302 = torch.aten.view %4300, %4301 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4645 = torch.constant.int 1
    %int2_4646 = torch.constant.int 2
    %4303 = torch.aten.transpose.int %4302, %int1_4645, %int2_4646 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_4647 = torch.constant.int 0
    %4304 = torch.aten.clone %4303, %int0_4647 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4648 = torch.constant.int 1
    %int64_4649 = torch.constant.int 64
    %int1280_4650 = torch.constant.int 1280
    %4305 = torch.prim.ListConstruct %int1_4648, %int64_4649, %int1280_4650 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4306 = torch.aten._unsafe_view %4304, %4305 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_4651 = torch.constant.int 64
    %int1280_4652 = torch.constant.int 1280
    %4307 = torch.prim.ListConstruct %int64_4651, %int1280_4652 : (!torch.int, !torch.int) -> !torch.list<int>
    %4308 = torch.aten.view %4306, %4307 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %4309 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4653 = torch.constant.int 0
    %int1_4654 = torch.constant.int 1
    %4310 = torch.aten.transpose.int %4309, %int0_4653, %int1_4654 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.bias : tensor<1280xf16>
    %4311 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4655 = torch.constant.int 6
    %4312 = torch.prims.convert_element_type %4311, %int6_4655 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4656 = torch.constant.int 6
    %4313 = torch.prims.convert_element_type %4308, %int6_4656 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4657 = torch.constant.int 6
    %4314 = torch.prims.convert_element_type %4310, %int6_4657 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4315 = torch.aten.mm %4313, %4314 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4658 = torch.constant.int 1
    %4316 = torch.aten.mul.Scalar %4315, %int1_4658 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4659 = torch.constant.int 1
    %4317 = torch.aten.mul.Scalar %4312, %int1_4659 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4660 = torch.constant.int 1
    %4318 = torch.aten.add.Tensor %4316, %4317, %int1_4660 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4661 = torch.constant.int 5
    %4319 = torch.prims.convert_element_type %4318, %int5_4661 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4662 = torch.constant.int 1
    %int64_4663 = torch.constant.int 64
    %int1280_4664 = torch.constant.int 1280
    %4320 = torch.prim.ListConstruct %int1_4662, %int64_4663, %int1280_4664 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4321 = torch.aten.view %4319, %4320 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4665 = torch.constant.int 1
    %4322 = torch.aten.add.Tensor %4212, %4321, %int1_4665 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4666 = torch.constant.int 6
    %4323 = torch.prims.convert_element_type %4322, %int6_4666 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4667 = torch.constant.int 2
    %4324 = torch.prim.ListConstruct %int2_4667 : (!torch.int) -> !torch.list<int>
    %int0_4668 = torch.constant.int 0
    %true_4669 = torch.constant.bool true
    %result0_4670, %result1_4671 = torch.aten.var_mean.correction %4323, %4324, %int0_4668, %true_4669 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4672 = torch.constant.float 1.000000e-05
    %int1_4673 = torch.constant.int 1
    %4325 = torch.aten.add.Scalar %result0_4670, %float1.000000e-05_4672, %int1_4673 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4326 = torch.aten.rsqrt %4325 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4674 = torch.constant.int 1
    %4327 = torch.aten.sub.Tensor %4322, %result1_4671, %int1_4674 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4328 = torch.aten.mul.Tensor %4327, %4326 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.weight : tensor<1280xf16>
    %4329 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4330 = torch.aten.mul.Tensor %4328, %4329 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.bias : tensor<1280xf16>
    %4331 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4675 = torch.constant.int 1
    %4332 = torch.aten.add.Tensor %4330, %4331, %int1_4675 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4676 = torch.constant.int 5
    %4333 = torch.prims.convert_element_type %4332, %int5_4676 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4677 = torch.constant.int 5
    %4334 = torch.prims.convert_element_type %result1_4671, %int5_4677 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4678 = torch.constant.int 5
    %4335 = torch.prims.convert_element_type %4326, %int5_4678 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4679 = torch.constant.int 64
    %int1280_4680 = torch.constant.int 1280
    %4336 = torch.prim.ListConstruct %int64_4679, %int1280_4680 : (!torch.int, !torch.int) -> !torch.list<int>
    %4337 = torch.aten.view %4333, %4336 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.weight : tensor<5120x1280xf16>
    %4338 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_4681 = torch.constant.int 0
    %int1_4682 = torch.constant.int 1
    %4339 = torch.aten.transpose.int %4338, %int0_4681, %int1_4682 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.bias : tensor<5120xf16>
    %4340 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_4683 = torch.constant.int 6
    %4341 = torch.prims.convert_element_type %4340, %int6_4683 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_4684 = torch.constant.int 6
    %4342 = torch.prims.convert_element_type %4337, %int6_4684 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4685 = torch.constant.int 6
    %4343 = torch.prims.convert_element_type %4339, %int6_4685 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4344 = torch.aten.mm %4342, %4343 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_4686 = torch.constant.int 1
    %4345 = torch.aten.mul.Scalar %4344, %int1_4686 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_4687 = torch.constant.int 1
    %4346 = torch.aten.mul.Scalar %4341, %int1_4687 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_4688 = torch.constant.int 1
    %4347 = torch.aten.add.Tensor %4345, %4346, %int1_4688 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_4689 = torch.constant.int 5
    %4348 = torch.prims.convert_element_type %4347, %int5_4689 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_4690 = torch.constant.int 1
    %int64_4691 = torch.constant.int 64
    %int5120_4692 = torch.constant.int 5120
    %4349 = torch.prim.ListConstruct %int1_4690, %int64_4691, %int5120_4692 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4350 = torch.aten.view %4348, %4349 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_4693 = torch.constant.str "none"
    %4351 = torch.aten.gelu %4350, %str_4693 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_4694 = torch.constant.int 64
    %int5120_4695 = torch.constant.int 5120
    %4352 = torch.prim.ListConstruct %int64_4694, %int5120_4695 : (!torch.int, !torch.int) -> !torch.list<int>
    %4353 = torch.aten.view %4351, %4352 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.weight : tensor<1280x5120xf16>
    %4354 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4696 = torch.constant.int 0
    %int1_4697 = torch.constant.int 1
    %4355 = torch.aten.transpose.int %4354, %int0_4696, %int1_4697 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.bias : tensor<1280xf16>
    %4356 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.27.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4698 = torch.constant.int 6
    %4357 = torch.prims.convert_element_type %4356, %int6_4698 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4699 = torch.constant.int 6
    %4358 = torch.prims.convert_element_type %4353, %int6_4699 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_4700 = torch.constant.int 6
    %4359 = torch.prims.convert_element_type %4355, %int6_4700 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4360 = torch.aten.mm %4358, %4359 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4701 = torch.constant.int 1
    %4361 = torch.aten.mul.Scalar %4360, %int1_4701 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4702 = torch.constant.int 1
    %4362 = torch.aten.mul.Scalar %4357, %int1_4702 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4703 = torch.constant.int 1
    %4363 = torch.aten.add.Tensor %4361, %4362, %int1_4703 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4704 = torch.constant.int 5
    %4364 = torch.prims.convert_element_type %4363, %int5_4704 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4705 = torch.constant.int 1
    %int64_4706 = torch.constant.int 64
    %int1280_4707 = torch.constant.int 1280
    %4365 = torch.prim.ListConstruct %int1_4705, %int64_4706, %int1280_4707 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4366 = torch.aten.view %4364, %4365 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4708 = torch.constant.int 1
    %4367 = torch.aten.add.Tensor %4322, %4366, %int1_4708 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4709 = torch.constant.int 6
    %4368 = torch.prims.convert_element_type %4367, %int6_4709 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4710 = torch.constant.int 2
    %4369 = torch.prim.ListConstruct %int2_4710 : (!torch.int) -> !torch.list<int>
    %int0_4711 = torch.constant.int 0
    %true_4712 = torch.constant.bool true
    %result0_4713, %result1_4714 = torch.aten.var_mean.correction %4368, %4369, %int0_4711, %true_4712 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4715 = torch.constant.float 1.000000e-05
    %int1_4716 = torch.constant.int 1
    %4370 = torch.aten.add.Scalar %result0_4713, %float1.000000e-05_4715, %int1_4716 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4371 = torch.aten.rsqrt %4370 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4717 = torch.constant.int 1
    %4372 = torch.aten.sub.Tensor %4367, %result1_4714, %int1_4717 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4373 = torch.aten.mul.Tensor %4372, %4371 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.weight : tensor<1280xf16>
    %4374 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4375 = torch.aten.mul.Tensor %4373, %4374 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.bias : tensor<1280xf16>
    %4376 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4718 = torch.constant.int 1
    %4377 = torch.aten.add.Tensor %4375, %4376, %int1_4718 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4719 = torch.constant.int 5
    %4378 = torch.prims.convert_element_type %4377, %int5_4719 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4720 = torch.constant.int 5
    %4379 = torch.prims.convert_element_type %result1_4714, %int5_4720 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4721 = torch.constant.int 5
    %4380 = torch.prims.convert_element_type %4371, %int5_4721 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4722 = torch.constant.int 64
    %int1280_4723 = torch.constant.int 1280
    %4381 = torch.prim.ListConstruct %int64_4722, %int1280_4723 : (!torch.int, !torch.int) -> !torch.list<int>
    %4382 = torch.aten.view %4378, %4381 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %4383 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4724 = torch.constant.int 0
    %int1_4725 = torch.constant.int 1
    %4384 = torch.aten.transpose.int %4383, %int0_4724, %int1_4725 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.bias : tensor<1280xf16>
    %4385 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4726 = torch.constant.int 6
    %4386 = torch.prims.convert_element_type %4385, %int6_4726 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4727 = torch.constant.int 6
    %4387 = torch.prims.convert_element_type %4382, %int6_4727 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4728 = torch.constant.int 6
    %4388 = torch.prims.convert_element_type %4384, %int6_4728 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4389 = torch.aten.mm %4387, %4388 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4729 = torch.constant.int 1
    %4390 = torch.aten.mul.Scalar %4389, %int1_4729 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4730 = torch.constant.int 1
    %4391 = torch.aten.mul.Scalar %4386, %int1_4730 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4731 = torch.constant.int 1
    %4392 = torch.aten.add.Tensor %4390, %4391, %int1_4731 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4732 = torch.constant.int 5
    %4393 = torch.prims.convert_element_type %4392, %int5_4732 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4733 = torch.constant.int 1
    %int64_4734 = torch.constant.int 64
    %int1280_4735 = torch.constant.int 1280
    %4394 = torch.prim.ListConstruct %int1_4733, %int64_4734, %int1280_4735 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4395 = torch.aten.view %4393, %4394 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_4736 = torch.constant.float 1.250000e-01
    %4396 = torch.aten.mul.Scalar %4395, %float1.250000e-01_4736 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_4737 = torch.constant.int 64
    %int1280_4738 = torch.constant.int 1280
    %4397 = torch.prim.ListConstruct %int64_4737, %int1280_4738 : (!torch.int, !torch.int) -> !torch.list<int>
    %4398 = torch.aten.view %4378, %4397 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %4399 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4739 = torch.constant.int 0
    %int1_4740 = torch.constant.int 1
    %4400 = torch.aten.transpose.int %4399, %int0_4739, %int1_4740 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.bias : tensor<1280xf16>
    %4401 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4741 = torch.constant.int 6
    %4402 = torch.prims.convert_element_type %4401, %int6_4741 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4742 = torch.constant.int 6
    %4403 = torch.prims.convert_element_type %4398, %int6_4742 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4743 = torch.constant.int 6
    %4404 = torch.prims.convert_element_type %4400, %int6_4743 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4405 = torch.aten.mm %4403, %4404 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4744 = torch.constant.int 1
    %4406 = torch.aten.mul.Scalar %4405, %int1_4744 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4745 = torch.constant.int 1
    %4407 = torch.aten.mul.Scalar %4402, %int1_4745 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4746 = torch.constant.int 1
    %4408 = torch.aten.add.Tensor %4406, %4407, %int1_4746 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4747 = torch.constant.int 5
    %4409 = torch.prims.convert_element_type %4408, %int5_4747 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4748 = torch.constant.int 1
    %int64_4749 = torch.constant.int 64
    %int1280_4750 = torch.constant.int 1280
    %4410 = torch.prim.ListConstruct %int1_4748, %int64_4749, %int1280_4750 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4411 = torch.aten.view %4409, %4410 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4751 = torch.constant.int 1
    %int-1_4752 = torch.constant.int -1
    %int20_4753 = torch.constant.int 20
    %int64_4754 = torch.constant.int 64
    %4412 = torch.prim.ListConstruct %int1_4751, %int-1_4752, %int20_4753, %int64_4754 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4413 = torch.aten.view %4411, %4412 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4755 = torch.constant.int 1
    %int2_4756 = torch.constant.int 2
    %4414 = torch.aten.transpose.int %4413, %int1_4755, %int2_4756 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4757 = torch.constant.int 0
    %4415 = torch.aten.clone %4414, %int0_4757 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_4758 = torch.constant.int 64
    %int1280_4759 = torch.constant.int 1280
    %4416 = torch.prim.ListConstruct %int64_4758, %int1280_4759 : (!torch.int, !torch.int) -> !torch.list<int>
    %4417 = torch.aten.view %4378, %4416 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %4418 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4760 = torch.constant.int 0
    %int1_4761 = torch.constant.int 1
    %4419 = torch.aten.transpose.int %4418, %int0_4760, %int1_4761 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.bias : tensor<1280xf16>
    %4420 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4762 = torch.constant.int 6
    %4421 = torch.prims.convert_element_type %4420, %int6_4762 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4763 = torch.constant.int 6
    %4422 = torch.prims.convert_element_type %4417, %int6_4763 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4764 = torch.constant.int 6
    %4423 = torch.prims.convert_element_type %4419, %int6_4764 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4424 = torch.aten.mm %4422, %4423 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4765 = torch.constant.int 1
    %4425 = torch.aten.mul.Scalar %4424, %int1_4765 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4766 = torch.constant.int 1
    %4426 = torch.aten.mul.Scalar %4421, %int1_4766 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4767 = torch.constant.int 1
    %4427 = torch.aten.add.Tensor %4425, %4426, %int1_4767 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4768 = torch.constant.int 5
    %4428 = torch.prims.convert_element_type %4427, %int5_4768 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4769 = torch.constant.int 1
    %int64_4770 = torch.constant.int 64
    %int1280_4771 = torch.constant.int 1280
    %4429 = torch.prim.ListConstruct %int1_4769, %int64_4770, %int1280_4771 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4430 = torch.aten.view %4428, %4429 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4772 = torch.constant.int 1
    %int-1_4773 = torch.constant.int -1
    %int20_4774 = torch.constant.int 20
    %int64_4775 = torch.constant.int 64
    %4431 = torch.prim.ListConstruct %int1_4772, %int-1_4773, %int20_4774, %int64_4775 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4432 = torch.aten.view %4430, %4431 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4776 = torch.constant.int 1
    %int2_4777 = torch.constant.int 2
    %4433 = torch.aten.transpose.int %4432, %int1_4776, %int2_4777 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4778 = torch.constant.int 0
    %4434 = torch.aten.clone %4433, %int0_4778 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4779 = torch.constant.int 1
    %int64_4780 = torch.constant.int 64
    %int20_4781 = torch.constant.int 20
    %int64_4782 = torch.constant.int 64
    %4435 = torch.prim.ListConstruct %int1_4779, %int64_4780, %int20_4781, %int64_4782 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4436 = torch.aten.view %4396, %4435 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4783 = torch.constant.int 1
    %int2_4784 = torch.constant.int 2
    %4437 = torch.aten.transpose.int %4436, %int1_4783, %int2_4784 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4785 = torch.constant.int 0
    %4438 = torch.aten.clone %4437, %int0_4785 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4786 = torch.constant.int 20
    %int-1_4787 = torch.constant.int -1
    %int64_4788 = torch.constant.int 64
    %4439 = torch.prim.ListConstruct %int20_4786, %int-1_4787, %int64_4788 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4440 = torch.aten.view %4438, %4439 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4789 = torch.constant.int 20
    %int-1_4790 = torch.constant.int -1
    %int64_4791 = torch.constant.int 64
    %4441 = torch.prim.ListConstruct %int20_4789, %int-1_4790, %int64_4791 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4442 = torch.aten.view %4415, %4441 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4792 = torch.constant.int 20
    %int-1_4793 = torch.constant.int -1
    %int64_4794 = torch.constant.int 64
    %4443 = torch.prim.ListConstruct %int20_4792, %int-1_4793, %int64_4794 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4444 = torch.aten.view %4434, %4443 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_4795 = torch.constant.int 1
    %int2_4796 = torch.constant.int 2
    %4445 = torch.aten.transpose.int %4442, %int1_4795, %int2_4796 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %4446 = torch.aten.bmm %4440, %4445 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4797 = torch.constant.int 1
    %int20_4798 = torch.constant.int 20
    %int64_4799 = torch.constant.int 64
    %int64_4800 = torch.constant.int 64
    %4447 = torch.prim.ListConstruct %int1_4797, %int20_4798, %int64_4799, %int64_4800 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4448 = torch.aten.view %4446, %4447 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4801 = torch.constant.int 1
    %4449 = torch.aten.add.Tensor %4448, %27, %int1_4801 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4802 = torch.constant.int 20
    %int64_4803 = torch.constant.int 64
    %int64_4804 = torch.constant.int 64
    %4450 = torch.prim.ListConstruct %int20_4802, %int64_4803, %int64_4804 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4451 = torch.aten.view %4449, %4450 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_4805 = torch.constant.int -1
    %false_4806 = torch.constant.bool false
    %4452 = torch.aten._softmax %4451, %int-1_4805, %false_4806 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %4453 = torch.aten.detach %4452 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_4807 = torch.constant.none
    %4454 = torch.aten.clone %4452, %none_4807 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %4455 = torch.aten.bmm %4454, %4444 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4808 = torch.constant.int 1
    %int20_4809 = torch.constant.int 20
    %int64_4810 = torch.constant.int 64
    %int64_4811 = torch.constant.int 64
    %4456 = torch.prim.ListConstruct %int1_4808, %int20_4809, %int64_4810, %int64_4811 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4457 = torch.aten.view %4455, %4456 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4812 = torch.constant.int 1
    %int2_4813 = torch.constant.int 2
    %4458 = torch.aten.transpose.int %4457, %int1_4812, %int2_4813 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_4814 = torch.constant.int 0
    %4459 = torch.aten.clone %4458, %int0_4814 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4815 = torch.constant.int 1
    %int64_4816 = torch.constant.int 64
    %int1280_4817 = torch.constant.int 1280
    %4460 = torch.prim.ListConstruct %int1_4815, %int64_4816, %int1280_4817 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4461 = torch.aten._unsafe_view %4459, %4460 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_4818 = torch.constant.int 64
    %int1280_4819 = torch.constant.int 1280
    %4462 = torch.prim.ListConstruct %int64_4818, %int1280_4819 : (!torch.int, !torch.int) -> !torch.list<int>
    %4463 = torch.aten.view %4461, %4462 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %4464 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4820 = torch.constant.int 0
    %int1_4821 = torch.constant.int 1
    %4465 = torch.aten.transpose.int %4464, %int0_4820, %int1_4821 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.bias : tensor<1280xf16>
    %4466 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4822 = torch.constant.int 6
    %4467 = torch.prims.convert_element_type %4466, %int6_4822 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4823 = torch.constant.int 6
    %4468 = torch.prims.convert_element_type %4463, %int6_4823 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4824 = torch.constant.int 6
    %4469 = torch.prims.convert_element_type %4465, %int6_4824 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4470 = torch.aten.mm %4468, %4469 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4825 = torch.constant.int 1
    %4471 = torch.aten.mul.Scalar %4470, %int1_4825 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4826 = torch.constant.int 1
    %4472 = torch.aten.mul.Scalar %4467, %int1_4826 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4827 = torch.constant.int 1
    %4473 = torch.aten.add.Tensor %4471, %4472, %int1_4827 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4828 = torch.constant.int 5
    %4474 = torch.prims.convert_element_type %4473, %int5_4828 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4829 = torch.constant.int 1
    %int64_4830 = torch.constant.int 64
    %int1280_4831 = torch.constant.int 1280
    %4475 = torch.prim.ListConstruct %int1_4829, %int64_4830, %int1280_4831 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4476 = torch.aten.view %4474, %4475 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4832 = torch.constant.int 1
    %4477 = torch.aten.add.Tensor %4367, %4476, %int1_4832 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4833 = torch.constant.int 6
    %4478 = torch.prims.convert_element_type %4477, %int6_4833 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4834 = torch.constant.int 2
    %4479 = torch.prim.ListConstruct %int2_4834 : (!torch.int) -> !torch.list<int>
    %int0_4835 = torch.constant.int 0
    %true_4836 = torch.constant.bool true
    %result0_4837, %result1_4838 = torch.aten.var_mean.correction %4478, %4479, %int0_4835, %true_4836 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4839 = torch.constant.float 1.000000e-05
    %int1_4840 = torch.constant.int 1
    %4480 = torch.aten.add.Scalar %result0_4837, %float1.000000e-05_4839, %int1_4840 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4481 = torch.aten.rsqrt %4480 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4841 = torch.constant.int 1
    %4482 = torch.aten.sub.Tensor %4477, %result1_4838, %int1_4841 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4483 = torch.aten.mul.Tensor %4482, %4481 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.weight : tensor<1280xf16>
    %4484 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4485 = torch.aten.mul.Tensor %4483, %4484 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.bias : tensor<1280xf16>
    %4486 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4842 = torch.constant.int 1
    %4487 = torch.aten.add.Tensor %4485, %4486, %int1_4842 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4843 = torch.constant.int 5
    %4488 = torch.prims.convert_element_type %4487, %int5_4843 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4844 = torch.constant.int 5
    %4489 = torch.prims.convert_element_type %result1_4838, %int5_4844 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4845 = torch.constant.int 5
    %4490 = torch.prims.convert_element_type %4481, %int5_4845 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4846 = torch.constant.int 64
    %int1280_4847 = torch.constant.int 1280
    %4491 = torch.prim.ListConstruct %int64_4846, %int1280_4847 : (!torch.int, !torch.int) -> !torch.list<int>
    %4492 = torch.aten.view %4488, %4491 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.weight : tensor<5120x1280xf16>
    %4493 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_4848 = torch.constant.int 0
    %int1_4849 = torch.constant.int 1
    %4494 = torch.aten.transpose.int %4493, %int0_4848, %int1_4849 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.bias : tensor<5120xf16>
    %4495 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_4850 = torch.constant.int 6
    %4496 = torch.prims.convert_element_type %4495, %int6_4850 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_4851 = torch.constant.int 6
    %4497 = torch.prims.convert_element_type %4492, %int6_4851 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4852 = torch.constant.int 6
    %4498 = torch.prims.convert_element_type %4494, %int6_4852 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4499 = torch.aten.mm %4497, %4498 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_4853 = torch.constant.int 1
    %4500 = torch.aten.mul.Scalar %4499, %int1_4853 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_4854 = torch.constant.int 1
    %4501 = torch.aten.mul.Scalar %4496, %int1_4854 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_4855 = torch.constant.int 1
    %4502 = torch.aten.add.Tensor %4500, %4501, %int1_4855 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_4856 = torch.constant.int 5
    %4503 = torch.prims.convert_element_type %4502, %int5_4856 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_4857 = torch.constant.int 1
    %int64_4858 = torch.constant.int 64
    %int5120_4859 = torch.constant.int 5120
    %4504 = torch.prim.ListConstruct %int1_4857, %int64_4858, %int5120_4859 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4505 = torch.aten.view %4503, %4504 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_4860 = torch.constant.str "none"
    %4506 = torch.aten.gelu %4505, %str_4860 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_4861 = torch.constant.int 64
    %int5120_4862 = torch.constant.int 5120
    %4507 = torch.prim.ListConstruct %int64_4861, %int5120_4862 : (!torch.int, !torch.int) -> !torch.list<int>
    %4508 = torch.aten.view %4506, %4507 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.weight : tensor<1280x5120xf16>
    %4509 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4863 = torch.constant.int 0
    %int1_4864 = torch.constant.int 1
    %4510 = torch.aten.transpose.int %4509, %int0_4863, %int1_4864 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.bias : tensor<1280xf16>
    %4511 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.28.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4865 = torch.constant.int 6
    %4512 = torch.prims.convert_element_type %4511, %int6_4865 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4866 = torch.constant.int 6
    %4513 = torch.prims.convert_element_type %4508, %int6_4866 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_4867 = torch.constant.int 6
    %4514 = torch.prims.convert_element_type %4510, %int6_4867 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4515 = torch.aten.mm %4513, %4514 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4868 = torch.constant.int 1
    %4516 = torch.aten.mul.Scalar %4515, %int1_4868 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4869 = torch.constant.int 1
    %4517 = torch.aten.mul.Scalar %4512, %int1_4869 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4870 = torch.constant.int 1
    %4518 = torch.aten.add.Tensor %4516, %4517, %int1_4870 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4871 = torch.constant.int 5
    %4519 = torch.prims.convert_element_type %4518, %int5_4871 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4872 = torch.constant.int 1
    %int64_4873 = torch.constant.int 64
    %int1280_4874 = torch.constant.int 1280
    %4520 = torch.prim.ListConstruct %int1_4872, %int64_4873, %int1280_4874 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4521 = torch.aten.view %4519, %4520 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4875 = torch.constant.int 1
    %4522 = torch.aten.add.Tensor %4477, %4521, %int1_4875 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_4876 = torch.constant.int 6
    %4523 = torch.prims.convert_element_type %4522, %int6_4876 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_4877 = torch.constant.int 2
    %4524 = torch.prim.ListConstruct %int2_4877 : (!torch.int) -> !torch.list<int>
    %int0_4878 = torch.constant.int 0
    %true_4879 = torch.constant.bool true
    %result0_4880, %result1_4881 = torch.aten.var_mean.correction %4523, %4524, %int0_4878, %true_4879 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_4882 = torch.constant.float 1.000000e-05
    %int1_4883 = torch.constant.int 1
    %4525 = torch.aten.add.Scalar %result0_4880, %float1.000000e-05_4882, %int1_4883 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4526 = torch.aten.rsqrt %4525 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_4884 = torch.constant.int 1
    %4527 = torch.aten.sub.Tensor %4522, %result1_4881, %int1_4884 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4528 = torch.aten.mul.Tensor %4527, %4526 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.weight : tensor<1280xf16>
    %4529 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4530 = torch.aten.mul.Tensor %4528, %4529 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.bias : tensor<1280xf16>
    %4531 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4885 = torch.constant.int 1
    %4532 = torch.aten.add.Tensor %4530, %4531, %int1_4885 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_4886 = torch.constant.int 5
    %4533 = torch.prims.convert_element_type %4532, %int5_4886 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_4887 = torch.constant.int 5
    %4534 = torch.prims.convert_element_type %result1_4881, %int5_4887 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_4888 = torch.constant.int 5
    %4535 = torch.prims.convert_element_type %4526, %int5_4888 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_4889 = torch.constant.int 64
    %int1280_4890 = torch.constant.int 1280
    %4536 = torch.prim.ListConstruct %int64_4889, %int1280_4890 : (!torch.int, !torch.int) -> !torch.list<int>
    %4537 = torch.aten.view %4533, %4536 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %4538 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4891 = torch.constant.int 0
    %int1_4892 = torch.constant.int 1
    %4539 = torch.aten.transpose.int %4538, %int0_4891, %int1_4892 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.bias : tensor<1280xf16>
    %4540 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4893 = torch.constant.int 6
    %4541 = torch.prims.convert_element_type %4540, %int6_4893 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4894 = torch.constant.int 6
    %4542 = torch.prims.convert_element_type %4537, %int6_4894 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4895 = torch.constant.int 6
    %4543 = torch.prims.convert_element_type %4539, %int6_4895 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4544 = torch.aten.mm %4542, %4543 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4896 = torch.constant.int 1
    %4545 = torch.aten.mul.Scalar %4544, %int1_4896 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4897 = torch.constant.int 1
    %4546 = torch.aten.mul.Scalar %4541, %int1_4897 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4898 = torch.constant.int 1
    %4547 = torch.aten.add.Tensor %4545, %4546, %int1_4898 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4899 = torch.constant.int 5
    %4548 = torch.prims.convert_element_type %4547, %int5_4899 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4900 = torch.constant.int 1
    %int64_4901 = torch.constant.int 64
    %int1280_4902 = torch.constant.int 1280
    %4549 = torch.prim.ListConstruct %int1_4900, %int64_4901, %int1280_4902 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4550 = torch.aten.view %4548, %4549 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_4903 = torch.constant.float 1.250000e-01
    %4551 = torch.aten.mul.Scalar %4550, %float1.250000e-01_4903 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_4904 = torch.constant.int 64
    %int1280_4905 = torch.constant.int 1280
    %4552 = torch.prim.ListConstruct %int64_4904, %int1280_4905 : (!torch.int, !torch.int) -> !torch.list<int>
    %4553 = torch.aten.view %4533, %4552 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %4554 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4906 = torch.constant.int 0
    %int1_4907 = torch.constant.int 1
    %4555 = torch.aten.transpose.int %4554, %int0_4906, %int1_4907 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.bias : tensor<1280xf16>
    %4556 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4908 = torch.constant.int 6
    %4557 = torch.prims.convert_element_type %4556, %int6_4908 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4909 = torch.constant.int 6
    %4558 = torch.prims.convert_element_type %4553, %int6_4909 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4910 = torch.constant.int 6
    %4559 = torch.prims.convert_element_type %4555, %int6_4910 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4560 = torch.aten.mm %4558, %4559 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4911 = torch.constant.int 1
    %4561 = torch.aten.mul.Scalar %4560, %int1_4911 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4912 = torch.constant.int 1
    %4562 = torch.aten.mul.Scalar %4557, %int1_4912 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4913 = torch.constant.int 1
    %4563 = torch.aten.add.Tensor %4561, %4562, %int1_4913 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4914 = torch.constant.int 5
    %4564 = torch.prims.convert_element_type %4563, %int5_4914 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4915 = torch.constant.int 1
    %int64_4916 = torch.constant.int 64
    %int1280_4917 = torch.constant.int 1280
    %4565 = torch.prim.ListConstruct %int1_4915, %int64_4916, %int1280_4917 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4566 = torch.aten.view %4564, %4565 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4918 = torch.constant.int 1
    %int-1_4919 = torch.constant.int -1
    %int20_4920 = torch.constant.int 20
    %int64_4921 = torch.constant.int 64
    %4567 = torch.prim.ListConstruct %int1_4918, %int-1_4919, %int20_4920, %int64_4921 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4568 = torch.aten.view %4566, %4567 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4922 = torch.constant.int 1
    %int2_4923 = torch.constant.int 2
    %4569 = torch.aten.transpose.int %4568, %int1_4922, %int2_4923 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4924 = torch.constant.int 0
    %4570 = torch.aten.clone %4569, %int0_4924 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_4925 = torch.constant.int 64
    %int1280_4926 = torch.constant.int 1280
    %4571 = torch.prim.ListConstruct %int64_4925, %int1280_4926 : (!torch.int, !torch.int) -> !torch.list<int>
    %4572 = torch.aten.view %4533, %4571 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %4573 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4927 = torch.constant.int 0
    %int1_4928 = torch.constant.int 1
    %4574 = torch.aten.transpose.int %4573, %int0_4927, %int1_4928 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.bias : tensor<1280xf16>
    %4575 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4929 = torch.constant.int 6
    %4576 = torch.prims.convert_element_type %4575, %int6_4929 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4930 = torch.constant.int 6
    %4577 = torch.prims.convert_element_type %4572, %int6_4930 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4931 = torch.constant.int 6
    %4578 = torch.prims.convert_element_type %4574, %int6_4931 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4579 = torch.aten.mm %4577, %4578 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4932 = torch.constant.int 1
    %4580 = torch.aten.mul.Scalar %4579, %int1_4932 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4933 = torch.constant.int 1
    %4581 = torch.aten.mul.Scalar %4576, %int1_4933 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4934 = torch.constant.int 1
    %4582 = torch.aten.add.Tensor %4580, %4581, %int1_4934 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4935 = torch.constant.int 5
    %4583 = torch.prims.convert_element_type %4582, %int5_4935 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4936 = torch.constant.int 1
    %int64_4937 = torch.constant.int 64
    %int1280_4938 = torch.constant.int 1280
    %4584 = torch.prim.ListConstruct %int1_4936, %int64_4937, %int1280_4938 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4585 = torch.aten.view %4583, %4584 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4939 = torch.constant.int 1
    %int-1_4940 = torch.constant.int -1
    %int20_4941 = torch.constant.int 20
    %int64_4942 = torch.constant.int 64
    %4586 = torch.prim.ListConstruct %int1_4939, %int-1_4940, %int20_4941, %int64_4942 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4587 = torch.aten.view %4585, %4586 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4943 = torch.constant.int 1
    %int2_4944 = torch.constant.int 2
    %4588 = torch.aten.transpose.int %4587, %int1_4943, %int2_4944 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4945 = torch.constant.int 0
    %4589 = torch.aten.clone %4588, %int0_4945 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4946 = torch.constant.int 1
    %int64_4947 = torch.constant.int 64
    %int20_4948 = torch.constant.int 20
    %int64_4949 = torch.constant.int 64
    %4590 = torch.prim.ListConstruct %int1_4946, %int64_4947, %int20_4948, %int64_4949 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4591 = torch.aten.view %4551, %4590 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4950 = torch.constant.int 1
    %int2_4951 = torch.constant.int 2
    %4592 = torch.aten.transpose.int %4591, %int1_4950, %int2_4951 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_4952 = torch.constant.int 0
    %4593 = torch.aten.clone %4592, %int0_4952 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4953 = torch.constant.int 20
    %int-1_4954 = torch.constant.int -1
    %int64_4955 = torch.constant.int 64
    %4594 = torch.prim.ListConstruct %int20_4953, %int-1_4954, %int64_4955 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4595 = torch.aten.view %4593, %4594 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4956 = torch.constant.int 20
    %int-1_4957 = torch.constant.int -1
    %int64_4958 = torch.constant.int 64
    %4596 = torch.prim.ListConstruct %int20_4956, %int-1_4957, %int64_4958 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4597 = torch.aten.view %4570, %4596 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_4959 = torch.constant.int 20
    %int-1_4960 = torch.constant.int -1
    %int64_4961 = torch.constant.int 64
    %4598 = torch.prim.ListConstruct %int20_4959, %int-1_4960, %int64_4961 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4599 = torch.aten.view %4589, %4598 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_4962 = torch.constant.int 1
    %int2_4963 = torch.constant.int 2
    %4600 = torch.aten.transpose.int %4597, %int1_4962, %int2_4963 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %4601 = torch.aten.bmm %4595, %4600 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4964 = torch.constant.int 1
    %int20_4965 = torch.constant.int 20
    %int64_4966 = torch.constant.int 64
    %int64_4967 = torch.constant.int 64
    %4602 = torch.prim.ListConstruct %int1_4964, %int20_4965, %int64_4966, %int64_4967 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4603 = torch.aten.view %4601, %4602 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4968 = torch.constant.int 1
    %4604 = torch.aten.add.Tensor %4603, %27, %int1_4968 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_4969 = torch.constant.int 20
    %int64_4970 = torch.constant.int 64
    %int64_4971 = torch.constant.int 64
    %4605 = torch.prim.ListConstruct %int20_4969, %int64_4970, %int64_4971 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4606 = torch.aten.view %4604, %4605 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_4972 = torch.constant.int -1
    %false_4973 = torch.constant.bool false
    %4607 = torch.aten._softmax %4606, %int-1_4972, %false_4973 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %4608 = torch.aten.detach %4607 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_4974 = torch.constant.none
    %4609 = torch.aten.clone %4607, %none_4974 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %4610 = torch.aten.bmm %4609, %4599 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_4975 = torch.constant.int 1
    %int20_4976 = torch.constant.int 20
    %int64_4977 = torch.constant.int 64
    %int64_4978 = torch.constant.int 64
    %4611 = torch.prim.ListConstruct %int1_4975, %int20_4976, %int64_4977, %int64_4978 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4612 = torch.aten.view %4610, %4611 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_4979 = torch.constant.int 1
    %int2_4980 = torch.constant.int 2
    %4613 = torch.aten.transpose.int %4612, %int1_4979, %int2_4980 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_4981 = torch.constant.int 0
    %4614 = torch.aten.clone %4613, %int0_4981 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_4982 = torch.constant.int 1
    %int64_4983 = torch.constant.int 64
    %int1280_4984 = torch.constant.int 1280
    %4615 = torch.prim.ListConstruct %int1_4982, %int64_4983, %int1280_4984 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4616 = torch.aten._unsafe_view %4614, %4615 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_4985 = torch.constant.int 64
    %int1280_4986 = torch.constant.int 1280
    %4617 = torch.prim.ListConstruct %int64_4985, %int1280_4986 : (!torch.int, !torch.int) -> !torch.list<int>
    %4618 = torch.aten.view %4616, %4617 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %4619 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4987 = torch.constant.int 0
    %int1_4988 = torch.constant.int 1
    %4620 = torch.aten.transpose.int %4619, %int0_4987, %int1_4988 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.bias : tensor<1280xf16>
    %4621 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4989 = torch.constant.int 6
    %4622 = torch.prims.convert_element_type %4621, %int6_4989 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4990 = torch.constant.int 6
    %4623 = torch.prims.convert_element_type %4618, %int6_4990 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_4991 = torch.constant.int 6
    %4624 = torch.prims.convert_element_type %4620, %int6_4991 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4625 = torch.aten.mm %4623, %4624 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_4992 = torch.constant.int 1
    %4626 = torch.aten.mul.Scalar %4625, %int1_4992 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_4993 = torch.constant.int 1
    %4627 = torch.aten.mul.Scalar %4622, %int1_4993 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4994 = torch.constant.int 1
    %4628 = torch.aten.add.Tensor %4626, %4627, %int1_4994 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_4995 = torch.constant.int 5
    %4629 = torch.prims.convert_element_type %4628, %int5_4995 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_4996 = torch.constant.int 1
    %int64_4997 = torch.constant.int 64
    %int1280_4998 = torch.constant.int 1280
    %4630 = torch.prim.ListConstruct %int1_4996, %int64_4997, %int1280_4998 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4631 = torch.aten.view %4629, %4630 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_4999 = torch.constant.int 1
    %4632 = torch.aten.add.Tensor %4522, %4631, %int1_4999 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_5000 = torch.constant.int 6
    %4633 = torch.prims.convert_element_type %4632, %int6_5000 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_5001 = torch.constant.int 2
    %4634 = torch.prim.ListConstruct %int2_5001 : (!torch.int) -> !torch.list<int>
    %int0_5002 = torch.constant.int 0
    %true_5003 = torch.constant.bool true
    %result0_5004, %result1_5005 = torch.aten.var_mean.correction %4633, %4634, %int0_5002, %true_5003 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_5006 = torch.constant.float 1.000000e-05
    %int1_5007 = torch.constant.int 1
    %4635 = torch.aten.add.Scalar %result0_5004, %float1.000000e-05_5006, %int1_5007 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4636 = torch.aten.rsqrt %4635 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_5008 = torch.constant.int 1
    %4637 = torch.aten.sub.Tensor %4632, %result1_5005, %int1_5008 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4638 = torch.aten.mul.Tensor %4637, %4636 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.weight : tensor<1280xf16>
    %4639 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4640 = torch.aten.mul.Tensor %4638, %4639 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.bias : tensor<1280xf16>
    %4641 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5009 = torch.constant.int 1
    %4642 = torch.aten.add.Tensor %4640, %4641, %int1_5009 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_5010 = torch.constant.int 5
    %4643 = torch.prims.convert_element_type %4642, %int5_5010 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_5011 = torch.constant.int 5
    %4644 = torch.prims.convert_element_type %result1_5005, %int5_5011 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_5012 = torch.constant.int 5
    %4645 = torch.prims.convert_element_type %4636, %int5_5012 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_5013 = torch.constant.int 64
    %int1280_5014 = torch.constant.int 1280
    %4646 = torch.prim.ListConstruct %int64_5013, %int1280_5014 : (!torch.int, !torch.int) -> !torch.list<int>
    %4647 = torch.aten.view %4643, %4646 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.weight : tensor<5120x1280xf16>
    %4648 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_5015 = torch.constant.int 0
    %int1_5016 = torch.constant.int 1
    %4649 = torch.aten.transpose.int %4648, %int0_5015, %int1_5016 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.bias : tensor<5120xf16>
    %4650 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_5017 = torch.constant.int 6
    %4651 = torch.prims.convert_element_type %4650, %int6_5017 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_5018 = torch.constant.int 6
    %4652 = torch.prims.convert_element_type %4647, %int6_5018 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5019 = torch.constant.int 6
    %4653 = torch.prims.convert_element_type %4649, %int6_5019 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4654 = torch.aten.mm %4652, %4653 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_5020 = torch.constant.int 1
    %4655 = torch.aten.mul.Scalar %4654, %int1_5020 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_5021 = torch.constant.int 1
    %4656 = torch.aten.mul.Scalar %4651, %int1_5021 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_5022 = torch.constant.int 1
    %4657 = torch.aten.add.Tensor %4655, %4656, %int1_5022 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_5023 = torch.constant.int 5
    %4658 = torch.prims.convert_element_type %4657, %int5_5023 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_5024 = torch.constant.int 1
    %int64_5025 = torch.constant.int 64
    %int5120_5026 = torch.constant.int 5120
    %4659 = torch.prim.ListConstruct %int1_5024, %int64_5025, %int5120_5026 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4660 = torch.aten.view %4658, %4659 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_5027 = torch.constant.str "none"
    %4661 = torch.aten.gelu %4660, %str_5027 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_5028 = torch.constant.int 64
    %int5120_5029 = torch.constant.int 5120
    %4662 = torch.prim.ListConstruct %int64_5028, %int5120_5029 : (!torch.int, !torch.int) -> !torch.list<int>
    %4663 = torch.aten.view %4661, %4662 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.weight : tensor<1280x5120xf16>
    %4664 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5030 = torch.constant.int 0
    %int1_5031 = torch.constant.int 1
    %4665 = torch.aten.transpose.int %4664, %int0_5030, %int1_5031 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.bias : tensor<1280xf16>
    %4666 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.29.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5032 = torch.constant.int 6
    %4667 = torch.prims.convert_element_type %4666, %int6_5032 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5033 = torch.constant.int 6
    %4668 = torch.prims.convert_element_type %4663, %int6_5033 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_5034 = torch.constant.int 6
    %4669 = torch.prims.convert_element_type %4665, %int6_5034 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4670 = torch.aten.mm %4668, %4669 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5035 = torch.constant.int 1
    %4671 = torch.aten.mul.Scalar %4670, %int1_5035 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5036 = torch.constant.int 1
    %4672 = torch.aten.mul.Scalar %4667, %int1_5036 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5037 = torch.constant.int 1
    %4673 = torch.aten.add.Tensor %4671, %4672, %int1_5037 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5038 = torch.constant.int 5
    %4674 = torch.prims.convert_element_type %4673, %int5_5038 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5039 = torch.constant.int 1
    %int64_5040 = torch.constant.int 64
    %int1280_5041 = torch.constant.int 1280
    %4675 = torch.prim.ListConstruct %int1_5039, %int64_5040, %int1280_5041 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4676 = torch.aten.view %4674, %4675 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5042 = torch.constant.int 1
    %4677 = torch.aten.add.Tensor %4632, %4676, %int1_5042 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_5043 = torch.constant.int 6
    %4678 = torch.prims.convert_element_type %4677, %int6_5043 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_5044 = torch.constant.int 2
    %4679 = torch.prim.ListConstruct %int2_5044 : (!torch.int) -> !torch.list<int>
    %int0_5045 = torch.constant.int 0
    %true_5046 = torch.constant.bool true
    %result0_5047, %result1_5048 = torch.aten.var_mean.correction %4678, %4679, %int0_5045, %true_5046 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_5049 = torch.constant.float 1.000000e-05
    %int1_5050 = torch.constant.int 1
    %4680 = torch.aten.add.Scalar %result0_5047, %float1.000000e-05_5049, %int1_5050 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4681 = torch.aten.rsqrt %4680 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_5051 = torch.constant.int 1
    %4682 = torch.aten.sub.Tensor %4677, %result1_5048, %int1_5051 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4683 = torch.aten.mul.Tensor %4682, %4681 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.weight : tensor<1280xf16>
    %4684 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4685 = torch.aten.mul.Tensor %4683, %4684 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.bias : tensor<1280xf16>
    %4686 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5052 = torch.constant.int 1
    %4687 = torch.aten.add.Tensor %4685, %4686, %int1_5052 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_5053 = torch.constant.int 5
    %4688 = torch.prims.convert_element_type %4687, %int5_5053 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_5054 = torch.constant.int 5
    %4689 = torch.prims.convert_element_type %result1_5048, %int5_5054 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_5055 = torch.constant.int 5
    %4690 = torch.prims.convert_element_type %4681, %int5_5055 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_5056 = torch.constant.int 64
    %int1280_5057 = torch.constant.int 1280
    %4691 = torch.prim.ListConstruct %int64_5056, %int1280_5057 : (!torch.int, !torch.int) -> !torch.list<int>
    %4692 = torch.aten.view %4688, %4691 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %4693 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5058 = torch.constant.int 0
    %int1_5059 = torch.constant.int 1
    %4694 = torch.aten.transpose.int %4693, %int0_5058, %int1_5059 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.bias : tensor<1280xf16>
    %4695 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5060 = torch.constant.int 6
    %4696 = torch.prims.convert_element_type %4695, %int6_5060 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5061 = torch.constant.int 6
    %4697 = torch.prims.convert_element_type %4692, %int6_5061 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5062 = torch.constant.int 6
    %4698 = torch.prims.convert_element_type %4694, %int6_5062 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4699 = torch.aten.mm %4697, %4698 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5063 = torch.constant.int 1
    %4700 = torch.aten.mul.Scalar %4699, %int1_5063 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5064 = torch.constant.int 1
    %4701 = torch.aten.mul.Scalar %4696, %int1_5064 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5065 = torch.constant.int 1
    %4702 = torch.aten.add.Tensor %4700, %4701, %int1_5065 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5066 = torch.constant.int 5
    %4703 = torch.prims.convert_element_type %4702, %int5_5066 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5067 = torch.constant.int 1
    %int64_5068 = torch.constant.int 64
    %int1280_5069 = torch.constant.int 1280
    %4704 = torch.prim.ListConstruct %int1_5067, %int64_5068, %int1280_5069 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4705 = torch.aten.view %4703, %4704 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_5070 = torch.constant.float 1.250000e-01
    %4706 = torch.aten.mul.Scalar %4705, %float1.250000e-01_5070 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_5071 = torch.constant.int 64
    %int1280_5072 = torch.constant.int 1280
    %4707 = torch.prim.ListConstruct %int64_5071, %int1280_5072 : (!torch.int, !torch.int) -> !torch.list<int>
    %4708 = torch.aten.view %4688, %4707 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %4709 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5073 = torch.constant.int 0
    %int1_5074 = torch.constant.int 1
    %4710 = torch.aten.transpose.int %4709, %int0_5073, %int1_5074 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.bias : tensor<1280xf16>
    %4711 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5075 = torch.constant.int 6
    %4712 = torch.prims.convert_element_type %4711, %int6_5075 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5076 = torch.constant.int 6
    %4713 = torch.prims.convert_element_type %4708, %int6_5076 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5077 = torch.constant.int 6
    %4714 = torch.prims.convert_element_type %4710, %int6_5077 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4715 = torch.aten.mm %4713, %4714 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5078 = torch.constant.int 1
    %4716 = torch.aten.mul.Scalar %4715, %int1_5078 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5079 = torch.constant.int 1
    %4717 = torch.aten.mul.Scalar %4712, %int1_5079 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5080 = torch.constant.int 1
    %4718 = torch.aten.add.Tensor %4716, %4717, %int1_5080 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5081 = torch.constant.int 5
    %4719 = torch.prims.convert_element_type %4718, %int5_5081 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5082 = torch.constant.int 1
    %int64_5083 = torch.constant.int 64
    %int1280_5084 = torch.constant.int 1280
    %4720 = torch.prim.ListConstruct %int1_5082, %int64_5083, %int1280_5084 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4721 = torch.aten.view %4719, %4720 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5085 = torch.constant.int 1
    %int-1_5086 = torch.constant.int -1
    %int20_5087 = torch.constant.int 20
    %int64_5088 = torch.constant.int 64
    %4722 = torch.prim.ListConstruct %int1_5085, %int-1_5086, %int20_5087, %int64_5088 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4723 = torch.aten.view %4721, %4722 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5089 = torch.constant.int 1
    %int2_5090 = torch.constant.int 2
    %4724 = torch.aten.transpose.int %4723, %int1_5089, %int2_5090 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_5091 = torch.constant.int 0
    %4725 = torch.aten.clone %4724, %int0_5091 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_5092 = torch.constant.int 64
    %int1280_5093 = torch.constant.int 1280
    %4726 = torch.prim.ListConstruct %int64_5092, %int1280_5093 : (!torch.int, !torch.int) -> !torch.list<int>
    %4727 = torch.aten.view %4688, %4726 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %4728 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5094 = torch.constant.int 0
    %int1_5095 = torch.constant.int 1
    %4729 = torch.aten.transpose.int %4728, %int0_5094, %int1_5095 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.bias : tensor<1280xf16>
    %4730 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5096 = torch.constant.int 6
    %4731 = torch.prims.convert_element_type %4730, %int6_5096 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5097 = torch.constant.int 6
    %4732 = torch.prims.convert_element_type %4727, %int6_5097 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5098 = torch.constant.int 6
    %4733 = torch.prims.convert_element_type %4729, %int6_5098 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4734 = torch.aten.mm %4732, %4733 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5099 = torch.constant.int 1
    %4735 = torch.aten.mul.Scalar %4734, %int1_5099 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5100 = torch.constant.int 1
    %4736 = torch.aten.mul.Scalar %4731, %int1_5100 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5101 = torch.constant.int 1
    %4737 = torch.aten.add.Tensor %4735, %4736, %int1_5101 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5102 = torch.constant.int 5
    %4738 = torch.prims.convert_element_type %4737, %int5_5102 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5103 = torch.constant.int 1
    %int64_5104 = torch.constant.int 64
    %int1280_5105 = torch.constant.int 1280
    %4739 = torch.prim.ListConstruct %int1_5103, %int64_5104, %int1280_5105 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4740 = torch.aten.view %4738, %4739 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5106 = torch.constant.int 1
    %int-1_5107 = torch.constant.int -1
    %int20_5108 = torch.constant.int 20
    %int64_5109 = torch.constant.int 64
    %4741 = torch.prim.ListConstruct %int1_5106, %int-1_5107, %int20_5108, %int64_5109 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4742 = torch.aten.view %4740, %4741 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5110 = torch.constant.int 1
    %int2_5111 = torch.constant.int 2
    %4743 = torch.aten.transpose.int %4742, %int1_5110, %int2_5111 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_5112 = torch.constant.int 0
    %4744 = torch.aten.clone %4743, %int0_5112 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_5113 = torch.constant.int 1
    %int64_5114 = torch.constant.int 64
    %int20_5115 = torch.constant.int 20
    %int64_5116 = torch.constant.int 64
    %4745 = torch.prim.ListConstruct %int1_5113, %int64_5114, %int20_5115, %int64_5116 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4746 = torch.aten.view %4706, %4745 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5117 = torch.constant.int 1
    %int2_5118 = torch.constant.int 2
    %4747 = torch.aten.transpose.int %4746, %int1_5117, %int2_5118 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_5119 = torch.constant.int 0
    %4748 = torch.aten.clone %4747, %int0_5119 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_5120 = torch.constant.int 20
    %int-1_5121 = torch.constant.int -1
    %int64_5122 = torch.constant.int 64
    %4749 = torch.prim.ListConstruct %int20_5120, %int-1_5121, %int64_5122 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4750 = torch.aten.view %4748, %4749 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_5123 = torch.constant.int 20
    %int-1_5124 = torch.constant.int -1
    %int64_5125 = torch.constant.int 64
    %4751 = torch.prim.ListConstruct %int20_5123, %int-1_5124, %int64_5125 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4752 = torch.aten.view %4725, %4751 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_5126 = torch.constant.int 20
    %int-1_5127 = torch.constant.int -1
    %int64_5128 = torch.constant.int 64
    %4753 = torch.prim.ListConstruct %int20_5126, %int-1_5127, %int64_5128 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4754 = torch.aten.view %4744, %4753 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_5129 = torch.constant.int 1
    %int2_5130 = torch.constant.int 2
    %4755 = torch.aten.transpose.int %4752, %int1_5129, %int2_5130 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %4756 = torch.aten.bmm %4750, %4755 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_5131 = torch.constant.int 1
    %int20_5132 = torch.constant.int 20
    %int64_5133 = torch.constant.int 64
    %int64_5134 = torch.constant.int 64
    %4757 = torch.prim.ListConstruct %int1_5131, %int20_5132, %int64_5133, %int64_5134 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4758 = torch.aten.view %4756, %4757 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_5135 = torch.constant.int 1
    %4759 = torch.aten.add.Tensor %4758, %27, %int1_5135 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_5136 = torch.constant.int 20
    %int64_5137 = torch.constant.int 64
    %int64_5138 = torch.constant.int 64
    %4760 = torch.prim.ListConstruct %int20_5136, %int64_5137, %int64_5138 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4761 = torch.aten.view %4759, %4760 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_5139 = torch.constant.int -1
    %false_5140 = torch.constant.bool false
    %4762 = torch.aten._softmax %4761, %int-1_5139, %false_5140 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %4763 = torch.aten.detach %4762 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_5141 = torch.constant.none
    %4764 = torch.aten.clone %4762, %none_5141 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %4765 = torch.aten.bmm %4764, %4754 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_5142 = torch.constant.int 1
    %int20_5143 = torch.constant.int 20
    %int64_5144 = torch.constant.int 64
    %int64_5145 = torch.constant.int 64
    %4766 = torch.prim.ListConstruct %int1_5142, %int20_5143, %int64_5144, %int64_5145 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4767 = torch.aten.view %4765, %4766 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_5146 = torch.constant.int 1
    %int2_5147 = torch.constant.int 2
    %4768 = torch.aten.transpose.int %4767, %int1_5146, %int2_5147 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_5148 = torch.constant.int 0
    %4769 = torch.aten.clone %4768, %int0_5148 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5149 = torch.constant.int 1
    %int64_5150 = torch.constant.int 64
    %int1280_5151 = torch.constant.int 1280
    %4770 = torch.prim.ListConstruct %int1_5149, %int64_5150, %int1280_5151 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4771 = torch.aten._unsafe_view %4769, %4770 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_5152 = torch.constant.int 64
    %int1280_5153 = torch.constant.int 1280
    %4772 = torch.prim.ListConstruct %int64_5152, %int1280_5153 : (!torch.int, !torch.int) -> !torch.list<int>
    %4773 = torch.aten.view %4771, %4772 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %4774 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5154 = torch.constant.int 0
    %int1_5155 = torch.constant.int 1
    %4775 = torch.aten.transpose.int %4774, %int0_5154, %int1_5155 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.bias : tensor<1280xf16>
    %4776 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5156 = torch.constant.int 6
    %4777 = torch.prims.convert_element_type %4776, %int6_5156 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5157 = torch.constant.int 6
    %4778 = torch.prims.convert_element_type %4773, %int6_5157 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5158 = torch.constant.int 6
    %4779 = torch.prims.convert_element_type %4775, %int6_5158 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4780 = torch.aten.mm %4778, %4779 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5159 = torch.constant.int 1
    %4781 = torch.aten.mul.Scalar %4780, %int1_5159 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5160 = torch.constant.int 1
    %4782 = torch.aten.mul.Scalar %4777, %int1_5160 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5161 = torch.constant.int 1
    %4783 = torch.aten.add.Tensor %4781, %4782, %int1_5161 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5162 = torch.constant.int 5
    %4784 = torch.prims.convert_element_type %4783, %int5_5162 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5163 = torch.constant.int 1
    %int64_5164 = torch.constant.int 64
    %int1280_5165 = torch.constant.int 1280
    %4785 = torch.prim.ListConstruct %int1_5163, %int64_5164, %int1280_5165 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4786 = torch.aten.view %4784, %4785 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5166 = torch.constant.int 1
    %4787 = torch.aten.add.Tensor %4677, %4786, %int1_5166 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_5167 = torch.constant.int 6
    %4788 = torch.prims.convert_element_type %4787, %int6_5167 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_5168 = torch.constant.int 2
    %4789 = torch.prim.ListConstruct %int2_5168 : (!torch.int) -> !torch.list<int>
    %int0_5169 = torch.constant.int 0
    %true_5170 = torch.constant.bool true
    %result0_5171, %result1_5172 = torch.aten.var_mean.correction %4788, %4789, %int0_5169, %true_5170 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_5173 = torch.constant.float 1.000000e-05
    %int1_5174 = torch.constant.int 1
    %4790 = torch.aten.add.Scalar %result0_5171, %float1.000000e-05_5173, %int1_5174 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4791 = torch.aten.rsqrt %4790 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_5175 = torch.constant.int 1
    %4792 = torch.aten.sub.Tensor %4787, %result1_5172, %int1_5175 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4793 = torch.aten.mul.Tensor %4792, %4791 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.weight : tensor<1280xf16>
    %4794 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4795 = torch.aten.mul.Tensor %4793, %4794 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.bias : tensor<1280xf16>
    %4796 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5176 = torch.constant.int 1
    %4797 = torch.aten.add.Tensor %4795, %4796, %int1_5176 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_5177 = torch.constant.int 5
    %4798 = torch.prims.convert_element_type %4797, %int5_5177 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_5178 = torch.constant.int 5
    %4799 = torch.prims.convert_element_type %result1_5172, %int5_5178 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_5179 = torch.constant.int 5
    %4800 = torch.prims.convert_element_type %4791, %int5_5179 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_5180 = torch.constant.int 64
    %int1280_5181 = torch.constant.int 1280
    %4801 = torch.prim.ListConstruct %int64_5180, %int1280_5181 : (!torch.int, !torch.int) -> !torch.list<int>
    %4802 = torch.aten.view %4798, %4801 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.weight : tensor<5120x1280xf16>
    %4803 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_5182 = torch.constant.int 0
    %int1_5183 = torch.constant.int 1
    %4804 = torch.aten.transpose.int %4803, %int0_5182, %int1_5183 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.bias : tensor<5120xf16>
    %4805 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_5184 = torch.constant.int 6
    %4806 = torch.prims.convert_element_type %4805, %int6_5184 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_5185 = torch.constant.int 6
    %4807 = torch.prims.convert_element_type %4802, %int6_5185 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5186 = torch.constant.int 6
    %4808 = torch.prims.convert_element_type %4804, %int6_5186 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4809 = torch.aten.mm %4807, %4808 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_5187 = torch.constant.int 1
    %4810 = torch.aten.mul.Scalar %4809, %int1_5187 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_5188 = torch.constant.int 1
    %4811 = torch.aten.mul.Scalar %4806, %int1_5188 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_5189 = torch.constant.int 1
    %4812 = torch.aten.add.Tensor %4810, %4811, %int1_5189 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_5190 = torch.constant.int 5
    %4813 = torch.prims.convert_element_type %4812, %int5_5190 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_5191 = torch.constant.int 1
    %int64_5192 = torch.constant.int 64
    %int5120_5193 = torch.constant.int 5120
    %4814 = torch.prim.ListConstruct %int1_5191, %int64_5192, %int5120_5193 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4815 = torch.aten.view %4813, %4814 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_5194 = torch.constant.str "none"
    %4816 = torch.aten.gelu %4815, %str_5194 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_5195 = torch.constant.int 64
    %int5120_5196 = torch.constant.int 5120
    %4817 = torch.prim.ListConstruct %int64_5195, %int5120_5196 : (!torch.int, !torch.int) -> !torch.list<int>
    %4818 = torch.aten.view %4816, %4817 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.weight : tensor<1280x5120xf16>
    %4819 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5197 = torch.constant.int 0
    %int1_5198 = torch.constant.int 1
    %4820 = torch.aten.transpose.int %4819, %int0_5197, %int1_5198 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.bias : tensor<1280xf16>
    %4821 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.30.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5199 = torch.constant.int 6
    %4822 = torch.prims.convert_element_type %4821, %int6_5199 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5200 = torch.constant.int 6
    %4823 = torch.prims.convert_element_type %4818, %int6_5200 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_5201 = torch.constant.int 6
    %4824 = torch.prims.convert_element_type %4820, %int6_5201 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4825 = torch.aten.mm %4823, %4824 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5202 = torch.constant.int 1
    %4826 = torch.aten.mul.Scalar %4825, %int1_5202 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5203 = torch.constant.int 1
    %4827 = torch.aten.mul.Scalar %4822, %int1_5203 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5204 = torch.constant.int 1
    %4828 = torch.aten.add.Tensor %4826, %4827, %int1_5204 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5205 = torch.constant.int 5
    %4829 = torch.prims.convert_element_type %4828, %int5_5205 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5206 = torch.constant.int 1
    %int64_5207 = torch.constant.int 64
    %int1280_5208 = torch.constant.int 1280
    %4830 = torch.prim.ListConstruct %int1_5206, %int64_5207, %int1280_5208 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4831 = torch.aten.view %4829, %4830 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5209 = torch.constant.int 1
    %4832 = torch.aten.add.Tensor %4787, %4831, %int1_5209 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_5210 = torch.constant.int 6
    %4833 = torch.prims.convert_element_type %4832, %int6_5210 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_5211 = torch.constant.int 2
    %4834 = torch.prim.ListConstruct %int2_5211 : (!torch.int) -> !torch.list<int>
    %int0_5212 = torch.constant.int 0
    %true_5213 = torch.constant.bool true
    %result0_5214, %result1_5215 = torch.aten.var_mean.correction %4833, %4834, %int0_5212, %true_5213 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_5216 = torch.constant.float 1.000000e-05
    %int1_5217 = torch.constant.int 1
    %4835 = torch.aten.add.Scalar %result0_5214, %float1.000000e-05_5216, %int1_5217 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4836 = torch.aten.rsqrt %4835 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_5218 = torch.constant.int 1
    %4837 = torch.aten.sub.Tensor %4832, %result1_5215, %int1_5218 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4838 = torch.aten.mul.Tensor %4837, %4836 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.weight : tensor<1280xf16>
    %4839 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4840 = torch.aten.mul.Tensor %4838, %4839 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.bias : tensor<1280xf16>
    %4841 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5219 = torch.constant.int 1
    %4842 = torch.aten.add.Tensor %4840, %4841, %int1_5219 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_5220 = torch.constant.int 5
    %4843 = torch.prims.convert_element_type %4842, %int5_5220 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_5221 = torch.constant.int 5
    %4844 = torch.prims.convert_element_type %result1_5215, %int5_5221 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_5222 = torch.constant.int 5
    %4845 = torch.prims.convert_element_type %4836, %int5_5222 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_5223 = torch.constant.int 64
    %int1280_5224 = torch.constant.int 1280
    %4846 = torch.prim.ListConstruct %int64_5223, %int1280_5224 : (!torch.int, !torch.int) -> !torch.list<int>
    %4847 = torch.aten.view %4843, %4846 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.weight : tensor<1280x1280xf16>
    %4848 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5225 = torch.constant.int 0
    %int1_5226 = torch.constant.int 1
    %4849 = torch.aten.transpose.int %4848, %int0_5225, %int1_5226 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.bias : tensor<1280xf16>
    %4850 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.q_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5227 = torch.constant.int 6
    %4851 = torch.prims.convert_element_type %4850, %int6_5227 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5228 = torch.constant.int 6
    %4852 = torch.prims.convert_element_type %4847, %int6_5228 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5229 = torch.constant.int 6
    %4853 = torch.prims.convert_element_type %4849, %int6_5229 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4854 = torch.aten.mm %4852, %4853 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5230 = torch.constant.int 1
    %4855 = torch.aten.mul.Scalar %4854, %int1_5230 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5231 = torch.constant.int 1
    %4856 = torch.aten.mul.Scalar %4851, %int1_5231 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5232 = torch.constant.int 1
    %4857 = torch.aten.add.Tensor %4855, %4856, %int1_5232 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5233 = torch.constant.int 5
    %4858 = torch.prims.convert_element_type %4857, %int5_5233 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5234 = torch.constant.int 1
    %int64_5235 = torch.constant.int 64
    %int1280_5236 = torch.constant.int 1280
    %4859 = torch.prim.ListConstruct %int1_5234, %int64_5235, %int1280_5236 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4860 = torch.aten.view %4858, %4859 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %float1.250000e-01_5237 = torch.constant.float 1.250000e-01
    %4861 = torch.aten.mul.Scalar %4860, %float1.250000e-01_5237 : !torch.vtensor<[1,64,1280],f16>, !torch.float -> !torch.vtensor<[1,64,1280],f16>
    %int64_5238 = torch.constant.int 64
    %int1280_5239 = torch.constant.int 1280
    %4862 = torch.prim.ListConstruct %int64_5238, %int1280_5239 : (!torch.int, !torch.int) -> !torch.list<int>
    %4863 = torch.aten.view %4843, %4862 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.weight : tensor<1280x1280xf16>
    %4864 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5240 = torch.constant.int 0
    %int1_5241 = torch.constant.int 1
    %4865 = torch.aten.transpose.int %4864, %int0_5240, %int1_5241 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.bias : tensor<1280xf16>
    %4866 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.k_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5242 = torch.constant.int 6
    %4867 = torch.prims.convert_element_type %4866, %int6_5242 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5243 = torch.constant.int 6
    %4868 = torch.prims.convert_element_type %4863, %int6_5243 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5244 = torch.constant.int 6
    %4869 = torch.prims.convert_element_type %4865, %int6_5244 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4870 = torch.aten.mm %4868, %4869 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5245 = torch.constant.int 1
    %4871 = torch.aten.mul.Scalar %4870, %int1_5245 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5246 = torch.constant.int 1
    %4872 = torch.aten.mul.Scalar %4867, %int1_5246 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5247 = torch.constant.int 1
    %4873 = torch.aten.add.Tensor %4871, %4872, %int1_5247 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5248 = torch.constant.int 5
    %4874 = torch.prims.convert_element_type %4873, %int5_5248 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5249 = torch.constant.int 1
    %int64_5250 = torch.constant.int 64
    %int1280_5251 = torch.constant.int 1280
    %4875 = torch.prim.ListConstruct %int1_5249, %int64_5250, %int1280_5251 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4876 = torch.aten.view %4874, %4875 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5252 = torch.constant.int 1
    %int-1_5253 = torch.constant.int -1
    %int20_5254 = torch.constant.int 20
    %int64_5255 = torch.constant.int 64
    %4877 = torch.prim.ListConstruct %int1_5252, %int-1_5253, %int20_5254, %int64_5255 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4878 = torch.aten.view %4876, %4877 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5256 = torch.constant.int 1
    %int2_5257 = torch.constant.int 2
    %4879 = torch.aten.transpose.int %4878, %int1_5256, %int2_5257 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_5258 = torch.constant.int 0
    %4880 = torch.aten.clone %4879, %int0_5258 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int64_5259 = torch.constant.int 64
    %int1280_5260 = torch.constant.int 1280
    %4881 = torch.prim.ListConstruct %int64_5259, %int1280_5260 : (!torch.int, !torch.int) -> !torch.list<int>
    %4882 = torch.aten.view %4843, %4881 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.weight : tensor<1280x1280xf16>
    %4883 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5261 = torch.constant.int 0
    %int1_5262 = torch.constant.int 1
    %4884 = torch.aten.transpose.int %4883, %int0_5261, %int1_5262 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.bias : tensor<1280xf16>
    %4885 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.v_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5263 = torch.constant.int 6
    %4886 = torch.prims.convert_element_type %4885, %int6_5263 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5264 = torch.constant.int 6
    %4887 = torch.prims.convert_element_type %4882, %int6_5264 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5265 = torch.constant.int 6
    %4888 = torch.prims.convert_element_type %4884, %int6_5265 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4889 = torch.aten.mm %4887, %4888 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5266 = torch.constant.int 1
    %4890 = torch.aten.mul.Scalar %4889, %int1_5266 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5267 = torch.constant.int 1
    %4891 = torch.aten.mul.Scalar %4886, %int1_5267 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5268 = torch.constant.int 1
    %4892 = torch.aten.add.Tensor %4890, %4891, %int1_5268 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5269 = torch.constant.int 5
    %4893 = torch.prims.convert_element_type %4892, %int5_5269 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5270 = torch.constant.int 1
    %int64_5271 = torch.constant.int 64
    %int1280_5272 = torch.constant.int 1280
    %4894 = torch.prim.ListConstruct %int1_5270, %int64_5271, %int1280_5272 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4895 = torch.aten.view %4893, %4894 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5273 = torch.constant.int 1
    %int-1_5274 = torch.constant.int -1
    %int20_5275 = torch.constant.int 20
    %int64_5276 = torch.constant.int 64
    %4896 = torch.prim.ListConstruct %int1_5273, %int-1_5274, %int20_5275, %int64_5276 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4897 = torch.aten.view %4895, %4896 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5277 = torch.constant.int 1
    %int2_5278 = torch.constant.int 2
    %4898 = torch.aten.transpose.int %4897, %int1_5277, %int2_5278 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_5279 = torch.constant.int 0
    %4899 = torch.aten.clone %4898, %int0_5279 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int1_5280 = torch.constant.int 1
    %int64_5281 = torch.constant.int 64
    %int20_5282 = torch.constant.int 20
    %int64_5283 = torch.constant.int 64
    %4900 = torch.prim.ListConstruct %int1_5280, %int64_5281, %int20_5282, %int64_5283 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4901 = torch.aten.view %4861, %4900 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5284 = torch.constant.int 1
    %int2_5285 = torch.constant.int 2
    %4902 = torch.aten.transpose.int %4901, %int1_5284, %int2_5285 : !torch.vtensor<[1,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int0_5286 = torch.constant.int 0
    %4903 = torch.aten.clone %4902, %int0_5286 : !torch.vtensor<[1,20,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_5287 = torch.constant.int 20
    %int-1_5288 = torch.constant.int -1
    %int64_5289 = torch.constant.int 64
    %4904 = torch.prim.ListConstruct %int20_5287, %int-1_5288, %int64_5289 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4905 = torch.aten.view %4903, %4904 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_5290 = torch.constant.int 20
    %int-1_5291 = torch.constant.int -1
    %int64_5292 = torch.constant.int 64
    %4906 = torch.prim.ListConstruct %int20_5290, %int-1_5291, %int64_5292 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4907 = torch.aten.view %4880, %4906 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int20_5293 = torch.constant.int 20
    %int-1_5294 = torch.constant.int -1
    %int64_5295 = torch.constant.int 64
    %4908 = torch.prim.ListConstruct %int20_5293, %int-1_5294, %int64_5295 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4909 = torch.aten.view %4899, %4908 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int1_5296 = torch.constant.int 1
    %int2_5297 = torch.constant.int 2
    %4910 = torch.aten.transpose.int %4907, %int1_5296, %int2_5297 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[20,64,64],f16>
    %4911 = torch.aten.bmm %4905, %4910 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_5298 = torch.constant.int 1
    %int20_5299 = torch.constant.int 20
    %int64_5300 = torch.constant.int 64
    %int64_5301 = torch.constant.int 64
    %4912 = torch.prim.ListConstruct %int1_5298, %int20_5299, %int64_5300, %int64_5301 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4913 = torch.aten.view %4911, %4912 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_5302 = torch.constant.int 1
    %4914 = torch.aten.add.Tensor %4913, %27, %int1_5302 : !torch.vtensor<[1,20,64,64],f16>, !torch.vtensor<[1,1,64,64],f16>, !torch.int -> !torch.vtensor<[1,20,64,64],f16>
    %int20_5303 = torch.constant.int 20
    %int64_5304 = torch.constant.int 64
    %int64_5305 = torch.constant.int 64
    %4915 = torch.prim.ListConstruct %int20_5303, %int64_5304, %int64_5305 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4916 = torch.aten.view %4914, %4915 : !torch.vtensor<[1,20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[20,64,64],f16>
    %int-1_5306 = torch.constant.int -1
    %false_5307 = torch.constant.bool false
    %4917 = torch.aten._softmax %4916, %int-1_5306, %false_5307 : !torch.vtensor<[20,64,64],f16>, !torch.int, !torch.bool -> !torch.vtensor<[20,64,64],f16>
    %4918 = torch.aten.detach %4917 : !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %none_5308 = torch.constant.none
    %4919 = torch.aten.clone %4917, %none_5308 : !torch.vtensor<[20,64,64],f16>, !torch.none -> !torch.vtensor<[20,64,64],f16>
    %4920 = torch.aten.bmm %4919, %4909 : !torch.vtensor<[20,64,64],f16>, !torch.vtensor<[20,64,64],f16> -> !torch.vtensor<[20,64,64],f16>
    %int1_5309 = torch.constant.int 1
    %int20_5310 = torch.constant.int 20
    %int64_5311 = torch.constant.int 64
    %int64_5312 = torch.constant.int 64
    %4921 = torch.prim.ListConstruct %int1_5309, %int20_5310, %int64_5311, %int64_5312 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4922 = torch.aten.view %4920, %4921 : !torch.vtensor<[20,64,64],f16>, !torch.list<int> -> !torch.vtensor<[1,20,64,64],f16>
    %int1_5313 = torch.constant.int 1
    %int2_5314 = torch.constant.int 2
    %4923 = torch.aten.transpose.int %4922, %int1_5313, %int2_5314 : !torch.vtensor<[1,20,64,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int0_5315 = torch.constant.int 0
    %4924 = torch.aten.clone %4923, %int0_5315 : !torch.vtensor<[1,64,20,64],f16>, !torch.int -> !torch.vtensor<[1,64,20,64],f16>
    %int1_5316 = torch.constant.int 1
    %int64_5317 = torch.constant.int 64
    %int1280_5318 = torch.constant.int 1280
    %4925 = torch.prim.ListConstruct %int1_5316, %int64_5317, %int1280_5318 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4926 = torch.aten._unsafe_view %4924, %4925 : !torch.vtensor<[1,64,20,64],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int64_5319 = torch.constant.int 64
    %int1280_5320 = torch.constant.int 1280
    %4927 = torch.prim.ListConstruct %int64_5319, %int1280_5320 : (!torch.int, !torch.int) -> !torch.list<int>
    %4928 = torch.aten.view %4926, %4927 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.weight : tensor<1280x1280xf16>
    %4929 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5321 = torch.constant.int 0
    %int1_5322 = torch.constant.int 1
    %4930 = torch.aten.transpose.int %4929, %int0_5321, %int1_5322 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.bias : tensor<1280xf16>
    %4931 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.self_attn.out_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5323 = torch.constant.int 6
    %4932 = torch.prims.convert_element_type %4931, %int6_5323 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5324 = torch.constant.int 6
    %4933 = torch.prims.convert_element_type %4928, %int6_5324 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5325 = torch.constant.int 6
    %4934 = torch.prims.convert_element_type %4930, %int6_5325 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4935 = torch.aten.mm %4933, %4934 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5326 = torch.constant.int 1
    %4936 = torch.aten.mul.Scalar %4935, %int1_5326 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5327 = torch.constant.int 1
    %4937 = torch.aten.mul.Scalar %4932, %int1_5327 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5328 = torch.constant.int 1
    %4938 = torch.aten.add.Tensor %4936, %4937, %int1_5328 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5329 = torch.constant.int 5
    %4939 = torch.prims.convert_element_type %4938, %int5_5329 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5330 = torch.constant.int 1
    %int64_5331 = torch.constant.int 64
    %int1280_5332 = torch.constant.int 1280
    %4940 = torch.prim.ListConstruct %int1_5330, %int64_5331, %int1280_5332 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4941 = torch.aten.view %4939, %4940 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5333 = torch.constant.int 1
    %4942 = torch.aten.add.Tensor %4832, %4941, %int1_5333 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_5334 = torch.constant.int 6
    %4943 = torch.prims.convert_element_type %4942, %int6_5334 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_5335 = torch.constant.int 2
    %4944 = torch.prim.ListConstruct %int2_5335 : (!torch.int) -> !torch.list<int>
    %int0_5336 = torch.constant.int 0
    %true_5337 = torch.constant.bool true
    %result0_5338, %result1_5339 = torch.aten.var_mean.correction %4943, %4944, %int0_5336, %true_5337 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_5340 = torch.constant.float 1.000000e-05
    %int1_5341 = torch.constant.int 1
    %4945 = torch.aten.add.Scalar %result0_5338, %float1.000000e-05_5340, %int1_5341 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4946 = torch.aten.rsqrt %4945 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_5342 = torch.constant.int 1
    %4947 = torch.aten.sub.Tensor %4942, %result1_5339, %int1_5342 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4948 = torch.aten.mul.Tensor %4947, %4946 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.weight : tensor<1280xf16>
    %4949 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4950 = torch.aten.mul.Tensor %4948, %4949 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.bias : tensor<1280xf16>
    %4951 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.layer_norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5343 = torch.constant.int 1
    %4952 = torch.aten.add.Tensor %4950, %4951, %int1_5343 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_5344 = torch.constant.int 5
    %4953 = torch.prims.convert_element_type %4952, %int5_5344 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_5345 = torch.constant.int 5
    %4954 = torch.prims.convert_element_type %result1_5339, %int5_5345 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_5346 = torch.constant.int 5
    %4955 = torch.prims.convert_element_type %4946, %int5_5346 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int64_5347 = torch.constant.int 64
    %int1280_5348 = torch.constant.int 1280
    %4956 = torch.prim.ListConstruct %int64_5347, %int1280_5348 : (!torch.int, !torch.int) -> !torch.list<int>
    %4957 = torch.aten.view %4953, %4956 : !torch.vtensor<[1,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[64,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.weight : tensor<5120x1280xf16>
    %4958 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.weight : tensor<5120x1280xf16> -> !torch.vtensor<[5120,1280],f16>
    %int0_5349 = torch.constant.int 0
    %int1_5350 = torch.constant.int 1
    %4959 = torch.aten.transpose.int %4958, %int0_5349, %int1_5350 : !torch.vtensor<[5120,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.bias : tensor<5120xf16>
    %4960 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc1.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_5351 = torch.constant.int 6
    %4961 = torch.prims.convert_element_type %4960, %int6_5351 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_5352 = torch.constant.int 6
    %4962 = torch.prims.convert_element_type %4957, %int6_5352 : !torch.vtensor<[64,1280],f16>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int6_5353 = torch.constant.int 6
    %4963 = torch.prims.convert_element_type %4959, %int6_5353 : !torch.vtensor<[1280,5120],f16>, !torch.int -> !torch.vtensor<[1280,5120],f32>
    %4964 = torch.aten.mm %4962, %4963 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280,5120],f32> -> !torch.vtensor<[64,5120],f32>
    %int1_5354 = torch.constant.int 1
    %4965 = torch.aten.mul.Scalar %4964, %int1_5354 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int1_5355 = torch.constant.int 1
    %4966 = torch.aten.mul.Scalar %4961, %int1_5355 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_5356 = torch.constant.int 1
    %4967 = torch.aten.add.Tensor %4965, %4966, %int1_5356 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int5_5357 = torch.constant.int 5
    %4968 = torch.prims.convert_element_type %4967, %int5_5357 : !torch.vtensor<[64,5120],f32>, !torch.int -> !torch.vtensor<[64,5120],f16>
    %int1_5358 = torch.constant.int 1
    %int64_5359 = torch.constant.int 64
    %int5120_5360 = torch.constant.int 5120
    %4969 = torch.prim.ListConstruct %int1_5358, %int64_5359, %int5120_5360 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4970 = torch.aten.view %4968, %4969 : !torch.vtensor<[64,5120],f16>, !torch.list<int> -> !torch.vtensor<[1,64,5120],f16>
    %str_5361 = torch.constant.str "none"
    %4971 = torch.aten.gelu %4970, %str_5361 : !torch.vtensor<[1,64,5120],f16>, !torch.str -> !torch.vtensor<[1,64,5120],f16>
    %int64_5362 = torch.constant.int 64
    %int5120_5363 = torch.constant.int 5120
    %4972 = torch.prim.ListConstruct %int64_5362, %int5120_5363 : (!torch.int, !torch.int) -> !torch.list<int>
    %4973 = torch.aten.view %4971, %4972 : !torch.vtensor<[1,64,5120],f16>, !torch.list<int> -> !torch.vtensor<[64,5120],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.weight = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.weight : tensor<1280x5120xf16>
    %4974 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5364 = torch.constant.int 0
    %int1_5365 = torch.constant.int 1
    %4975 = torch.aten.transpose.int %4974, %int0_5364, %int1_5365 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.bias = util.global.load @_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.bias : tensor<1280xf16>
    %4976 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.encoder.layers.31.mlp.fc2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5366 = torch.constant.int 6
    %4977 = torch.prims.convert_element_type %4976, %int6_5366 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5367 = torch.constant.int 6
    %4978 = torch.prims.convert_element_type %4973, %int6_5367 : !torch.vtensor<[64,5120],f16>, !torch.int -> !torch.vtensor<[64,5120],f32>
    %int6_5368 = torch.constant.int 6
    %4979 = torch.prims.convert_element_type %4975, %int6_5368 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4980 = torch.aten.mm %4978, %4979 : !torch.vtensor<[64,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[64,1280],f32>
    %int1_5369 = torch.constant.int 1
    %4981 = torch.aten.mul.Scalar %4980, %int1_5369 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int1_5370 = torch.constant.int 1
    %4982 = torch.aten.mul.Scalar %4977, %int1_5370 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5371 = torch.constant.int 1
    %4983 = torch.aten.add.Tensor %4981, %4982, %int1_5371 : !torch.vtensor<[64,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f32>
    %int5_5372 = torch.constant.int 5
    %4984 = torch.prims.convert_element_type %4983, %int5_5372 : !torch.vtensor<[64,1280],f32>, !torch.int -> !torch.vtensor<[64,1280],f16>
    %int1_5373 = torch.constant.int 1
    %int64_5374 = torch.constant.int 64
    %int1280_5375 = torch.constant.int 1280
    %4985 = torch.prim.ListConstruct %int1_5373, %int64_5374, %int1280_5375 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4986 = torch.aten.view %4984, %4985 : !torch.vtensor<[64,1280],f16>, !torch.list<int> -> !torch.vtensor<[1,64,1280],f16>
    %int1_5376 = torch.constant.int 1
    %4987 = torch.aten.add.Tensor %4942, %4986, %int1_5376 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int6_5377 = torch.constant.int 6
    %4988 = torch.prims.convert_element_type %4987, %int6_5377 : !torch.vtensor<[1,64,1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int2_5378 = torch.constant.int 2
    %4989 = torch.prim.ListConstruct %int2_5378 : (!torch.int) -> !torch.list<int>
    %int0_5379 = torch.constant.int 0
    %true_5380 = torch.constant.bool true
    %result0_5381, %result1_5382 = torch.aten.var_mean.correction %4988, %4989, %int0_5379, %true_5380 : !torch.vtensor<[1,64,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[1,64,1],f32>, !torch.vtensor<[1,64,1],f32>
    %float1.000000e-05_5383 = torch.constant.float 1.000000e-05
    %int1_5384 = torch.constant.int 1
    %4990 = torch.aten.add.Scalar %result0_5381, %float1.000000e-05_5383, %int1_5384 : !torch.vtensor<[1,64,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,1],f32>
    %4991 = torch.aten.rsqrt %4990 : !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1],f32>
    %int1_5385 = torch.constant.int 1
    %4992 = torch.aten.sub.Tensor %4987, %result1_5382, %int1_5385 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %4993 = torch.aten.mul.Tensor %4992, %4991 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1,64,1],f32> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.final_layer_norm.weight = util.global.load @_params.text_encoder_model.text_model.final_layer_norm.weight : tensor<1280xf16>
    %4994 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.final_layer_norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4995 = torch.aten.mul.Tensor %4993, %4994 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[1,64,1280],f32>
    %_params.text_encoder_model.text_model.final_layer_norm.bias = util.global.load @_params.text_encoder_model.text_model.final_layer_norm.bias : tensor<1280xf16>
    %4996 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_model.final_layer_norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5386 = torch.constant.int 1
    %4997 = torch.aten.add.Tensor %4995, %4996, %int1_5386 : !torch.vtensor<[1,64,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,64,1280],f32>
    %int5_5387 = torch.constant.int 5
    %4998 = torch.prims.convert_element_type %4997, %int5_5387 : !torch.vtensor<[1,64,1280],f32>, !torch.int -> !torch.vtensor<[1,64,1280],f16>
    %int5_5388 = torch.constant.int 5
    %4999 = torch.prims.convert_element_type %result1_5382, %int5_5388 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int5_5389 = torch.constant.int 5
    %5000 = torch.prims.convert_element_type %4991, %int5_5389 : !torch.vtensor<[1,64,1],f32>, !torch.int -> !torch.vtensor<[1,64,1],f16>
    %int1_5390 = torch.constant.int 1
    %none_5391 = torch.constant.none
    %none_5392 = torch.constant.none
    %cpu_5393 = torch.constant.device "cpu"
    %false_5394 = torch.constant.bool false
    %5001 = torch.aten.arange %int1_5390, %none_5391, %none_5392, %cpu_5393, %false_5394 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int3_5395 = torch.constant.int 3
    %5002 = torch.prims.convert_element_type %1, %int3_5395 : !torch.vtensor<[1,64],si64>, !torch.int -> !torch.vtensor<[1,64],si32>
    %int-1_5396 = torch.constant.int -1
    %false_5397 = torch.constant.bool false
    %5003 = torch.aten.argmax %5002, %int-1_5396, %false_5397 : !torch.vtensor<[1,64],si32>, !torch.int, !torch.bool -> !torch.vtensor<[1],si64>
    %5004 = torch.prim.ListConstruct %5001, %5003 : (!torch.vtensor<[1],si64>, !torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
    %5005 = torch.aten.index.Tensor %4998, %5004 : !torch.vtensor<[1,64,1280],f16>, !torch.list<optional<vtensor>> -> !torch.vtensor<[1,1280],f16>
    %_params.text_encoder_model.text_projection.weight = util.global.load @_params.text_encoder_model.text_projection.weight : tensor<1280x1280xf16>
    %5006 = torch_c.from_builtin_tensor %_params.text_encoder_model.text_projection.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5398 = torch.constant.int 0
    %int1_5399 = torch.constant.int 1
    %5007 = torch.aten.transpose.int %5006, %int0_5398, %int1_5399 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %5008 = torch.aten.mm %5005, %5007 : !torch.vtensor<[1,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1,1280],f16>
    return %4832, %5008 : !torch.vtensor<[1,64,1280],f16>, !torch.vtensor<[1,1280],f16>
  }
}

{-#
  dialect_resources: {
    builtin: {
      torch_tensor_1_77_torch.int64: "0x0800000000000000000000000100000000000000020000000000000003000000000000000400000000000000050000000000000006000000000000000700000000000000080000000000000009000000000000000A000000000000000B000000000000000C000000000000000D000000000000000E000000000000000F0000000000000010000000000000001100000000000000120000000000000013000000000000001400000000000000150000000000000016000000000000001700000000000000180000000000000019000000000000001A000000000000001B000000000000001C000000000000001D000000000000001E000000000000001F0000000000000020000000000000002100000000000000220000000000000023000000000000002400000000000000250000000000000026000000000000002700000000000000280000000000000029000000000000002A000000000000002B000000000000002C000000000000002D000000000000002E000000000000002F0000000000000030000000000000003100000000000000320000000000000033000000000000003400000000000000350000000000000036000000000000003700000000000000380000000000000039000000000000003A000000000000003B000000000000003C000000000000003D000000000000003E000000000000003F0000000000000040000000000000004100000000000000420000000000000043000000000000004400000000000000450000000000000046000000000000004700000000000000480000000000000049000000000000004A000000000000004B000000000000004C00000000000000"
    }
  }
#-}
